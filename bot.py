"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸ›¡ï¸ Military Intel Bot v9 â€” FULLY FIXED                            â•‘
â•‘                                                                          â•‘
â•‘  Ø¨Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø±ÙØ¹â€ŒØ´Ø¯Ù‡:                                                         â•‘
â•‘  âœ… Bug1: Cutoff Ø«Ø§Ø¨Øª Û±Û· Ø¯Ù‚ÛŒÙ‚Ù‡ â†’ Cutoff Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© (Û² Ø³Ø§Ø¹Øª)               â•‘
â•‘  âœ… Bug2: Nitter Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø±Ø¯Ù‡ â†’ RSSHub Ø¨Ø§ Ûµ instance fallback            â•‘
â•‘  âœ… Bug3: Û±Û· URL Ù…Ø±Ø¯Ù‡ â†’ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†â€ŒÙ‡Ø§ÛŒ ØªØ³Øªâ€ŒØ´Ø¯Ù‡                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os, json, hashlib, asyncio, logging, re
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

# â”€â”€ Hazm Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§Ø±Ø³ÛŒ â”€â”€
try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S")
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE       = "seen.json"
MAX_NEW_PER_RUN = 20
MAX_MSG_LEN     = 4096
SEND_DELAY      = 2
TEHRAN_TZ       = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âœ… FIX 1 â€” CUTOFF Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© (Ù†Ù‡ Ø«Ø§Ø¨Øª!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ø´Ú©Ù„ Ù‚Ø¨Ù„ÛŒ: cutoff Ø«Ø§Ø¨Øª Û²Û³:Û´Û¸ UTC Ø¨ÙˆØ¯ØŒ Ø¨Ø§Øª Û°Û°:Û°Ûµ Ø§Ø¬Ø±Ø§ Ø´Ø¯
# â†’ ÙÙ‚Ø· Û±Û· Ø¯Ù‚ÛŒÙ‚Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ â†’ Ù‡Ù…Ù‡ Ø®Ø¨Ø±Ù‡Ø§ Ø±Ø¯ Ø´Ø¯Ù†Ø¯!
#
# Ø±Ø§Ù‡â€ŒØ­Ù„: cutoff = "Ù‡Ù…ÛŒÙ† Ø§Ù„Ø§Ù† Ù…Ù†Ù‡Ø§ÛŒ Û² Ø³Ø§Ø¹Øª"
# seen.json Ø¶Ø¯ ØªÚ©Ø±Ø§Ø± Ø§Ø³Øª â†’ Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø¯ÙˆØ¨Ø§Ø± Ù†Ù…ÛŒâ€ŒØ±ÙˆØ¯
# Û² Ø³Ø§Ø¹Øª = Ù¾Ù†Ø¬Ø±Ù‡ Ø§ÛŒÙ…Ù† Ú©Ù‡ Ù‡Ù… Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ù‡Ù… Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒÙ‡Ø§ Ø±Ø¯ Ù…ÛŒâ€ŒØ´Ù†

def get_cutoff() -> datetime:
    """Cutoff Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©: Ù‡Ù…ÛŒØ´Ù‡ Û² Ø³Ø§Ø¹Øª Ù‚Ø¨Ù„ Ø§Ø² Ø§Ù„Ø§Ù†"""
    return datetime.now(timezone.utc) - timedelta(hours=2)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ Û±. ÙÛŒØ¯Ù‡Ø§ÛŒ RSS â€” URLÙ‡Ø§ÛŒ ØªØ³Øªâ€ŒØ´Ø¯Ù‡ Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [

    # â•â• Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ â•â•
    {"name": "ğŸŒ Reuters World",       "url": "https://feeds.reuters.com/reuters/worldNews"},
    {"name": "ğŸŒ Reuters Top",         "url": "https://feeds.reuters.com/reuters/topNews"},
    {"name": "ğŸŒ AP Top",              "url": "https://feeds.apnews.com/rss/apf-topnews"},
    {"name": "ğŸŒ AP World",            "url": "https://feeds.apnews.com/rss/apf-WorldNews"},
    {"name": "ğŸŒ AP Military",         "url": "https://apnews.com/hub/military-and-defense?format=rss"},
    {"name": "ğŸŒ Bloomberg Politics",  "url": "https://feeds.bloomberg.com/politics/news.rss"},
    {"name": "ğŸŒ WSJ World",           "url": "https://feeds.a.dj.com/rss/RSSWorldNews.xml"},
    # âœ… NYT â€” Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­
    {"name": "ğŸŒ NYT World",           "url": "https://rss.nytimes.com/services/xml/rss/nyt/World.rss"},
    {"name": "ğŸŒ NYT Middle East",     "url": "https://rss.nytimes.com/services/xml/rss/nyt/MiddleEast.rss"},
    {"name": "ğŸŒ CNN Middle East",     "url": "http://rss.cnn.com/rss/edition_meast.rss"},
    {"name": "ğŸŒ CNN World",           "url": "http://rss.cnn.com/rss/edition_world.rss"},
    # âœ… BBC â€” Ø¨Ø§ redirect Ø®ÙˆØ¯Ú©Ø§Ø± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    {"name": "ğŸŒ BBC Middle East",     "url": "https://feeds.bbci.co.uk/news/world/middle_east/rss.xml"},
    {"name": "ğŸŒ BBC World",           "url": "https://feeds.bbci.co.uk/news/world/rss.xml"},
    {"name": "ğŸŒ Al Jazeera",          "url": "https://www.aljazeera.com/xml/rss/all.xml"},
    {"name": "ğŸŒ Fox News World",      "url": "https://moxie.foxnews.com/google-publisher/world.xml"},
    {"name": "ğŸŒ Politico NatSec",     "url": "https://rss.politico.com/defense.xml"},
    {"name": "ğŸŒ Politico Politics",   "url": "https://rss.politico.com/politics-news.xml"},
    # âœ… The Hill â€” Ø¨Ø§ redirect Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    {"name": "ğŸŒ The Hill",            "url": "https://thehill.com/news/feed/"},
    {"name": "ğŸŒ Foreign Policy",      "url": "https://foreignpolicy.com/feed/"},
    {"name": "ğŸŒ Foreign Affairs",     "url": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "ğŸŒ The Intercept",       "url": "https://theintercept.com/feed/?rss=1"},
    {"name": "ğŸŒ Middle East Eye",     "url": "https://www.middleeasteye.net/rss"},

    # â•â• Ø§Ú©Ø³ÛŒÙˆØ³ â€” Ø§Ø² Ø·Ø±ÛŒÙ‚ Google News (API Ù…Ø±Ø¯Ù‡) â•â•
    # âœ… Axios API Ù‡Ø§ Ù‡Ù…Ù‡ 404 Ø´Ø¯Ù†Ø¯ â€” Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Google News
    {"name": "ğŸ“° Axios (GNews)",       "url": "https://news.google.com/rss/search?q=site:axios.com+national+security+iran+israel&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ“° Axios World (GNews)", "url": "https://news.google.com/rss/search?q=site:axios.com+military+iran+israel+war&hl=en-US&gl=US&ceid=US:en"},

    # â•â• Ø¢Ù…Ø±ÛŒÚ©Ø§ Ù†Ø¸Ø§Ù…ÛŒ â•â•
    # âœ… Pentagon â€” Ø¨Ø§ redirect
    {"name": "ğŸ‡ºğŸ‡¸ Pentagon",           "url": "https://www.defense.gov/DesktopModules/ArticleCS/RSS.ashx?ContentType=1&Site=945&max=10"},
    # CENTCOM RSS â€” 403 Ø¯Ø± Ø¨Ø¹Ø¶ÛŒ Ù…ÙˆØ§Ù‚Ø¹ØŒ Google News Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
    {"name": "ğŸ‡ºğŸ‡¸ CENTCOM (GNews)",    "url": "https://news.google.com/rss/search?q=CENTCOM+site:centcom.mil&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ‡ºğŸ‡¸ USNI News",          "url": "https://news.usni.org/feed"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",   "url": "https://breakingdefense.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense News",       "url": "https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"name": "ğŸ‡ºğŸ‡¸ Military Times",     "url": "https://www.militarytimes.com/arc/outboundfeeds/rss/"},
    # âœ… Stars & Stripes â€” URL Ø¬Ø¯ÛŒØ¯
    {"name": "ğŸ‡ºğŸ‡¸ Stars & Stripes",    "url": "https://www.stripes.com/feed"},
    {"name": "ğŸ‡ºğŸ‡¸ C4ISRNET",           "url": "https://www.c4isrnet.com/arc/outboundfeeds/rss/"},
    # âœ… The War Zone â€” URL Ø¬Ø¯ÛŒØ¯
    {"name": "ğŸ‡ºğŸ‡¸ The War Zone",       "url": "https://www.thedrive.com/the-war-zone/feed"},
    {"name": "ğŸ‡ºğŸ‡¸ War on Rocks",       "url": "https://warontherocks.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Task & Purpose",     "url": "https://taskandpurpose.com/feed/"},

    # â•â• Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â•â•
    # âœ… IDF â€” URL Ø¬Ø¯ÛŒØ¯
    {"name": "ğŸ‡®ğŸ‡± IDF (GNews)",        "url": "https://news.google.com/rss/search?q=IDF+site:idf.il&hl=en-US&gl=US&ceid=US:en"},
    # âœ… JP Military â†’ JP All (Military Ù…Ø±Ø¯Ù‡)
    {"name": "ğŸ‡®ğŸ‡± Jerusalem Post",     "url": "https://www.jpost.com/rss/rssfeedsheadlines.aspx"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",    "url": "https://www.timesofisrael.com/feed/"},
    # âœ… Haaretz â†’ Ø§Ø² Google News (403 Ù…Ø³ØªÙ‚ÛŒÙ…)
    {"name": "ğŸ‡®ğŸ‡± Haaretz (GNews)",    "url": "https://news.google.com/rss/search?q=site:haaretz.com+iran+israel+military&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ‡®ğŸ‡± Israel Hayom",       "url": "https://www.israelhayom.com/feed/"},
    # âœ… Ynetnews â€” URL ØµØ­ÛŒØ­
    {"name": "ğŸ‡®ğŸ‡± Ynetnews",           "url": "https://www.ynetnews.com/RSS/EnglishFeed.xml"},
    {"name": "ğŸ‡®ğŸ‡± i24 News",           "url": "https://www.i24news.tv/en/rss"},
    # âœ… Arutz Sheva â€” URL Ø¬Ø¯ÛŒØ¯
    {"name": "ğŸ‡®ğŸ‡± Arutz Sheva",        "url": "https://www.israelnationalnews.com/rss.aspx"},

    # â•â• Ø§ÛŒØ±Ø§Ù† â•â•
    {"name": "ğŸ‡®ğŸ‡· Iran International", "url": "https://www.iranintl.com/en/rss"},
    # âœ… Radio Farda â€” URL Ø¬Ø¯ÛŒØ¯
    {"name": "ğŸ‡®ğŸ‡· Radio Farda",        "url": "https://www.rferl.org/api/epiqeguqiup"},

    # â•â• ØªØ­Ù„ÛŒÙ„ÛŒ / OSINT â•â•
    # âœ… ISW â†’ Ø§Ø² Google News (403 Ù…Ø³ØªÙ‚ÛŒÙ…)
    {"name": "ğŸ” ISW (GNews)",         "url": "https://news.google.com/rss/search?q=site:understandingwar.org&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ” Long War Journal",    "url": "https://www.longwarjournal.org/feed"},
    {"name": "ğŸ” Bellingcat",          "url": "https://www.bellingcat.com/feed/"},
    {"name": "ğŸ” OSINT Defender",      "url": "https://osintdefender.com/feed/"},
    # âœ… RAND â€” URL Ø¬Ø¯ÛŒØ¯
    {"name": "ğŸ” RAND Defense",        "url": "https://www.rand.org/topics/defense-and-security.xml"},
    # âœ… Lawfare â†’ GNews
    {"name": "ğŸ” Lawfare (GNews)",     "url": "https://news.google.com/rss/search?q=site:lawfaremedia.org+iran+israel&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ” Just Security",       "url": "https://www.justsecurity.org/feed/"},
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ Û². Google News â€” Û²Û° Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡Ø¯ÙÙ…Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GOOGLE_QUERIES = [
    ("âš”ï¸ Iran Israel War",      "Iran Israel war attack strike"),
    ("âš”ï¸ Iran Airstrike",       "Iran airstrike bomb explosion"),
    ("âš”ï¸ US Iran Military",     "United States Iran military IRGC"),
    ("âš”ï¸ IDF Operation",        "IDF military operation strike"),
    ("âš”ï¸ Iran Nuclear",         "Iran nuclear IAEA uranium enrichment"),
    ("âš”ï¸ Iran Missile Drone",   "Iran ballistic missile drone attack"),
    ("âš”ï¸ Hezbollah IDF",        "Hezbollah IDF Lebanon border strike"),
    ("âš”ï¸ Strait Hormuz",        "Strait Hormuz tanker navy seized"),
    ("âš”ï¸ IRGC Attack",          "IRGC Revolutionary Guard base attack"),
    ("âš”ï¸ Israel Strike Syria",  "Israel airstrike Syria Iraq Iran"),
    ("âš”ï¸ Mossad Operation",     "Mossad covert operation intelligence"),
    ("âš”ï¸ Khamenei Netanyahu",   "Khamenei Netanyahu war threat"),
    ("âš”ï¸ US Navy Gulf",         "US carrier strike group Persian Gulf"),
    ("âš”ï¸ Iron Dome",            "Iron Dome Patriot Arrow intercept missile"),
    ("âš”ï¸ Iran Sanctions",       "Iran sanctions oil SWIFT 2026"),
    ("âš”ï¸ Red Sea Houthis",      "Red Sea Houthi attack ship missile"),
    ("âš”ï¸ Gaza Deal 2026",       "Gaza ceasefire Hamas IDF deal 2026"),
    ("âš”ï¸ Iran Proxy Militia",   "Iran proxy militia Iraq Syria US base"),
    ("âš”ï¸ Nuclear Escalation",   "nuclear military escalation Middle East"),
    ("âš”ï¸ Trump Iran Israel",    "Trump Iran Israel military policy"),
]

def gnews(q):
    return f"https://news.google.com/rss/search?q={q.replace(' ','+')}&hl=en-US&gl=US&ceid=US:en&num=15"

GOOGLE_FEEDS = [{"name": n, "url": gnews(q), "is_google": True} for n, q in GOOGLE_QUERIES]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ Û³. ØªÙˆÛŒÛŒØªØ±/X â€” âœ… FIX 2: RSSHub (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Nitter Ù…Ø±Ø¯Ù‡) â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# Ù…Ø´Ú©Ù„: Nitter.poast.org â†’ 403, nitter.kavin.rocks â†’ 502
# Ø±Ø§Ù‡â€ŒØ­Ù„: RSSHub â€” Ú†Ù†Ø¯ instance Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ fallback
#
# RSSHub instances Ø¹Ù…ÙˆÙ…ÛŒ (ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª):
RSSHUB_INSTANCES = [
    "https://rsshub.app",
    "https://rsshub.rssforever.com",
    "https://hub.slarker.me",
    "https://rsshub.feeded.app",
    "https://rsshub.woodland.cafe",
]

TWITTER_ACCOUNTS = [
    # â”€â”€ OSINT / Ø§Ø·Ù„Ø§Ø¹Ø§Øª â”€â”€
    ("ğŸ” OSINT Defender",       "OSINTdefender"),
    ("ğŸ” Intel Crab",           "IntelCrab"),
    ("ğŸ” War Monitor",          "WarMonitor3"),
    ("ğŸ” Conflicts.media",      "Conflicts"),
    ("ğŸ” Aurora Intel",         "AuroraIntel"),
    ("ğŸ” GeoConfirmed",         "GeoConfirmed"),

    # â”€â”€ Axios Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† â”€â”€
    ("ğŸ“° Axios: Barak Ravid",   "BarakRavid"),       # Ø§Ø³Ø±Ø§ÛŒÛŒÙ„/Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ
    ("ğŸ“° Axios: Alex Ward",     "alexward1961"),      # Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ
    ("ğŸ“° Axios: Zach Basu",     "ZachBasu"),

    # â”€â”€ Reuters Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† â”€â”€
    ("ğŸ“° Reuters: Idrees Ali",  "idreesali114"),      # Pentagon
    ("ğŸ“° Reuters: Phil Stewart","phil_stewart_"),
    ("ğŸ“° Reuters: Jonathan L",  "JLanday"),

    # â”€â”€ NYT Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† â”€â”€
    ("ğŸ“° NYT: Farnaz Fassihi",  "farnazfassihi"),    # Ø§ÛŒØ±Ø§Ù†/Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡
    ("ğŸ“° NYT: Eric Schmitt",    "EricSchmittNYT"),    # Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ
    ("ğŸ“° NYT: Helene Cooper",   "helenecooper"),      # Pentagon

    # â”€â”€ WaPo Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† â”€â”€
    ("ğŸ“° WaPo: Dan Lamothe",    "DanLamothe"),

    # â”€â”€ Politico / FP Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† â”€â”€
    ("ğŸ“° Politico: Lara S",     "laraseligman"),
    ("ğŸ“° FP: Jack Detsch",      "JackDetsch"),
    ("ğŸ“° FP: Robbie Gramer",    "RobbieGramer"),
    ("ğŸ“° NatashaBertrand",      "NatashaBertrand"),

    # â”€â”€ Ø±Ø³Ù…ÛŒ â”€â”€
    ("ğŸ‡®ğŸ‡± IDF Official",        "IDF"),
    ("ğŸ‡ºğŸ‡¸ CENTCOM",             "CENTCOM"),
    ("ğŸ‡ºğŸ‡¸ Dept of Defense",     "DeptofDefense"),

    # â”€â”€ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â”€â”€
    ("ğŸ‡®ğŸ‡± Yossi Melman",        "yossi_melman"),
    ("ğŸ‡®ğŸ‡± Seth Frantzman",      "sfrantzman"),
    ("ğŸ‡®ğŸ‡± Avi Issacharoff",     "AviIssacharoff"),

    # â”€â”€ Ø§ÛŒØ±Ø§Ù† â”€â”€
    ("ğŸ‡®ğŸ‡· Iran Intl English",   "IranIntl_En"),

    # â”€â”€ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ â”€â”€
    ("ğŸŒ Joyce Karam",          "Joyce_Karam"),
    ("ğŸŒ Ragip Soylu",          "ragipsoylu"),

    # â”€â”€ Ù‡Ø´Ø¯Ø§Ø± â”€â”€
    ("âš ï¸ DEFCON Level",         "DEFCONLevel"),
    ("âš ï¸ Arms Control Wonk",    "ArmsControlWonk"),
]

def get_twitter_feeds():
    feeds = []
    for name, handle in TWITTER_ACCOUNTS:
        # Ù‡Ù…Ù‡ RSSHub instance Ù‡Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† fallback
        urls = [f"{inst}/twitter/user/{handle}" for inst in RSSHUB_INSTANCES]
        feeds.append({
            "name": f"ğ• {name}",
            "twitter_handle": handle,
            "twitter_urls": urls,   # â† Ù„ÛŒØ³Øª URL Ø¨Ø±Ø§ÛŒ Ø§Ù…ØªØ­Ø§Ù†
        })
    return feeds

TWITTER_FEEDS = get_twitter_feeds()
ALL_FEEDS = RSS_FEEDS + GOOGLE_FEEDS  # Twitter Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ fetch Ù…ÛŒâ€ŒØ´ÙˆØ¯

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÙÛŒÙ„ØªØ±Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEYWORDS = [
    "Ø³Ù¾Ø§Ù‡","Ù…ÙˆØ´Ú©","Ø¬Ù†Ú¯","Ø­Ù…Ù„Ù‡","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø§ÛŒØ±Ø§Ù†","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ù¾Ù‡Ù¾Ø§Ø¯","Ù†Ø¸Ø§Ù…ÛŒ",
    "iran","irgc","khamenei","tehran","revolutionary guard","nuclear",
    "israel","idf","mossad","tel aviv","netanyahu",
    "hamas","hezbollah","houthi","ansarallah",
    "pentagon","centcom","us forces","us military","us base","american",
    "strike","airstrike","missile","ballistic","drone","uav",
    "attack","bomb","explosion","assassination","operation",
    "warship","carrier","navy","air force","troops",
    "persian gulf","strait of hormuz","red sea","middle east",
    "iron dome","arrow","patriot","hypersonic",
    "uranium","enrichment","natanz","fordo","iaea",
    "intelligence","cia","covert","sanction","embargo",
    "gaza","west bank","lebanon","syria","iraq","yemen","bahrain",
    "trump","rubio","waltz","war","conflict","escalat","deploy",
]

def is_fresh(entry: dict) -> bool:
    """âœ… FIX: Cutoff Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© â€” Û² Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±"""
    cutoff = get_cutoff()
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t:
            return False   # Ø¨Ø¯ÙˆÙ† ØªØ§Ø±ÛŒØ® = Ø±Ø¯
        dt = datetime(*t[:6], tzinfo=timezone.utc)
        age = (datetime.now(timezone.utc) - dt).total_seconds() / 3600
        if dt < cutoff:
            return False
        return True
    except:
        return False

def is_relevant(entry: dict, is_twitter: bool = False) -> bool:
    text = " ".join([
        str(entry.get("title", "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()
    if is_twitter:
        tw_kw = ["iran","israel","idf","irgc","strike","war","attack","missile",
                 "drone","military","nuclear","hezbollah","hamas","houthi",
                 "centcom","pentagon","gaza","lebanon","tehran","netanyahu","khamenei"]
        return any(k in text for k in tw_kw)
    return any(k in text for k in KEYWORDS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ÙÛŒØ¯Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_one_rss(client: httpx.AsyncClient, cfg: dict) -> list:
    url = cfg["url"]
    try:
        r = await client.get(url, timeout=httpx.Timeout(12.0),
                             headers={"User-Agent": "Mozilla/5.0 MilNewsBot/9.0"})
        if r.status_code == 200:
            entries = feedparser.parse(r.text).entries
            if entries:
                return entries
    except:
        pass
    return []

async def fetch_one_twitter(client: httpx.AsyncClient, cfg: dict) -> tuple[list, str]:
    """âœ… FIX: RSSHub Ø¨Ø§ fallback â€” Ù‡Ù…Ù‡ instance Ù‡Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯"""
    for url in cfg["twitter_urls"]:
        try:
            r = await client.get(url, timeout=httpx.Timeout(10.0),
                                 headers={"User-Agent": "Mozilla/5.0 MilNewsBot/9.0"})
            if r.status_code == 200:
                entries = feedparser.parse(r.text).entries
                if entries:
                    log.debug(f"  ğ• {cfg['twitter_handle']} â† {url.split('/')[2]}")
                    return entries, cfg["name"]
        except:
            continue
    return [], cfg["name"]

async def fetch_all(client: httpx.AsyncClient) -> list:
    # RSS Ùˆ Google News Ù‡Ù…Ø²Ù…Ø§Ù†
    rss_tasks = [fetch_one_rss(client, cfg) for cfg in ALL_FEEDS]
    rss_results = await asyncio.gather(*rss_tasks, return_exceptions=True)

    out = []
    for i, res in enumerate(rss_results):
        if isinstance(res, list):
            for entry in res:
                out.append((entry, ALL_FEEDS[i], False))

    # Twitter Ù‡Ù…Ø²Ù…Ø§Ù†
    tw_tasks = [fetch_one_twitter(client, cfg) for cfg in TWITTER_FEEDS]
    tw_results = await asyncio.gather(*tw_tasks, return_exceptions=True)

    tw_ok = 0
    for res in tw_results:
        if isinstance(res, tuple):
            entries, name = res
            for entry in entries:
                fake_cfg = {"name": name}
                out.append((entry, fake_cfg, True))
            if entries:
                tw_ok += 1

    log.info(f"  ğ• ØªÙˆÛŒÛŒØªØ±: {tw_ok}/{len(TWITTER_FEEDS)} Ø§Ú©Ø§Ù†Øª Ù…ÙˆÙÙ‚")
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Gemini
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

async def translate(client: httpx.AsyncClient, title: str, summary: str) -> tuple[str, str]:
    if not GEMINI_API_KEY or len(title.strip()) < 3:
        return title, summary

    prompt = f"""ÙˆØ¸ÛŒÙÙ‡: ØªØ±Ø¬Ù…Ù‡ Ø¯Ù‚ÛŒÙ‚ Ø®Ø¨Ø± Ù†Ø¸Ø§Ù…ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù†.
Ø²Ø¨Ø§Ù† ÙˆØ±ÙˆØ¯ÛŒ: Ù‡Ø± Ø²Ø¨Ø§Ù†ÛŒ (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø¹Ø¨Ø±ÛŒØŒ Ø¹Ø±Ø¨ÛŒ...)
Ø®Ø±ÙˆØ¬ÛŒ: ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ â€” Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ØŒ Ø¨Ø¯ÙˆÙ† Ù¾Ø±Ø§Ù†ØªØ²

Ù‚ÙˆØ§Ù†ÛŒÙ†:
Û±. ÙÙ‚Ø· ØªØ±Ø¬Ù…Ù‡ØŒ Ù‡ÛŒÚ† Ú†ÛŒØ² Ø§Ø¶Ø§ÙÙ‡
Û². Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ Ø±Ø§ Ø­ÙØ¸ Ú©Ù† (Ù†ØªØ§Ù†ÛŒØ§Ù‡ÙˆØŒ Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒØŒ Ù†Ø§ØªÙˆØŒ IRGC...)
Û³. Ù„Ø­Ù† Ø±Ø³Ù…ÛŒ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ
Û´. Ø§Ú¯Ø± Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªØŒ ØªØ±Ø¬Ù…Ù‡ Ú©ÙˆØªØ§Ù‡ Ø¨Ù†ÙˆÛŒØ³

ÙØ±Ù…Øª Ø¯Ù‚ÛŒÙ‚:
Ø¹Ù†ÙˆØ§Ù†: [ØªØ±Ø¬Ù…Ù‡]
---
Ù…ØªÙ†: [ØªØ±Ø¬Ù…Ù‡]

===
Ø¹Ù†ÙˆØ§Ù†: {title[:400]}
Ù…ØªÙ†: {summary[:700]}"""

    for attempt in range(2):
        try:
            r = await client.post(
                f"{GEMINI_URL}?key={GEMINI_API_KEY}",
                json={
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": {"temperature": 0.05, "maxOutputTokens": 1024}
                },
                timeout=httpx.Timeout(25.0)
            )
            if r.status_code == 200:
                raw = r.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
                raw = re.sub(r'^(Ø¹Ù†ÙˆØ§Ù†|Ù…ØªÙ†):\s*', '', raw, flags=re.MULTILINE)
                raw = raw.replace("**", "").replace("*", "")
                parts = raw.split("---", 1)
                if len(parts) == 2:
                    return nfa(parts[0].strip()), nfa(parts[1].strip())
                return nfa(raw.strip()), ""
            elif r.status_code == 429:
                wait = int(r.headers.get("Retry-After", 20))
                log.warning(f"â³ Gemini rate limit {wait}s")
                await asyncio.sleep(wait)
            else:
                log.debug(f"Gemini {r.status_code}")
                break
        except Exception as e:
            log.debug(f"Gemini: {e}")
            if attempt == 0:
                await asyncio.sleep(3)

    return title, summary

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(text: str) -> str:
    if not text: return ""
    return BeautifulSoup(str(text), "html.parser").get_text(" ", strip=True)

def make_id(entry: dict) -> str:
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def make_title_id(title: str) -> str:
    """Ø¶Ø¯ ØªÚ©Ø±Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù† â€” Ø§Ø² Ú†Ù†Ø¯ Ù…Ù†Ø¨Ø¹ Ù…Ø®ØªÙ„Ù"""
    t = re.sub(r'[^a-z0-9\u0600-\u06FF]', '', title.lower())
    return "t:" + hashlib.md5(t[:200].encode("utf-8")).hexdigest()

def format_dt(entry: dict) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except: pass
    return ""

def esc(t: str) -> str:
    return (t or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def trim(t: str, n: int = 700) -> str:
    t = re.sub(r'\s+', ' ', t).strip()
    return t if len(t) <= n else t[:n].rsplit(" ", 1)[0] + "â€¦"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ø§ÙØ¸Ù‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen: set):
    with open(SEEN_FILE, "w") as f:
        json.dump(list(seen)[-12000:], f)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client: httpx.AsyncClient, text: str) -> bool:
    for attempt in range(4):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID,
                "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            }, timeout=httpx.Timeout(15.0))
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                wait = data.get("parameters", {}).get("retry_after", 20)
                log.warning(f"â³ TG rate limit {wait}s")
                await asyncio.sleep(wait)
            elif data.get("error_code") in (400, 403):
                log.error(f"TG fatal: {data.get('description')}")
                return False
            else:
                await asyncio.sleep(5)
        except Exception as e:
            log.warning(f"TG #{attempt+1}: {e}")
            await asyncio.sleep(8)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡!"); return

    seen = load_seen()
    cutoff = get_cutoff()
    tehran_cutoff = cutoff.astimezone(TEHRAN_TZ).strftime('%Y/%m/%d %H:%M')
    log.info(f"ğŸš€ {len(RSS_FEEDS)+len(GOOGLE_FEEDS)} RSS/GNews + {len(TWITTER_FEEDS)} ØªÙˆÛŒÛŒØªØ±")
    log.info(f"ğŸ“… Cutoff: Ø¢Ø®Ø± Û² Ø³Ø§Ø¹Øª ({tehran_cutoff} ØªÙ‡Ø±Ø§Ù† Ø¨Ù‡ Ø¨Ø¹Ø¯)")
    log.info(f"ğŸ’¾ Ø­Ø§ÙØ¸Ù‡: {len(seen)} Ø®Ø¨Ø± Ù‚Ø¨Ù„ÛŒ")

    async with httpx.AsyncClient(follow_redirects=True) as client:

        # â”€â”€ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù† â”€â”€
        log.info("â¬ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù†...")
        raw = await fetch_all(client)
        log.info(f"ğŸ“¥ {len(raw)} Ø¢ÛŒØªÙ… Ø®Ø§Ù…")

        # â”€â”€ ÙÛŒÙ„ØªØ± â”€â”€
        collected = []
        title_seen = set()
        old_count = 0
        irrel_count = 0

        for entry, cfg, is_tw in raw:
            eid = make_id(entry)
            if eid in seen:
                continue
            if not is_fresh(entry):
                seen.add(eid)
                old_count += 1
                continue
            if not is_relevant(entry, is_twitter=is_tw):
                seen.add(eid)
                irrel_count += 1
                continue
            raw_title = clean_html(entry.get("title", ""))
            tid = make_title_id(raw_title)
            if tid in title_seen:
                seen.add(eid)
                continue
            title_seen.add(tid)
            collected.append((eid, entry, cfg, is_tw))

        log.info(f"ğŸ“Š ÙÛŒÙ„ØªØ±: {old_count} Ù‚Ø¯ÛŒÙ…ÛŒ | {irrel_count} Ù†Ø§Ù…Ø±ØªØ¨Ø· | {len(collected)} Ø¬Ø¯ÛŒØ¯")

        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ MAX_NEW_PER_RUN (Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø§ÙˆÙ„)
        collected = list(reversed(collected))
        if len(collected) > MAX_NEW_PER_RUN:
            log.warning(f"âš ï¸ {len(collected)} â†’ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ {MAX_NEW_PER_RUN}")
            collected = collected[-MAX_NEW_PER_RUN:]

        # â”€â”€ ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„ â”€â”€
        sent = 0
        for eid, entry, cfg, is_tw in collected:
            en_title = trim(clean_html(entry.get("title", "")), 300)
            en_sum   = trim(clean_html(entry.get("summary") or entry.get("description") or ""), 700)
            link     = entry.get("link", "")
            dt       = format_dt(entry)
            icon     = "ğ•" if is_tw else "ğŸ“¡"

            log.info(f"ğŸ”„ {en_title[:55]}...")
            fa_title, fa_sum = await translate(client, en_title, en_sum)

            # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù…
            lines = [f"ğŸ”´ <b>{esc(fa_title)}</b>", ""]
            if fa_sum and len(fa_sum) > 10 and fa_sum.lower() not in fa_title.lower():
                lines += [esc(fa_sum), ""]
            lines += ["â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€", f"ğŸ“Œ <i>{esc(en_title)}</i>"]
            if dt:    lines.append(dt)
            lines.append(f"{icon} <b>{cfg['name']}</b>")
            if link:  lines.append(f'ğŸ”— <a href="{link}">Ù…Ù†Ø¨Ø¹</a>')

            if await tg_send(client, "\n".join(lines)):
                seen.add(eid)
                sent += 1
                log.info("  âœ… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")
            else:
                log.error("  âŒ Ø§Ø±Ø³Ø§Ù„ Ù†Ø§Ù…ÙˆÙÙ‚")
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"ğŸ Ù¾Ø§ÛŒØ§Ù† | {sent}/{len(collected)} Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")

if __name__ == "__main__":
    asyncio.run(main())
