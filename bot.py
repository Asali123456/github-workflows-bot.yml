import os, json, hashlib, asyncio, logging, re, io
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_OK = True
except ImportError:
    PIL_OK = False

try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("WarBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE         = "seen.json"
STORIES_FILE      = "stories.json"
GEMINI_STATE_FILE = "gemini_state.json"
FLIGHT_ALERT_FILE = "flight_alerts.json"
RUN_STATE_FILE    = "run_state.json"
NITTER_CACHE_FILE = "nitter_cache.json"

# â”€â”€ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù‡Ø± Ø§Ø¬Ø±Ø§ ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± ØªØ§Ø²Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯
CUTOFF_BUFFER_MIN  = 2    # buffer Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² miss (Ú©ÙˆÚ†Ú©â€ŒØªØ± = Ø³Ø±ÛŒØ¹â€ŒØªØ±)
MAX_LOOKBACK_MIN   = 12   # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ø±Ú¯Ø´Øª â€” Ú©Ù…ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø² cron interval (10min)
SEEN_TTL_HOURS     = 6
NITTER_CACHE_TTL   = 900

MAX_NEW_PER_RUN    = 30   # Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§Ø² Ø¯Ø³Øª Ù†Ø±ÙØªÙ† Ø®Ø¨Ø±
MAX_MSG_LEN        = 4096
SEND_DELAY         = 0.5
JACCARD_THRESHOLD  = 0.72  # Ø¢Ø²Ø§Ø¯ØªØ± â€” ÙÙ‚Ø· Ø®Ø¨Ø±Ù‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù† Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
MAX_STORIES        = 120   # Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± â†’ Ø®Ø¨Ø±Ù‡Ø§ÛŒ ØªØ§Ø²Ù‡ Ø¨ÛŒØ´ØªØ±
RSS_TIMEOUT        = 7.0
TG_TIMEOUT         = 10.0
TW_TIMEOUT         = 5.0
RICH_CARD_THRESHOLD = 7

TEHRAN_TZ = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ù†Ø§Ø¨Ø¹ RSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IRAN_FEEDS = [
    {"n":"ğŸ‡®ğŸ‡· IRNA English",       "u":"https://en.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Mehr News EN",        "u":"https://en.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tasnim News EN",      "u":"https://www.tasnimnews.com/en/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Fars News EN",        "u":"https://www.farsnews.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Press TV",            "u":"https://www.presstv.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· ISNA English",        "u":"https://en.isna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tehran Times",        "u":"https://www.tehrantimes.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran International", "u":"https://www.iranintl.com/en/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran Wire EN",        "u":"https://iranwire.com/en/feed/"},
    {"n":"ğŸ‡®ğŸ‡· Radio Farda EN",      "u":"https://en.radiofarda.com/api/zqpqetrruqo"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ ØªØ³Ù†ÛŒÙ…",      "u":"https://www.tasnimnews.com/fa/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ù…Ù‡Ø±",         "u":"https://www.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø§ÛŒØ±Ù†Ø§",       "u":"https://www.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ ÙØ§Ø±Ø³",        "u":"https://www.farsnews.ir/rss/fa"},
    {"n":"ğŸ‡®ğŸ‡· Ù…Ø´Ø±Ù‚ Ù†ÛŒÙˆØ²",             "u":"https://www.mashreghnews.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø¯ÙØ§Ø¹ Ù¾Ø±Ø³",             "u":"https://www.defapress.ir/fa/rss"},
    {"n":"ğŸ‡®ğŸ‡· YJC Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ",         "u":"https://www.yjc.ir/fa/rss/allnews"},
    # Google News â€” Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚
    {"n":"ğŸ“° GNews Iran War",      "u":"https://news.google.com/rss/search?q=Iran+war+attack+military&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GNews IRGC",          "u":"https://news.google.com/rss/search?q=IRGC+Iran+Israel+US+military&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GNews Ø§ÛŒØ±Ø§Ù† Ø­Ù…Ù„Ù‡",     "u":"https://news.google.com/rss/search?q=Ø§ÛŒØ±Ø§Ù†+Ø­Ù…Ù„Ù‡+Ù…ÙˆØ´Ú©+Ø§Ø³Ø±Ø§ÛŒÛŒÙ„+Ø¢Ù…Ø±ÛŒÚ©Ø§&hl=fa&gl=IR&ceid=IR:fa&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GNews Ø³Ù¾Ø§Ù‡",          "u":"https://news.google.com/rss/search?q=Ø³Ù¾Ø§Ù‡+Ù¾Ø§Ø³Ø¯Ø§Ø±Ø§Ù†+Ø¹Ù…Ù„ÛŒØ§Øª&hl=fa&gl=IR&ceid=IR:fa&num=10&tbs=qdr:d"},
]
ISRAEL_FEEDS = [
    # âŒ jpost.com/rss/rssfeedsheadlines.aspx Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨ÙˆØ¯ â€” Ø¢Ø¯Ø±Ø³ ØµØ­ÛŒØ­:
    {"n":"ğŸ‡®ğŸ‡± Jerusalem Post",       "u":"https://rss.jpost.com/rss/rssfeedsheadlines"},
    {"n":"ğŸ‡®ğŸ‡± Times of Israel",      "u":"https://www.timesofisrael.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± Haaretz EN",           "u":"https://www.haaretz.com/srv/haaretz-latest-articles.rss"},
    {"n":"ğŸ‡®ğŸ‡± Israel Hayom EN",      "u":"https://www.israelhayom.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± Arutz Sheva",          "u":"https://www.israelnationalnews.com/rss.aspx"},
    {"n":"ğŸ‡®ğŸ‡± i24 News",             "u":"https://www.i24news.tv/en/rss"},
    {"n":"ğŸ“° GNews Israel Iran",    "u":"https://news.google.com/rss/search?q=Israel+Iran+attack+strike&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GNews IDF",            "u":"https://news.google.com/rss/search?q=IDF+military+operation+Iran&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GNews Iron Dome",      "u":"https://news.google.com/rss/search?q=Iron+Dome+Arrow+missile+defense&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]
USA_FEEDS = [
    # âŒ feeds.reuters.com â€” Ø§Ø² Ú˜ÙˆØ¦Ù† Û²Û°Û²Û° Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡ØŒ Ø­Ø°Ù Ø´Ø¯
    # âŒ feeds.apnews.com/rss/apf-WorldNews â€” Ù‚Ø¯ÛŒÙ…ÛŒØŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯
    {"n":"ğŸ‡ºğŸ‡¸ AP World",             "u":"https://apnews.com/hub/world-news.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ AP Middle East",       "u":"https://apnews.com/hub/middle-east.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ CNN Middle East",      "u":"http://rss.cnn.com/rss/edition_meast.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ NBC World News",       "u":"https://feeds.nbcnews.com/feeds/worldnews"},
    {"n":"ğŸ‡ºğŸ‡¸ USNI News",            "u":"https://news.usni.org/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Breaking Defense",     "u":"https://breakingdefense.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ The War Zone",         "u":"https://www.twz.com/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Defense News",         "u":"https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"n":"ğŸ‡ºğŸ‡¸ Military Times",       "u":"https://www.militarytimes.com/arc/outboundfeeds/rss/"},
    {"n":"ğŸ‡ºğŸ‡¸ Long War Journal",     "u":"https://www.longwarjournal.org/feed"},
    {"n":"ğŸ“° GNews US Iran",        "u":"https://news.google.com/rss/search?q=US+military+Iran+strike+sanction&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GNews CENTCOM",        "u":"https://news.google.com/rss/search?q=CENTCOM+Middle+East+military+operation&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GNews Houthi",         "u":"https://news.google.com/rss/search?q=Houthi+Iran+Red+Sea+US+Navy&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
    {"n":"âš ï¸ GNews Nuclear",         "u":"https://news.google.com/rss/search?q=Iran+nuclear+uranium+IAEA+enrichment&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]
EMBASSY_FEEDS = [
    {"n":"ğŸ›ï¸ US State Travel",      "u":"https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html.rss"},
    {"n":"ğŸ›ï¸ UK FCDO Iran",         "u":"https://www.gov.uk/foreign-travel-advice/iran.atom"},
    {"n":"ğŸ›ï¸ Embassy Evacuation",   "u":"https://news.google.com/rss/search?q=embassy+evacuation+Iran+warning+2026&hl=en-US&gl=US&ceid=US:en&num=10"},
]
INTL_FEEDS = [
    {"n":"ğŸŒ BBC Middle East",  "u":"https://feeds.bbci.co.uk/news/world/middle_east/rss.xml"},
    {"n":"ğŸŒ Al Jazeera",       "u":"https://www.aljazeera.com/xml/rss/all.xml"},
    {"n":"ğŸŒ Middle East Eye",  "u":"https://www.middleeasteye.net/rss"},
    {"n":"ğŸŒ Foreign Policy",   "u":"https://foreignpolicy.com/feed/"},
    {"n":"ğŸŒ The Guardian ME",  "u":"https://www.theguardian.com/world/middleeast/rss"},
]

ALL_RSS_FEEDS = IRAN_FEEDS + ISRAEL_FEEDS + USA_FEEDS + EMBASSY_FEEDS + INTL_FEEDS
EMBASSY_SET   = {id(f) for f in EMBASSY_FEEDS}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X handles
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_HANDLES = [
    # â”€â”€â”€ OSINT / Breaking â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # âŒ "OSINTdefender" Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨ÙˆØ¯ â€” handle ÙˆØ§Ù‚Ø¹ÛŒ @sentdefender Ø§Ø³Øª
    ("ğŸ” OSINTdefender",        "sentdefender"),
    ("ğŸ” OSINTtechnical",       "Osinttechnical"),
    ("ğŸ” IntelCrab",            "IntelCrab"),
    ("ğŸ” GeoConfirmed",         "GeoConfirmed"),
    ("ğŸ” WarMonitor",           "WarMonitor3"),
    ("ğŸ” AuroraIntel",          "AuroraIntel"),
    ("ğŸ” Faytuks",              "Faytuks"),
    ("ğŸ” Clash Report",         "clashreport"),
    ("ğŸ” Megatron",             "Megatron_Ron"),
    ("ğŸ” ELINT News",           "ELINTNews"),
    ("ğŸ” War Zone TW",          "TheWarZoneTW"),
    # â”€â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¯ÙˆÙ„ØªÛŒ / Ù†Ø¸Ø§Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡ºğŸ‡¸ CENTCOM",              "CENTCOM"),
    ("ğŸ‡ºğŸ‡¸ DoD",                  "DeptofDefense"),
    ("ğŸ‡ºğŸ‡¸ Natasha Bertrand",     "NatashaBertrand"),
    ("ğŸ‡ºğŸ‡¸ Barak Ravid",          "BarakRavid"),
    ("ğŸ‡ºğŸ‡¸ Idrees Ali",           "idreesali114"),
    ("ğŸ‡ºğŸ‡¸ Jack Detsch",          "JackDetsch"),
    ("ğŸ‡ºğŸ‡¸ Lara Seligman",        "laraseligman"),
    ("ğŸ‡ºğŸ‡¸ Jim Sciutto",          "jimsciutto"),
    # â”€â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡± IDF",                  "IDF"),
    ("ğŸ‡®ğŸ‡± Israeli PM",           "IsraeliPM"),
    ("ğŸ‡®ğŸ‡± Yossi Melman",         "yossi_melman"),
    ("ğŸ‡®ğŸ‡± Seth Frantzman",       "sfrantzman"),
    ("ğŸ‡®ğŸ‡± Emanuel Fabian",       "manniefabian"),
    ("ğŸ‡®ğŸ‡± Anna Ahronheim",       "AAhronheim"),
    # â”€â”€â”€ Ø§ÛŒØ±Ø§Ù† / Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡· IranIntl EN",          "IranIntl_En"),
    ("ğŸ‡®ğŸ‡· IRNA EN",              "IRNA_English"),
    ("ğŸ‡®ğŸ‡· Press TV",             "PressTV"),
    ("ğŸ‡®ğŸ‡· Farnaz Fassihi",       "farnazfassihi"),
    ("ğŸ‡®ğŸ‡· Kasra Aarabi",         "KasraAarabi"),
    # â”€â”€â”€ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Brk",       "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ AFP News",              "AFPnews"),
    # â”€â”€â”€ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ” Ian Bremmer",           "ianbremmer"),
    ("ğŸ” Ellie Geranmayeh",      "EllieGeranmayeh"),
    ("ğŸ” Michael Knights",       "Mikeknightsiraq"),
    ("ğŸ” Aric Toler",            "AricToler"),
    ("âš ï¸ DEFCONLevel",           "DEFCONLevel"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TELEGRAM_CHANNELS = [
    # OSINT â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
    ("ğŸ”´ Middle East Spectator", "Middle_East_Spectator"),
    ("ğŸ”´ Intel Slava Z",         "intelslava"),
    ("ğŸ”´ ELINT News",            "ELINTNews"),
    ("ğŸ”´ Clash Report",          "ClashReport"),
    ("ğŸ”´ Megatron OSINT",        "Megatron_Ron"),
    ("ğŸ”´ Disclose TV",           "disclosetv"),
    ("ğŸ” OSINTtechnical",        "Osinttechnical"),
    ("ğŸ” Aurora Intel",          "Aurora_Intel"),
    ("ğŸ” War Monitor",           "WarMonitor3"),
    # Ø§ÛŒØ±Ø§Ù† ÙØ§Ø±Ø³ÛŒ
    # âŒ "IranIntlPersian" Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨ÙˆØ¯ â€” handle ÙˆØ§Ù‚Ø¹ÛŒ @IranintlTV Ø§Ø³Øª (Û± Ù…ÛŒÙ„ÛŒÙˆÙ† Ø¹Ø¶Ùˆ)
    ("ğŸ‡®ğŸ‡· Iran Intl Persian",   "IranintlTV"),
    ("ğŸ‡®ğŸ‡· ØªØ³Ù†ÛŒÙ… ÙØ§Ø±Ø³ÛŒ",          "tasnimnewsfa"),
    ("ğŸ‡®ğŸ‡· Ù…Ù‡Ø± ÙØ§Ø±Ø³ÛŒ",             "mehrnews_fa"),
    ("ğŸ‡®ğŸ‡· Ø§ÛŒØ±Ù†Ø§ ÙØ§Ø±Ø³ÛŒ",           "irnafarsi"),
    ("ğŸ‡®ğŸ‡· Press TV",              "PressTVnews"),
    # Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    ("ğŸ‡®ğŸ‡± Kann News",            "kann_news"),
    ("ğŸ‡®ğŸ‡± Times of Israel",      "timesofisrael"),
    # Ù…Ù†Ø·Ù‚Ù‡
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Breaking",  "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸ‡¾ğŸ‡² Masirah TV",           "AlMasirahNet"),
    ("ğŸ‡±ğŸ‡§ Naharnet",             "Naharnet"),
    # Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ GeoConfirmed",          "GeoConfirmed"),
    ("ğŸŒ IntelCrab",             "IntelCrab"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÙÛŒÙ„ØªØ± Ù…ÙˆØ¶ÙˆØ¹ÛŒ â€” Ø¢Ø²Ø§Ø¯: Ù‡Ø± Ø®Ø¨Ø± Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒØ±Ø§Ù†ØŒ Ø¢Ù…Ø±ÛŒÚ©Ø§ ÛŒØ§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IRAN_KW = [
    "iran","iranian","irgc","islamic republic","khamenei","tehran","persian gulf",
    "sepah","basij","quds force","rouhani","raisi","pezeshkian","zarif","araghchi",
    "Ø§ÛŒØ±Ø§Ù†","Ø³Ù¾Ø§Ù‡","Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ","ØªÙ‡Ø±Ø§Ù†","Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ","Ù¾Ø²Ø´Ú©ÛŒØ§Ù†","Ø¸Ø±ÛŒÙ","Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ",
]
USA_KW = [
    "united states","us military","pentagon","centcom","white house","biden","trump",
    "us navy","us air force","us army","cia","state department","secretary of state",
    "u.s.","u.s. navy","u.s. military","u.s. forces","american forces","american military",
    "american troops","carrier strike","uss ", "rubio","austin","hegseth","blinken",
    "american carrier","us carrier","us warship","us troops","us forces","us base",
    "Ø¢Ù…Ø±ÛŒÚ©Ø§","Ù¾Ù†ØªØ§Ú¯ÙˆÙ†","Ú©Ø§Ø® Ø³ÙÛŒØ¯","Ø¨Ø§ÛŒØ¯Ù†","ØªØ±Ø§Ù…Ù¾","Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø³ÛŒØ§","ÙˆØ²Ø§Ø±Øª Ø®Ø§Ø±Ø¬Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§",
]
ISRAEL_KW = [
    "israel","israeli","idf","netanyahu","tel aviv","mossad","iron dome","arrow",
    "iaf","israeli air force","knesset","bennett","herzog","gallant",
    "Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ","ØªÙ„â€ŒØ¢ÙˆÛŒÙˆ","Ù…ÙˆØ³Ø§Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†",
]
PROXY_KW = [
    "hamas","hezbollah","houthi","pij","islamic jihad","ansar allah",
    "Ø­Ù…Ø§Ø³","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡","Ø­ÙˆØ«ÛŒ","Ø¬Ù‡Ø§Ø¯ Ø§Ø³Ù„Ø§Ù…ÛŒ","Ø§Ù†ØµØ§Ø±Ø§Ù„Ù„Ù‡",
]
HARD_EXCLUDE = [
    "football","soccer","basketball","olympic","sport","cooking","recipe",
    "fashion","celebrity","entertainment","music award","box office","nba","nfl",
    "ÙÙˆØªØ¨Ø§Ù„","Ø³ÛŒÙ†Ù…Ø§","Ù…ÙˆØ³ÛŒÙ‚ÛŒ","Ø¢Ø´Ù¾Ø²ÛŒ","Ù…Ø¯ Ùˆ Ù„Ø¨Ø§Ø³",
]
EMBASSY_OVERRIDE = [
    "evacuate","leave immediately","travel warning","security alert","emergency",
    "ØªØ®Ù„ÛŒÙ‡","ÙÙˆØ±ÛŒ ØªØ±Ú©","Ù‡Ø´Ø¯Ø§Ø± Ø§Ù…Ù†ÛŒØªÛŒ","Ø§Ø¶Ø·Ø±Ø§Ø±",
]

def is_war_relevant(text, is_embassy=False, is_tg=False, is_tw=False):
    """
    ÙÛŒÙ„ØªØ± Ø¢Ø²Ø§Ø¯ v18:
    Ù‡Ø± Ø®Ø¨Ø±ÛŒ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø§ÛŒØ±Ø§Ù†ØŒ Ø¢Ù…Ø±ÛŒÚ©Ø§ ÛŒØ§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø¨Ø§Ø´Ø¯ â€” Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    (Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ú©Ù„Ù…Ù‡ Ø¹Ù…Ù„/action)
    """
    txt = text.lower()
    # Ø­Ø°Ù Ù‚Ø·Ø¹ÛŒ
    if any(k in txt for k in HARD_EXCLUDE):
        return False
    # Ø³ÙØ§Ø±Øª: Ù‡Ù…ÛŒØ´Ù‡ pass
    if is_embassy and any(k in txt for k in EMBASSY_OVERRIDE):
        return True
    # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¶ÙˆØ± Ù‡Ø± ÛŒÚ© Ø§Ø² Ø³Ù‡ Ø·Ø±Ù Ø§ØµÙ„ÛŒ
    has_iran   = any(k in txt for k in IRAN_KW)
    has_usa    = any(k in txt for k in USA_KW)
    has_israel = any(k in txt for k in ISRAEL_KW)
    has_proxy  = any(k in txt for k in PROXY_KW)
    # Ù‡Ø± Ø®Ø¨Ø± Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø·Ø±Ù Ø§ØµÙ„ÛŒ = Ø§Ø±Ø³Ø§Ù„
    return has_iran or has_usa or has_israel or has_proxy

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X â€” Feb 2026 â€” ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª Ø§Ø² Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† uptime Ø¯Ø± GitHub Actions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…: Ø§Ú©Ø«Ø± Nitter instances Ø¯Ø± GitHub Actions IPs Ø¨Ù„Ø§Ú© Ù‡Ø³ØªÙ†Ø¯
# RSSHub Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¹ØªÙ…Ø§Ø¯ØªØ± Ø§Ø³Øª â€” Ø§Ø² Ø¢Ù† Ø§Ø¨ØªØ¯Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
RSSHUB_INSTANCES = [
    "https://rsshub.app",               # âœ… Ø§ØµÙ„ÛŒ â€” Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ±ÛŒÙ†
    "https://rsshub.rss.now.sh",       # âœ… mirror
    "https://rss.shab.fun",            # backup
    "https://rsshub.moeyy.xyz",        # backup
    "https://hub.slar.ru",             # backup
]
NITTER_INSTANCES = [
    "https://rss.xcancel.com",         # âœ… subdomain Ù…Ø³ØªÙ‚ÛŒÙ…
    "https://xcancel.com",             # âœ… redirect
    "https://nitter.poast.org",        # âœ… Ø§ØºÙ„Ø¨ Ø¯Ø± CI Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    "https://nitter.privacyredirect.com",
    "https://nitter.tiekoetter.com",
    "https://lightbrd.com",
    "https://nitter.catsarch.com",
    "https://n.ramle.be",
    "https://nitter.space",
    "https://nitter.net",
    "https://nitter.it",
    "https://nitter.unixfox.eu",
]

NITTER_HDR = {
    "User-Agent": "Mozilla/5.0 (compatible; Feedfetcher-Google; +http://www.google.com/feedfetcher.html)",
    "Accept": "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Cache-Control": "no-cache",
}
COMMON_UA = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

_nitter_pool: list[str]  = []
_rsshub_pool: list[str]  = []
_TW_SEMA: asyncio.Semaphore | None = None

def _load_nitter_cache() -> tuple[list, list, float]:
    try:
        if Path(NITTER_CACHE_FILE).exists():
            d = json.load(open(NITTER_CACHE_FILE))
            return d.get("nitter", []), d.get("rsshub", []), d.get("ts", 0.0)
    except: pass
    return [], [], 0.0

def _save_nitter_cache(nitter, rsshub):
    json.dump({"nitter": nitter, "rsshub": rsshub,
               "ts": datetime.now(timezone.utc).timestamp()},
              open(NITTER_CACHE_FILE, "w"))

def _is_rss(body: str, ct: str) -> bool:
    b = body[:600].lower()
    return ("xml" in ct) or ("<rss" in b) or ("<?xml" in b) or ("<feed" in b)

async def _try_rss(client: httpx.AsyncClient, url: str, timeout: float = TW_TIMEOUT) -> list:
    """
    RSS URL Ø±Ø§ fetch Ú©Ø±Ø¯Ù‡ entries Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    follow_redirects=True Ù…Ù‡Ù… Ø§Ø³Øª (xcancel.com â†’ rss.xcancel.com)
    """
    try:
        r = await client.get(url,
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=timeout,
                                                   write=5.0, pool=5.0))
        if r.status_code not in (200, 304):
            return []
        ct = r.headers.get("content-type", "")
        body = r.text or ""
        if not _is_rss(body, ct):
            return []
        parsed = feedparser.parse(body)
        entries = getattr(parsed, "entries", []) or []
        return [e for e in entries if len((e.get("title") or "").strip()) > 3]
    except Exception:
        return []

async def _probe_instance(client: httpx.AsyncClient, url: str,
                          handle: str = "OSINTdefender") -> tuple | None:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ÛŒÚ© instance ÙˆØ§Ù‚Ø¹Ø§Ù‹ RSS Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ù…Ù‡Ù…: ÙÙ‚Ø· Ø³Ø§Ø®ØªØ§Ø± RSS Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù†Ù‡ ØªØ¹Ø¯Ø§Ø¯ entries.
    """
    t0 = asyncio.get_running_loop().time()
    try:
        r = await client.get(f"{url}/{handle}/rss",
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=7.0,
                                                   write=5.0, pool=5.0))
        if r.status_code not in (200, 304):
            return None
        ct   = r.headers.get("content-type", "")
        body = r.text or ""
        # ÙÙ‚Ø· Ú†Ú© Ø³Ø§Ø®ØªØ§Ø± â€” Ù†Ù‡ entries
        if _is_rss(body, ct):
            ms = (asyncio.get_running_loop().time() - t0) * 1000
            return url, ms
    except Exception:
        pass
    return None

async def _probe_rsshub(client: httpx.AsyncClient, inst: str) -> tuple | None:
    t0 = asyncio.get_running_loop().time()
    try:
        r = await client.get(f"{inst}/twitter/user/OSINTdefender",
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=8.0,
                                                   write=5.0, pool=5.0))
        if r.status_code in (200, 304) and _is_rss(r.text or "", r.headers.get("content-type","")):
            ms = (asyncio.get_running_loop().time() - t0) * 1000
            return inst, ms
    except Exception:
        pass
    return None

async def build_twitter_pools(client: httpx.AsyncClient):
    """
    Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡: probe Ø­Ø°Ù Ø´Ø¯.
    Ù‡Ù…Ù‡ fetch_twitter Ù…Ø³ØªÙ‚ÛŒÙ… RSSHub â†’ Nitter Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.
    ÙÙ‚Ø· cache Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… Ú©Ù‡ Ø¢Ø®Ø±ÛŒÙ† instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø¨ÛŒØ§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
    """
    global _nitter_pool, _rsshub_pool
    cached_n, cached_r, ts = _load_nitter_cache()
    age = datetime.now(timezone.utc).timestamp() - ts
    # Ø§Ú¯Ù‡ cache Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª: Ø¢Ø®Ø±ÛŒÙ† instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø§ÙˆÙ„ Ø¨Ú¯Ø°Ø§Ø±
    if age < NITTER_CACHE_TTL:
        if cached_r: _rsshub_pool = cached_r + [i for i in RSSHUB_INSTANCES if i not in cached_r]
        if cached_n: _nitter_pool = cached_n + [i for i in NITTER_INSTANCES if i not in cached_n]
    if not _rsshub_pool: _rsshub_pool = list(RSSHUB_INSTANCES)
    if not _nitter_pool: _nitter_pool = list(NITTER_INSTANCES)
    log.info(f"ğ• pools: RSSHub={len(_rsshub_pool)} Nitter={len(_nitter_pool)}")

async def fetch_twitter(client: httpx.AsyncClient, label: str, handle: str) -> list:
    """
    Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§:
    1. RSSHub (Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ø¯Ø± GitHub Actions CI)
    2. Nitter instances
    Ø§ÙˆÙ„ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆÙÙ‚ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¯ÙØ¹Ù‡ Ø¨Ø¹Ø¯ Ø§ÙˆÙ„ Ø§Ù…ØªØ­Ø§Ù† Ø´ÙˆØ¯.
    """
    sema = _TW_SEMA or asyncio.Semaphore(15)
    async with sema:
        # â”€â”€ RSSHub Ø§ÙˆÙ„ (Ø¯Ø± CI Ø¨Ù‡ØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for inst in (_rsshub_pool or RSSHUB_INSTANCES):
            for path in (f"/twitter/user/{handle}", f"/x/user/{handle}"):
                e = await _try_rss(client, f"{inst}{path}", timeout=8.0)
                if e:
                    log.debug(f"ğ• {handle} â† RSSHub {inst.split('//')[-1]} ({len(e)})")
                    # Ø§ÛŒÙ† instance Ø±Ø§ Ø¨Ù‡ Ø§ÙˆÙ„ cache Ø¨ÙØ±Ø³Øª
                    _update_pool_cache(inst, is_rsshub=True)
                    return [(x, f"ğ• {label}", "tw", False) for x in e]

        # â”€â”€ Nitter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for inst in (_nitter_pool or NITTER_INSTANCES):
            e = await _try_rss(client, f"{inst}/{handle}/rss", timeout=6.0)
            if e:
                log.debug(f"ğ• {handle} â† Nitter {inst.split('//')[-1]} ({len(e)})")
                _update_pool_cache(inst, is_rsshub=False)
                return [(x, f"ğ• {label}", "tw", False) for x in e]

    log.debug(f"ğ• {handle}: Ù‡Ù…Ù‡ fail")
    return []

def _update_pool_cache(working_inst: str, is_rsshub: bool):
    """instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø¨Ù‡ Ø§ÙˆÙ„ Ù„ÛŒØ³Øª cache Ù…ÛŒâ€ŒØ¨Ø±Ø¯"""
    global _nitter_pool, _rsshub_pool
    if is_rsshub:
        pool = [working_inst] + [i for i in _rsshub_pool if i != working_inst]
        _rsshub_pool = pool
        json.dump({"nitter": _nitter_pool, "rsshub": pool,
                   "ts": datetime.now(timezone.utc).timestamp()},
                  open(NITTER_CACHE_FILE, "w"))
    else:
        pool = [working_inst] + [i for i in _nitter_pool if i != working_inst]
        _nitter_pool = pool
        json.dump({"nitter": pool, "rsshub": _rsshub_pool,
                   "ts": datetime.now(timezone.utc).timestamp()},
                  open(NITTER_CACHE_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADS-B
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ADSB_API     = "https://api.adsb.one/v2"
ADSB_REGIONS = [
    ("Ø§ÛŒØ±Ø§Ù†",          32.4, 53.7, 250),
    ("Ø®Ù„ÛŒØ¬â€ŒÙØ§Ø±Ø³",     26.5, 52.0, 250),
    ("Ø§Ø³Ø±Ø§ÛŒÛŒÙ„/Ù„Ø¨Ù†Ø§Ù†", 32.1, 35.2, 200),
    ("Ø¹Ø±Ø§Ù‚",           33.3, 44.4, 250),
]
# ÙÙ‚Ø· Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ø¬Ù†Ú¯ÛŒ Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ â€” Ø¨Ø¯ÙˆÙ† ØªØ±Ø§Ø¨Ø±ÛŒ (C17, KC135, C130, ...)
_COMBAT_TYPES   = {"F15","F16","F22","F35","F18","F14","SU35","SU30","MIG29",
                   "B52","B2","B1",        # Ø¨Ù…Ø¨â€ŒØ§ÙÚ©Ù†â€ŒÙ‡Ø§
                   "E3","E8","E767","E737", # Ù‡Ø´Ø¯Ø§Ø± Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… (AWACS)
                   "RC135","EP3","P8",      # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©
                   "U2","SR71","RQ4",       # Ù¾Ù‡Ù¾Ø§Ø¯/Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ø±ØªÙØ§Ø¹ Ø¨Ø§Ù„Ø§
                   "MQ9","MQ1","TB2","HESA",# Ù¾Ù‡Ù¾Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø³Ù„Ø­
                   "A10","AV8","AC130",     # Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ø²Ø¯ÛŒÚ©
                   "EA18","EA6",            # Ø¬Ù†Ú¯ Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©
                   }
_COMBAT_CALLSIGN = ["DOOM","BONE","BUCK","CIAO","JAKE","TORC","GRIM","HAVOC",
                    "GHOST","VIPER","EAGLE","RAPTOR","DEMON","REAPER","PREDATOR"]
_ADSB_SEEN    = set()

async def fetch_military_flights(client: httpx.AsyncClient) -> tuple[list, list]:
    """
    Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯: (msgs, aircraft_list)
    aircraft_list: [{"callsign","type","lat","lon","alt","gs","region"}, ...]
    """
    global _ADSB_SEEN
    msgs     = []
    aircraft = []
    try:
        try:
            if Path(FLIGHT_ALERT_FILE).exists():
                _ADSB_SEEN = set(json.load(open(FLIGHT_ALERT_FILE)).get("seen", []))
        except: pass

        for region, r_lat, r_lon, radius in ADSB_REGIONS:
            try:
                r = await client.get(f"{ADSB_API}/point/{r_lat}/{r_lon}/{radius}",
                                     timeout=httpx.Timeout(7.0),
                                     headers={"Accept": "application/json"})
                if r.status_code != 200: continue
                for ac in (r.json().get("ac") or []):
                    hex_id   = (ac.get("hex") or ac.get("icao","")).upper()
                    callsign = (ac.get("flight") or ac.get("callsign","")).strip()
                    cat      = (ac.get("category") or "").upper()
                    atype    = (ac.get("t") or ac.get("type","")).upper()
                    ac_lat   = ac.get("lat") or ac.get("latitude")
                    ac_lon   = ac.get("lon") or ac.get("longitude")
                    is_combat = (
                        any(atype.startswith(m) for m in _COMBAT_TYPES)
                        or any(callsign.startswith(p) for p in _COMBAT_CALLSIGN)
                        or cat in ("A5", "A6", "A7")  # ICAO military/UAV categories
                    )
                    if not is_combat: continue
                    uid = f"{hex_id}_{callsign}"
                    if uid in _ADSB_SEEN: continue
                    _ADSB_SEEN.add(uid)
                    alt = ac.get("alt_baro") or ac.get("alt", 0)
                    gs  = ac.get("gs") or ac.get("speed", 0)
                    msgs.append(
                        f"âœˆï¸ <b>ØªØ­Ø±Ú© Ù†Ø¸Ø§Ù…ÛŒ â€” {region}</b>\n"
                        f"Ù†ÙˆØ¹: <code>{atype or '?'}</code>  Ú©Ø§Ù„â€ŒØ³Ø§ÛŒÙ†: <code>{callsign or hex_id}</code>\n"
                        f"Ø§Ø±ØªÙØ§Ø¹: {alt:,} ft  Ø³Ø±Ø¹Øª: {gs} kt"
                    )
                    if ac_lat and ac_lon:
                        aircraft.append({
                            "callsign": callsign or hex_id,
                            "type":     atype or "?",
                            "lat":      float(ac_lat),
                            "lon":      float(ac_lon),
                            "alt":      alt,
                            "gs":       gs,
                            "region":   region,
                        })
            except Exception as e:
                log.debug(f"ADS-B {region}: {e}")

        json.dump({"seen": list(_ADSB_SEEN)[-300:]}, open(FLIGHT_ALERT_FILE, "w"))
    except Exception as e:
        log.warning(f"ADS-B: {e}")
    return msgs, aircraft


def make_flight_map(aircraft: list) -> "io.BytesIO | None":
    """
    Ù†Ù‚Ø´Ù‡ Ø¯Ù‚ÛŒÙ‚ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ Ø¨Ø§ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ
    Ù…Ø±Ø²Ù‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ + Ø´Ø¨Ú©Ù‡ Ù…Ø®ØªØµØ§Øª + Ø¨Ø±Ú†Ø³Ø¨
    """
    if not PIL_OK or not aircraft:
        return None
    try:
        W, H    = 1200, 800
        PAD_L   = 50    # ÙØ¶Ø§ÛŒ Ø³Ù…Øª Ú†Ù¾ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø¬Ø§Øª
        PAD_B   = 30    # ÙØ¶Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ†
        PAD_T   = 50    # Ù‡Ø¯Ø±
        MAP_W   = W - PAD_L
        MAP_H   = H - PAD_T - PAD_B

        # Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ â€” Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ Ú©Ø§Ù…Ù„
        LAT_MIN, LAT_MAX =  16.0, 43.0
        LON_MIN, LON_MAX =  26.0, 65.0

        def gp(lat, lon):
            """geo to pixel"""
            x = PAD_L + int((lon - LON_MIN) / (LON_MAX - LON_MIN) * MAP_W)
            y = PAD_T + int((LAT_MAX - lat) / (LAT_MAX - LAT_MIN) * MAP_H)
            return max(0, min(W-1, x)), max(0, min(H-1, y))

        # â”€â”€ Ø±Ù†Ú¯â€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        C_OCEAN  = (8,  28,  52)
        C_LAND   = (32, 45,  55)
        C_LAND2  = (38, 52,  62)   # Ø±Ù†Ú¯ Ù…ØªÙØ§ÙˆØª Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§ÛŒØ²
        C_BORDER = (80, 110, 140)
        C_GRID   = (22, 35,  48)
        C_GRID_L = (40, 58,  72)
        C_PLANE  = (255, 70,  50)
        C_PLANE2 = (255, 180, 50)   # Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒ Ø¯ÙˆÙ…
        C_LABEL  = (210, 230, 250)
        C_DIM    = (100, 130, 155)
        C_ACCENT = (255, 160, 30)
        C_HEAD   = (12,  18,  28)

        img = Image.new("RGB", (W, H), C_OCEAN)
        drw = ImageDraw.Draw(img)

        # â”€â”€ ÙÙˆÙ†Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            F14 = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 14)
            F12 = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
            F11 = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 11)
            FB  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 15)
            FBL = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 18)
        except:
            F14 = F12 = F11 = FB = FBL = ImageFont.load_default()

        # â”€â”€ Ø´Ø¨Ú©Ù‡ Ù…Ø®ØªØµØ§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for lat in range(17, 44, 2):
            y = gp(lat, LON_MIN)[1]
            drw.line([(PAD_L, y), (W, y)], fill=C_GRID, width=1)
            drw.text((2, y - 7), f"{lat}Â°", fill=C_DIM, font=F11)
        for lat in range(20, 44, 5):
            y = gp(lat, LON_MIN)[1]
            drw.line([(PAD_L, y), (W, y)], fill=C_GRID_L, width=1)

        for lon in range(28, 65, 2):
            x = gp(LAT_MIN, lon)[0]
            drw.line([(x, PAD_T), (x, H - PAD_B)], fill=C_GRID, width=1)
        for lon in range(30, 65, 5):
            x = gp(LAT_MIN, lon)[0]
            drw.line([(x, PAD_T), (x, H - PAD_B)], fill=C_GRID_L, width=1)
            drw.text((x - 8, H - PAD_B + 5), f"{lon}Â°", fill=C_DIM, font=F11)

        # â”€â”€ Ù…Ø±Ø²Ù‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ (Ù¾Ù„ÛŒÚ¯ÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨ÛŒ polygon) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ÙØ±Ù…Øª: [(lon, lat), ...] â€” Ù…Ø®ØªØµØ§Øª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ
        COUNTRIES = {
            "IRAN": {
                "color": (38, 52, 62),
                "pts": [
                    (44.0,37.0),(44.8,39.2),(45.5,39.6),(46.0,39.0),(47.0,39.5),
                    (48.0,40.0),(49.0,40.2),(50.0,40.0),(51.0,40.8),(52.0,41.0),
                    (53.0,41.5),(54.0,41.2),(55.0,41.0),(56.0,40.5),(57.0,40.0),
                    (58.0,39.5),(59.0,38.0),(60.0,37.0),(61.0,36.5),(61.5,35.0),
                    (61.0,34.0),(60.5,33.0),(60.0,31.5),(59.5,30.5),(58.5,29.5),
                    (57.5,28.0),(57.0,27.0),(56.5,27.0),(56.0,27.0),(55.0,26.5),
                    (54.0,26.5),(53.5,27.0),(53.0,26.5),(52.5,27.0),(52.0,27.0),
                    (51.5,27.5),(51.0,28.0),(50.5,28.5),(50.0,29.0),(49.5,29.5),
                    (49.0,30.0),(48.5,30.5),(48.0,31.5),(47.5,32.0),(47.0,33.0),
                    (46.5,33.5),(46.0,34.0),(45.5,35.0),(45.0,36.0),(44.5,36.5),
                    (44.0,37.0)
                ]
            },
            "IRAQ": {
                "color": (36, 50, 60),
                "pts": [
                    (38.8,33.4),(39.5,33.8),(40.0,34.2),(41.0,34.7),(42.0,35.2),
                    (43.0,36.0),(44.0,37.0),(44.5,36.5),(45.0,36.0),(45.5,35.0),
                    (46.0,34.0),(46.5,33.5),(47.0,33.0),(47.5,32.0),(48.0,31.5),
                    (48.5,30.5),(47.5,30.0),(47.0,29.5),(46.5,29.2),(46.0,29.0),
                    (44.7,29.2),(43.5,29.5),(42.0,30.5),(41.0,31.5),(40.0,32.0),
                    (39.0,32.5),(38.8,33.4)
                ]
            },
            "SYRIA": {
                "color": (34, 48, 58),
                "pts": [
                    (35.7,36.8),(36.0,36.5),(36.5,36.8),(37.0,36.5),(38.0,36.8),
                    (39.0,36.5),(40.0,36.8),(41.0,37.5),(42.0,37.2),(42.5,37.0),
                    (43.0,36.0),(42.0,35.2),(41.0,34.7),(40.0,34.2),(39.5,33.8),
                    (38.8,33.4),(38.0,33.5),(37.5,33.3),(37.0,33.5),(36.5,33.5),
                    (36.0,33.0),(35.8,33.5),(35.5,34.0),(35.7,35.0),(35.7,36.8)
                ]
            },
            "TURKEY": {
                "color": (36, 50, 60),
                "pts": [
                    (26.0,41.0),(27.0,41.5),(28.0,41.8),(29.0,41.5),(30.0,41.5),
                    (31.0,41.5),(32.0,42.0),(33.0,42.0),(34.0,42.0),(35.0,42.0),
                    (36.0,41.5),(37.0,41.5),(38.0,40.5),(39.0,40.5),(40.0,40.5),
                    (41.0,40.0),(42.0,40.5),(43.0,40.5),(44.0,40.0),(44.5,39.8),
                    (44.0,39.2),(43.0,38.5),(42.0,38.5),(41.0,38.5),(40.0,38.0),
                    (39.0,37.5),(38.0,37.0),(37.0,37.0),(36.5,36.8),(36.0,36.5),
                    (35.7,36.8),(35.5,36.5),(35.0,36.5),(34.5,37.0),(34.0,37.0),
                    (32.0,37.0),(30.0,36.5),(28.0,37.0),(26.5,38.0),(26.0,39.0),
                    (26.0,41.0)
                ]
            },
            "SAUDI": {
                "color": (34, 46, 56),
                "pts": [
                    (36.5,29.5),(37.0,29.0),(38.0,28.0),(39.0,27.0),(40.0,26.0),
                    (41.0,25.0),(42.0,24.5),(43.0,24.0),(44.0,23.5),(45.0,23.0),
                    (46.0,22.5),(47.0,22.0),(48.0,21.5),(49.0,21.0),(50.0,20.5),
                    (51.0,20.0),(52.0,19.5),(53.0,19.0),(54.0,18.5),(55.0,18.0),
                    (56.0,18.5),(56.0,20.0),(55.0,22.0),(54.0,24.0),(53.0,25.0),
                    (52.0,26.0),(51.0,27.0),(50.5,28.5),(50.0,29.0),(49.5,29.5),
                    (49.0,30.0),(48.5,30.5),(48.0,31.5),(47.5,32.0),(47.0,31.5),
                    (46.5,31.0),(46.0,29.0),(44.7,29.2),(43.5,29.5),(42.0,30.5),
                    (41.0,31.5),(40.0,32.0),(39.0,32.5),(38.8,33.4),(38.0,33.5),
                    (37.5,32.0),(37.0,31.0),(36.8,30.0),(36.5,29.5)
                ]
            },
            "ISRAEL_PAL": {
                "color": (40, 55, 68),
                "pts": [
                    (34.3,31.3),(34.5,31.0),(34.9,30.0),(35.1,29.5),(35.0,29.0),
                    (34.8,28.5),(34.5,29.5),(34.0,30.5),(33.8,31.0),(34.0,31.5),
                    (34.3,31.3)
                ]
            },
            "LEBANON": {
                "color": (36, 52, 64),
                "pts": [
                    (35.1,33.0),(35.7,34.0),(36.5,34.0),(36.6,33.5),(36.0,33.3),
                    (35.5,33.0),(35.1,33.0)
                ]
            },
            "JORDAN": {
                "color": (34, 48, 58),
                "pts": [
                    (34.9,30.0),(35.0,32.0),(35.5,33.0),(36.0,33.3),(36.5,33.5),
                    (36.6,33.5),(37.0,33.5),(38.0,33.5),(38.8,33.4),(39.0,32.5),
                    (39.0,31.5),(38.5,30.5),(37.5,30.0),(36.8,30.0),(36.5,29.5),
                    (36.0,29.5),(35.5,29.5),(35.2,29.6),(35.1,29.5),(34.9,30.0)
                ]
            },
            "YEMEN": {
                "color": (32, 45, 54),
                "pts": [
                    (42.5,16.5),(43.5,16.0),(44.5,15.5),(45.0,15.0),(45.5,14.5),
                    (46.0,14.0),(47.0,14.5),(48.0,14.0),(49.0,14.5),(50.0,15.0),
                    (51.0,16.0),(52.0,17.0),(53.0,17.5),(54.0,17.8),(55.0,17.5),
                    (55.5,16.5),(55.0,16.0),(54.5,15.5),(53.5,16.0),(52.5,17.0),
                    (51.5,17.0),(50.5,16.5),(49.5,16.0),(48.5,16.0),(47.5,16.5),
                    (46.5,17.0),(45.5,17.5),(44.5,17.5),(43.5,17.0),(42.5,16.5)
                ]
            },
            "UAE_OMAN": {
                "color": (34, 48, 58),
                "pts": [
                    (51.5,24.0),(52.5,24.5),(53.0,25.0),(54.0,25.5),(55.0,26.0),
                    (56.0,26.5),(57.0,27.0),(57.5,22.5),(56.5,22.0),(55.5,22.0),
                    (55.0,23.0),(54.0,24.0),(53.0,23.5),(52.5,23.5),(51.5,24.0)
                ]
            },
        }

        # Ø±Ø³Ù… Ú©Ø´ÙˆØ±Ù‡Ø§
        for country, info in COUNTRIES.items():
            pts_geo = info["pts"]
            if not pts_geo: continue
            pts_px = [gp(lat, lon) for lon, lat in pts_geo]
            drw.polygon(pts_px, fill=info["color"], outline=C_BORDER)

        # â”€â”€ Ù†Ø§Ù… Ú©Ø´ÙˆØ±Ù‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        LABELS = [
            (32.5, 53.0, "IRAN",    C_LABEL),
            (33.3, 44.4, "IRAQ",    C_DIM),
            (35.0, 38.5, "SYRIA",   C_DIM),
            (31.5, 35.0, "ISRAEL",  C_DIM),
            (25.0, 45.0, "SAUDI",   C_DIM),
            (24.5, 54.5, "UAE",     C_DIM),
            (15.5, 48.0, "YEMEN",   C_DIM),
            (32.0, 36.0, "JORDAN",  C_DIM),
            (33.5, 36.2, "LEBANON", C_DIM),
            (39.0, 35.0, "TURKEY",  C_DIM),
            (26.5, 51.5, "GULF",    (60, 100, 140)),
        ]
        for r_lat, r_lon, name, color in LABELS:
            if LAT_MIN <= r_lat <= LAT_MAX and LON_MIN <= r_lon <= LON_MAX:
                px, py = gp(r_lat, r_lon)
                drw.text((px, py), name, fill=color, font=F12)

        # â”€â”€ Ø®Ù„ÛŒØ¬ ÙØ§Ø±Ø³ (Ø¢Ø¨ÛŒâ€ŒØªØ±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        gulf_pts = [gp(lat, lon) for lon, lat in [
            (48.0,30.0),(50.0,29.5),(52.0,28.5),(54.0,27.5),(56.0,27.0),
            (57.0,26.0),(57.0,25.0),(55.0,24.5),(53.0,24.0),(51.0,24.0),
            (50.0,24.5),(49.0,25.5),(48.0,27.0),(48.0,30.0)
        ]]
        drw.polygon(gulf_pts, fill=(12, 40, 72), outline=None)

        # â”€â”€ Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        red_sea_pts = [gp(lat, lon) for lon, lat in [
            (32.5,30.0),(33.0,28.0),(34.0,26.0),(35.0,24.0),(36.0,22.0),
            (37.0,20.0),(38.0,18.0),(39.0,17.5),(40.0,17.0),(43.0,16.0),
            (43.0,17.0),(41.0,18.5),(40.0,20.0),(39.0,22.0),(38.0,24.0),
            (37.5,26.0),(37.0,28.0),(36.5,30.0),(32.5,30.0)
        ]]
        drw.polygon(red_sea_pts, fill=(10, 36, 65), outline=None)

        # â”€â”€ Ø¯Ø±ÛŒØ§ÛŒ Ù…Ø¯ÛŒØªØ±Ø§Ù†Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        med_pts = [gp(lat, lon) for lon, lat in [
            (26.0,36.5),(30.0,36.0),(32.0,34.5),(34.0,33.0),(35.7,36.8),
            (34.5,37.0),(32.0,37.0),(30.0,36.5),(28.0,37.0),(26.5,38.0),
            (26.0,36.5)
        ]]
        drw.polygon(med_pts, fill=(10, 36, 65), outline=None)

        # â”€â”€ Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plane_colors = [C_PLANE, C_PLANE2, (80, 200, 120), (180, 80, 255)]
        placed = []

        for idx, ac in enumerate(aircraft):
            lat, lon = ac["lat"], ac["lon"]
            if not (LAT_MIN <= lat <= LAT_MAX and LON_MIN <= lon <= LON_MAX):
                continue
            px, py = gp(lat, lon)

            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„
            shift = 0
            for ppx, ppy in placed:
                if abs(px - ppx) < 20 and abs(py - ppy) < 20:
                    py -= 25
                    break
            placed.append((px, py))

            pc = plane_colors[idx % len(plane_colors)]

            # Ø¯Ø§ÛŒØ±Ù‡ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø¯Ø±Ø®Ø´Ø§Ù†
            drw.ellipse([(px-18, py-18), (px+18, py+18)],
                        fill=(pc[0]//4, pc[1]//4, pc[2]//4), outline=pc, width=2)
            # Ù…Ø«Ù„Ø« Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§
            tri = [(px, py-12), (px-8, py+8), (px+8, py+8)]
            drw.polygon(tri, fill=pc, outline=(255,255,255))
            # Ù†Ù‚Ø·Ù‡ Ù…Ø±Ú©Ø²ÛŒ
            drw.ellipse([(px-3, py-3), (px+3, py+3)], fill=(255,255,255))

            # Ø®Ø· Ø±Ø§Ù‡Ù†Ù…Ø§ Ø¨Ù‡ Ø¨Ø±Ú†Ø³Ø¨
            lx = px + 22
            drw.line([(px+12, py), (lx-2, py)], fill=pc, width=1)

            # Ø¨Ø±Ú†Ø³Ø¨ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡
            label   = f"{ac['callsign']} / {ac['type']}"
            alt_txt = f"alt:{int(ac['alt'])//1000 if ac['alt'] else '?'}k  {ac['gs']}kt"
            drw.rectangle([(lx-2, py-14), (lx+170, py+18)],
                          fill=(12, 18, 28), outline=pc)
            drw.text((lx+2, py-13), label,   fill=pc,    font=FB)
            drw.text((lx+2, py+2),  alt_txt, fill=C_DIM, font=F11)

        # â”€â”€ Ù‡Ø¯Ø± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        drw.rectangle([(0, 0), (W, PAD_T - 2)], fill=C_HEAD)
        drw.rectangle([(0, PAD_T - 2), (W, PAD_T)], fill=C_ACCENT)
        now_str = datetime.now(TEHRAN_TZ).strftime("%H:%M  %Y/%m/%d")
        drw.text((10, 8),
                 f"âœˆ  Military Flights â€” Middle East  |  {now_str}  |  {len(aircraft)} aircraft tracked",
                 fill=C_ACCENT, font=FB)

        # â”€â”€ ÙÙˆØªØ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        drw.rectangle([(0, H - PAD_B), (W, H)], fill=C_HEAD)
        drw.text((10, H - PAD_B + 6), "Source: ADS-B Exchange  |  WarBot v17",
                 fill=C_DIM, font=F11)

        # â”€â”€ legend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        lx, ly = W - 200, PAD_T + 10
        drw.rectangle([(lx-5, ly-5), (W-5, ly + len(aircraft)*22 + 10)],
                      fill=(10, 15, 25), outline=C_BORDER)
        for i, ac in enumerate(aircraft):
            pc = plane_colors[i % len(plane_colors)]
            drw.rectangle([(lx, ly + i*22), (lx+12, ly + i*22 + 12)], fill=pc)
            drw.text((lx+16, ly + i*22 - 2),
                     f"{ac['callsign']} â€“ {ac['region']}", fill=C_LABEL, font=F11)

        buf = io.BytesIO()
        img.save(buf, "JPEG", quality=90)
        buf.seek(0)
        return buf

    except Exception as e:
        log.warning(f"flight_map error: {e}")
        import traceback; log.debug(traceback.format_exc())
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS + Telegram fetch
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_rss(client: httpx.AsyncClient, feed: dict) -> list:
    """RSS Ø¨Ø§ conditional GET (ETag/If-Modified-Since)"""
    try:
        hdrs = dict(COMMON_UA)
        hdrs["Accept"] = "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8"
        if feed.get("_etag"):      hdrs["If-None-Match"]     = feed["_etag"]
        if feed.get("_last_mod"):  hdrs["If-Modified-Since"] = feed["_last_mod"]
        r = await client.get(feed["u"], timeout=httpx.Timeout(RSS_TIMEOUT), headers=hdrs)
        if r.status_code == 304: return []
        if r.status_code != 200: return []
        if r.headers.get("ETag"):          feed["_etag"]     = r.headers["ETag"]
        if r.headers.get("Last-Modified"): feed["_last_mod"] = r.headers["Last-Modified"]
        entries = feedparser.parse(r.text).entries or []
        is_emb  = id(feed) in EMBASSY_SET
        return [(e, feed["n"], "rss", is_emb) for e in entries]
    except: return []

async def fetch_telegram_channel(client: httpx.AsyncClient, label: str,
                                  handle: str, cutoff: datetime) -> list:
    """
    scrape t.me/s/{handle} â€” ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² cutoff
    """
    url  = f"https://t.me/s/{handle}"
    hdrs = {
        "User-Agent": "TelegramBot (like TwitterBot)",
        "Accept": "text/html,application/xhtml+xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Cache-Control": "no-cache",
    }
    try:
        r = await client.get(url, timeout=httpx.Timeout(TG_TIMEOUT), headers=hdrs)
        if r.status_code not in (200, 301, 302): return []
        soup = BeautifulSoup(r.text, "html.parser")
        msgs = soup.select(".tgme_widget_message_wrap")
        if not msgs: return []
        results = []
        for msg in msgs[-30:]:
            txt_el = msg.select_one(".tgme_widget_message_text")
            text   = txt_el.get_text(" ", strip=True) if txt_el else ""
            if not text or len(text) < 15: continue
            time_el  = msg.select_one("time")
            dt_str   = time_el.get("datetime", "") if time_el else ""
            entry_dt = None
            if dt_str:
                try: entry_dt = datetime.fromisoformat(dt_str.replace("Z","+00:00"))
                except: pass
            # ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØªØ§Ø²Ù‡â€ŒØªØ± Ø§Ø² cutoff
            if entry_dt and entry_dt < cutoff: continue
            link_el = msg.select_one("a.tgme_widget_message_date")
            link    = link_el.get("href","") if link_el else f"https://t.me/{handle}"
            results.append(({
                "title":   text[:300],
                "summary": text[:800],
                "link":    link,
                "_tg_dt":  entry_dt,
            }, label, "tg", False))
        return results
    except Exception as e:
        log.debug(f"TG {handle}: {e}"); return []

async def fetch_all(client: httpx.AsyncClient, cutoff: datetime) -> list:
    """
    ÙˆØ§Ú©Ø´ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹
    cutoff Ø¨Ø±Ø§ÛŒ Telegram Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡ (RSS Ø§Ø² is_fresh Ø¯Ø± main ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´Ù‡)
    """
    await build_twitter_pools(client)

    rss_t = [fetch_rss(client, f) for f in ALL_RSS_FEEDS]
    tg_t  = [fetch_telegram_channel(client, l, h, cutoff) for l, h in TELEGRAM_CHANNELS]
    tw_t  = [fetch_twitter(client, l, h) for l, h in TWITTER_HANDLES]

    all_res = await asyncio.gather(*rss_t, *tg_t, *tw_t, return_exceptions=True)

    out = []; rss_ok = tg_ok = tw_ok = 0
    n_rss = len(ALL_RSS_FEEDS); n_tg = len(TELEGRAM_CHANNELS)
    for i, res in enumerate(all_res):
        if not isinstance(res, list): continue
        out.extend(res)
        if   i < n_rss:          rss_ok += bool(res)
        elif i < n_rss + n_tg:   tg_ok  += bool(res)
        else:                     tw_ok  += bool(res)

    log.info(f"  ğŸ“¡ RSS:{rss_ok}/{len(ALL_RSS_FEEDS)} "
             f" ğŸ“¢ TG:{tg_ok}/{len(TELEGRAM_CHANNELS)} "
             f" ğ•:{tw_ok}/{len(TWITTER_HANDLES)}")
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø± Ù…ØªÙ†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(t): return re.sub(r"<[^>]+>", " ", t or "").strip()
def trim(t, n):
    t = t.strip()
    return t if len(t) <= n else t[:n-1] + "â€¦"
def make_id(entry):
    k = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(k.encode()).hexdigest()
def esc(t):
    return re.sub(r"([<>&])", lambda m: {"<":"&lt;",">":"&gt;","&":"&amp;"}[m.group()], t)

def format_dt(entry) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
        tg_dt = entry.get("_tg_dt")
        if tg_dt:
            return tg_dt.astimezone(TEHRAN_TZ).strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
    except: pass
    return ""

def is_fresh(entry, cutoff: datetime) -> bool:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t: return datetime(*t[:6], tzinfo=timezone.utc) >= cutoff
        tg_dt = entry.get("_tg_dt")
        if tg_dt: return tg_dt >= cutoff
        return True  # Ø¨Ø¯ÙˆÙ† timestamp â†’ Ù¾Ø§Ø³ Ø¨Ø¯Ù‡ (seen.json ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ù‡)
    except: return True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dedup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_VIOLENCE_CODES  = {"MSL","AIR","ATK","KIA","DEF","EXP"}
_POLITICAL_CODES = {"THR","DIP","SAN","NUC","SPY","STM"}

def _stem(word):
    w = word.lower()
    for suf in ("ing","ed","tion","ment","er","Ù‡Ø§","Ù‡Ø§ÛŒ","\u200cÙ‡Ø§"):
        if w.endswith(suf) and len(w) > len(suf)+3: return w[:-len(suf)]
    return w

def _bag(text):
    return {_stem(w) for w in re.findall(r"[\w\u0600-\u06FF]{3,}", text.lower())}

def _entity_triple(title):
    txt = title.lower()
    actors = (
        ["iran","irgc","khamenei","Ø³Ù¾Ø§Ù‡","Ø§ÛŒØ±Ø§Ù†"],
        ["israel","idf","netanyahu","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„"],
        ["us ","usa","centcom","pentagon","Ø¢Ù…Ø±ÛŒÚ©Ø§"],
        ["hamas","Ø­Ù…Ø§Ø³"], ["hezbollah","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡"], ["houthi","Ø­ÙˆØ«ÛŒ"],
    )
    action_cats = {
        "MSL": ["missile","rocket","ballistic","Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯"],
        "AIR": ["airstrike","bombing","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†"],
        "ATK": ["attack","strike","Ø­Ù…Ù„Ù‡"],
        "KIA": ["killed","dead","casualties","Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯"],
        "DEF": ["intercept","iron dome","Ø±Ù‡Ú¯ÛŒØ±ÛŒ"],
        "EXP": ["explosion","blast","Ø§Ù†ÙØ¬Ø§Ø±"],
        "THR": ["threat","warn","ØªÙ‡Ø¯ÛŒØ¯"],
        "SAN": ["sanction","ØªØ­Ø±ÛŒÙ…"],
        "NUC": ["nuclear","uranium","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ"],
    }
    actor1, actor2, act = "", "", ""
    for i, grp in enumerate(actors):
        if any(a in txt for a in grp):
            if not actor1: actor1 = str(i)
            elif not actor2: actor2 = str(i)
    for code, kws in action_cats.items():
        if any(k in txt for k in kws): act = code; break
    return actor1, actor2, act

def is_story_dup(title: str, stories: list) -> bool:
    bag1 = _bag(title)
    if not bag1: return False
    a1, a2, act1 = _entity_triple(title)
    for item in stories:
        if not (isinstance(item, (list, tuple)) and len(item) == 3):
            continue
        _, prev_bag_raw, prev_triple = item
        prev_bag = set(prev_bag_raw) if isinstance(prev_bag_raw, list) else prev_bag_raw
        pa, pb, pact = prev_triple
        if act1 and pact and act1 in _VIOLENCE_CODES and pact in _VIOLENCE_CODES:
            if a1 == pa and a2 == pb: return True
        if act1 and pact and act1 in _POLITICAL_CODES and pact in _POLITICAL_CODES:
            if a1 == pa: return True
        union = bag1 | prev_bag
        if union and len(bag1 & prev_bag) / len(union) >= JACCARD_THRESHOLD:
            return True
    return False

def register_story(title: str, stories: list) -> list:
    stories.append([title, list(_bag(title)), list(_entity_triple(title))])
    return stories[-MAX_STORIES:]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# seen.json â€” Ø¨Ø§ TTL â€” ÙÙ‚Ø· Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    cutoff_ts = datetime.now(timezone.utc).timestamp() - SEEN_TTL_HOURS * 3600
    try:
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                return {k for k, v in raw.items() if v > cutoff_ts}
            elif isinstance(raw, list):
                # migrate Ø§Ø² ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… â€” ÙÙ‚Ø· ÛµÛ°Û° ØªØ§ Ø¢Ø®Ø±
                return set(raw[-500:])
    except: pass
    return set()

def save_seen(seen: set):
    now_ts    = datetime.now(timezone.utc).timestamp()
    cutoff_ts = now_ts - SEEN_TTL_HOURS * 3600
    try:
        existing = {}
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                existing = {k: v for k, v in raw.items() if v > cutoff_ts}
    except: existing = {}
    for eid in seen:
        if eid not in existing: existing[eid] = now_ts
    if len(existing) > 5000:
        existing = dict(sorted(existing.items(), key=lambda x: x[1], reverse=True)[:5000])
    json.dump(existing, open(SEEN_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# run_state â€” last_run Ø¨Ø±Ø§ÛŒ cutoff Ù‡ÙˆØ´Ù…Ù†Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_run_state() -> datetime:
    """Ø¢Ø®Ø±ÛŒÙ† Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ â€” Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ cutoff"""
    try:
        if Path(RUN_STATE_FILE).exists():
            d   = json.load(open(RUN_STATE_FILE))
            ts  = d.get("last_run", 0)
            if ts:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
    except: pass
    # Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§: MAX_LOOKBACK_MIN Ø¨Ù‡ Ø¹Ù‚Ø¨
    return datetime.now(timezone.utc) - timedelta(minutes=MAX_LOOKBACK_MIN)

def save_run_state():
    existing = {}
    try:
        if Path(RUN_STATE_FILE).exists():
            existing = json.load(open(RUN_STATE_FILE))
    except: pass
    existing["last_run"] = datetime.now(timezone.utc).timestamp()
    json.dump(existing, open(RUN_STATE_FILE, "w"))

def load_stories() -> list:
    try:
        if Path(STORIES_FILE).exists():
            raw = json.load(open(STORIES_FILE))
            # migrate ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… (2-tuple) Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ (3-tuple)
            result = []
            for item in raw:
                if isinstance(item, (list, tuple)) and len(item) == 2:
                    title = item[0]
                    result.append([title, list(_bag(title)), list(_entity_triple(title))])
                elif isinstance(item, (list, tuple)) and len(item) == 3:
                    result.append(item)
            return result
    except: pass
    return []

def save_stories(stories):
    json.dump(stories[-300:], open(STORIES_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªØ±Ø¬Ù…Ù‡ â€” Gemini Ø§ÙˆÙ„ØŒ MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† fallback
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_MODELS = [
    "gemini-2.0-flash",
    "gemini-1.5-flash",
    "gemini-1.5-flash-8b",
]

# ØªØ´Ø®ÛŒØµ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
def _is_farsi(text: str) -> bool:
    fa_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
    return fa_chars / max(len(text), 1) > 0.3

# ØªØ±Ø¬Ù…Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù† ÛŒÚ© Ù…ØªÙ† Ø§Ø² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ MyMemory
async def _translate_mymemory(client: httpx.AsyncClient, text: str) -> str:
    """MyMemory API â€” Ø±Ø§ÛŒÚ¯Ø§Ù†ØŒ Ø¨Ø¯ÙˆÙ† Ú©Ù„ÛŒØ¯ØŒ ØªØ§ ÛµÛ°Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¯Ø± Ø±ÙˆØ²"""
    if not text or _is_farsi(text):
        return text
    try:
        url = "https://api.mymemory.translated.net/get"
        r = await client.get(url,
            params={"q": text[:500], "langpair": "en|fa", "de": "warbot@github.com"},
            timeout=httpx.Timeout(8.0))
        if r.status_code == 200:
            data = r.json()
            tr = data.get("responseData", {}).get("translatedText", "")
            # MyMemory Ú¯Ø§Ù‡ÛŒ MYMEMORY WARNING Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
            if tr and "MYMEMORY WARNING" not in tr and len(tr) > 5:
                return tr
    except Exception as e:
        log.debug(f"MyMemory: {e}")
    return text

GEMINI_PROMPT = """ØªÙˆ ÛŒÚ© Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ø¬Ù†Ú¯ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒ. Ø§ÛŒÙ† Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ú©Ù†.

Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø±Ø§ Ø±Ø¹Ø§ÛŒØª Ú©Ù†:
###ITEM_0###
T: [Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ Ø¯Ø± ÛŒÚ© Ø®Ø·]
B: [Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„]
###ITEM_1###
T: [Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ]
B: [Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ]

Ù‚ÙˆØ§Ù†ÛŒÙ†:
- Ø§Ø³Ø§Ù…ÛŒ: Netanyahu=Ù†ØªØ§Ù†ÛŒØ§Ù‡ÙˆØŒ Khamenei=Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒØŒ IRGC=Ø³Ù¾Ø§Ù‡ØŒ IDF=Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ØŒ CENTCOM=Ø³ØªØ§Ø¯ Ù…Ø±Ú©Ø²ÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§
- Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¢Ù…Ø§Ø±ØŒ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±
- Ø§Ú¯Ù‡ Ø®Ø¨Ø± ÙØ§Ø±Ø³ÛŒÙ‡: ÙÙ‚Ø· Ù¾Ø§Ú©ÛŒØ²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†

===Ø®Ø¨Ø±Ù‡Ø§===
{items}"""

async def _translate_gemini(client: httpx.AsyncClient, articles: list) -> list | None:
    """ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Gemini â€” None Ø§Ú¯Ù‡ fail Ø´Ø¯"""
    if not GEMINI_API_KEY:
        return None
    items_txt = "".join(
        f"###ITEM_{i}###\nEN_TITLE: {t[:300]}\nEN_BODY: {s[:400]}\n\n"
        for i, (t, s) in enumerate(articles)
    )
    state = {}
    try:
        if Path(GEMINI_STATE_FILE).exists():
            state = json.load(open(GEMINI_STATE_FILE))
    except: pass
    models = state.get("models_order", GEMINI_MODELS)
    base   = "https://generativelanguage.googleapis.com/v1beta/models"

    for model in models:
        try:
            r = await client.post(
                f"{base}/{model}:generateContent?key={GEMINI_API_KEY}",
                json={
                    "contents": [{"parts": [{"text": GEMINI_PROMPT.format(items=items_txt)}]}],
                    "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
                },
                timeout=httpx.Timeout(40.0)
            )
            if r.status_code == 429:
                log.warning(f"Gemini {model}: rate-limit"); continue
            if r.status_code != 200:
                log.warning(f"Gemini {model}: HTTP {r.status_code} â€” {r.text[:200]}"); continue

            text_out = r.json()["candidates"][0]["content"]["parts"][0]["text"]
            log.info(f"ğŸŒ Gemini {model} OK")

            results = list(articles)
            ok_count = 0
            for i, (orig_t, orig_s) in enumerate(articles):
                blk = re.search(rf"###ITEM_{i}###\s*(.*?)(?=###ITEM_\d+###|\Z)", text_out, re.DOTALL)
                if not blk: continue
                block   = blk.group(1)
                t_match = re.search(r"^T:\s*(.+)$", block, re.MULTILINE)
                b_match = re.search(r"^B:\s*([\s\S]+?)$", block, re.MULTILINE)
                fa_t = t_match.group(1).strip() if t_match else ""
                fa_b = b_match.group(1).strip() if b_match else ""
                # fallback: Ù‡Ù…Ù‡ block Ø±Ø§ Ø¹Ù†ÙˆØ§Ù† Ø¨Ú¯ÛŒØ±
                if not fa_t:
                    fa_t = block.strip().split('\n')[0]
                if len(fa_t) > 5:
                    results[i] = (fa_t, fa_b or orig_s)
                    ok_count += 1
            log.info(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡: {ok_count}/{len(articles)} Ø®Ø¨Ø±")
            # Ù…Ø¯Ù„ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø±Ø§ Ø§ÙˆÙ„ Ø¨Ú¯Ø°Ø§Ø±
            state["models_order"] = [model] + [m for m in models if m != model]
            json.dump(state, open(GEMINI_STATE_FILE, "w"))
            return results
        except Exception as e:
            log.warning(f"Gemini {model}: {e}"); continue
    return None

async def translate_batch(client: httpx.AsyncClient, articles: list) -> list:
    """
    ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª:
    1. Gemini (Ø§Ú¯Ù‡ API key Ø¯Ø§Ø±ÛŒÙ…)
    2. MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† (ÙÙ‚Ø· Ø¹Ù†ÙˆØ§Ù†)
    3. Ù…ØªÙ† Ø§ØµÙ„ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØ±Ø¬Ù…Ù‡)
    """
    if not articles:
        return []

    results = list(articles)

    # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û±: Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if GEMINI_API_KEY:
        log.info(f"ğŸŒ Gemini: ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø®Ø¨Ø±...")
        gemini_res = await _translate_gemini(client, articles)
        if gemini_res:
            return gemini_res
        log.warning("ğŸŒ Gemini fail â€” fallback Ø¨Ù‡ MyMemory")
    else:
        log.info("ğŸŒ GEMINI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ â€” Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù†")

    # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û²: MyMemory â€” Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info(f"ğŸŒ MyMemory: ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø¹Ù†ÙˆØ§Ù†...")
    sema = asyncio.Semaphore(5)

    async def _tr(orig_t, orig_s):
        async with sema:
            if _is_farsi(orig_t):
                return (orig_t, orig_s)
            fa_t = await _translate_mymemory(client, orig_t)
            return (fa_t, orig_s)

    translated = await asyncio.gather(*[_tr(t, s) for t, s in articles])
    ok = sum(1 for i, (fa, _) in enumerate(translated) if fa != articles[i][0])
    log.info(f"ğŸŒ MyMemory: {ok}/{len(articles)} ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯")
    return list(translated)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Sentiment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BREAKING_KEYWORDS = [
    "breaking","urgent","alert","just in","explosion","airstrike","killed","dead",
    "war","attack","strike","nuclear","bomb","missile","invasion",
    "Ø­Ù…Ù„Ù‡","Ú©Ø´ØªÙ‡","Ø§Ù†ÙØ¬Ø§Ø±","Ø´Ù‡ÛŒØ¯","Ù…ÙˆØ´Ú©","ÙÙˆØ±ÛŒ","Ø®Ø¨Ø± ÙÙˆØ±ÛŒ","Ø§Ø¹Ù„Ø§Ù… Ø¬Ù†Ú¯",
]
IMPORTANCE_BOOST = {
    "ğŸ’€":4, "ğŸ”´":3, "ğŸ’¥":3, "ğŸš€":3, "â˜¢ï¸":3,
    "âœˆï¸":2, "ğŸš¢":2, "ğŸ›¡ï¸":2, "ğŸ•µï¸":2,
    "ğŸ”¥":1, "ğŸ’°":1, "âš ï¸":1,
}

SENTIMENT_RULES = [
    ("ğŸ’€", ["killed","dead","casualties","fatalities","wounded","martyred","massacre"],
           ["Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯","ØªÙ„ÙØ§Øª","Ú©Ø´ØªØ§Ø±","Ù…Ø¬Ø±ÙˆØ­"]),
    ("ğŸ”´", ["attack","struck","assault","launched attack","opened fire","bombed","targeted"],
           ["Ø­Ù…Ù„Ù‡","Ø¶Ø±Ø¨Ù‡","Ù…ÙˆØ±Ø¯ Ù‡Ø¯Ù","Ø­Ù…Ù„Ù‡ Ú©Ø±Ø¯"]),
    ("ğŸ’¥", ["explosion","blast","detonation","explode","blew up"],
           ["Ø§Ù†ÙØ¬Ø§Ø±","Ù…Ù†ÙØ¬Ø±","ØªØ±Ú©ÛŒØ¯"]),
    ("âœˆï¸", ["airstrike","air strike","air raid","warplane","f-35","f-15","b-52","f-16"],
           ["Ø­Ù…Ù„Ù‡ Ù‡ÙˆØ§ÛŒÛŒ","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†","Ø¬Ù†Ú¯Ù†Ø¯Ù‡"]),
    ("ğŸš€", ["missile","rocket","ballistic","cruise missile","drone strike","hypersonic"],
           ["Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯","Ù…ÙˆØ´Ú© Ø¨Ø§Ù„Ø³ØªÛŒÚ©","Ø±Ø§Ú©Øª"]),
    ("â˜¢ï¸", ["nuclear","uranium","enrichment","natanz","fordow","centrifuge","iaea"],
           ["Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…","ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ","Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ø³Ø§Ù†ØªØ±ÛŒÙÛŒÙˆÚ˜"]),
    ("ğŸš¢", ["navy","naval","warship","aircraft carrier","strait of hormuz","red sea"],
           ["Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ","Ù†Ø§Ùˆ","ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²","Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø®"]),
    ("ğŸ•µï¸", ["intelligence","mossad","cia","spy","covert","assassination","sabotage","cyber"],
           ["Ø¬Ø§Ø³ÙˆØ³ÛŒ","Ù…ÙˆØ³Ø§Ø¯","Ø®Ø±Ø§Ø¨Ú©Ø§Ø±ÛŒ","ØªØ±ÙˆØ±","Ø³Ø§ÛŒØ¨Ø±ÛŒ"]),
    ("ğŸ›¡ï¸", ["intercept","shot down","iron dome","air defense","patriot"],
           ["Ø±Ù‡Ú¯ÛŒØ±ÛŒ","Ù¾Ø¯Ø§ÙÙ†Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†","Ø³Ø±Ù†Ú¯ÙˆÙ†"]),
    ("ğŸ”¥", ["escalat","tension","brink of war","retaliat","provocation"],
           ["ØªØ´Ø¯ÛŒØ¯","ØªÙ†Ø´","ØªÙ„Ø§ÙÛŒ","Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯"]),
    ("ğŸ’°", ["sanction","embargo","swift","freeze assets"],
           ["ØªØ­Ø±ÛŒÙ…","Ù…Ø­Ø§ØµØ±Ù‡ Ø§Ù‚ØªØµØ§Ø¯ÛŒ"]),
    ("âš ï¸", ["threat","warn","warning","ultimatum","red line","will respond"],
           ["ØªÙ‡Ø¯ÛŒØ¯","Ù‡Ø´Ø¯Ø§Ø±","Ø®Ø· Ù‚Ø±Ù…Ø²","Ø§ÙˆÙ„ØªÛŒÙ…Ø§ØªÙˆÙ…"]),
    ("ğŸ¤", ["negotiation","talks","deal","diplomacy","ceasefire","agreement"],
           ["Ù…Ø°Ø§Ú©Ø±Ù‡","ØªÙˆØ§ÙÙ‚","Ø¢ØªØ´â€ŒØ¨Ø³","Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ"]),
    ("ğŸ“œ", ["statement","declared","announced","press conference","spokesperson"],
           ["Ø¨ÛŒØ§Ù†ÛŒÙ‡","Ø§Ø¹Ù„Ø§Ù…","Ù†Ø´Ø³Øª Ø®Ø¨Ø±ÛŒ","Ø³Ø®Ù†Ú¯Ùˆ"]),
]

def analyze_sentiment(text: str) -> list:
    txt = text.lower()
    found = []
    for icon, en_kws, fa_kws in SENTIMENT_RULES:
        if any(kw in txt for kw in en_kws) or any(kw in txt for kw in fa_kws):
            found.append(icon)
        if len(found) >= 3: break
    return found or ["ğŸ“°"]

def calc_importance(title: str, body: str, icons: list, stype: str) -> int:
    txt = (title + " " + body).lower()
    score = sum(IMPORTANCE_BOOST.get(ic, 0) for ic in icons)
    if any(k in txt for k in BREAKING_KEYWORDS): score += 2
    if stype == "tw" and score > 0: score += 1
    return min(score, 10)

def sentiment_bar(icons): return "  ".join(icons)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Telegram Ø§Ø±Ø³Ø§Ù„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _tgapi(path: str) -> str:
    return f"https://api.telegram.org/bot{BOT_TOKEN}/{path}"

async def tg_send_text(client: httpx.AsyncClient, text: str) -> bool:
    text = text[:MAX_MSG_LEN]
    for attempt in range(3):
        try:
            r = await client.post(_tgapi("sendMessage"),
                json={"chat_id": CHANNEL_ID, "text": text,
                      "parse_mode": "HTML", "disable_web_page_preview": False},
                timeout=httpx.Timeout(15.0))
            d = r.json()
            if r.status_code == 200 and d.get("ok"): return True
            if d.get("error_code") == 429:
                wait = d.get("parameters", {}).get("retry_after", 20)
                await asyncio.sleep(wait)
            elif attempt < 2:
                await asyncio.sleep(3)
        except Exception as e:
            log.warning(f"TG send: {e}")
            if attempt < 2: await asyncio.sleep(5)
    return False

async def tg_send_photo(client: httpx.AsyncClient, buf: io.BytesIO,
                         caption: str) -> bool:
    caption = caption[:1024]
    try:
        buf.seek(0)
        r = await client.post(_tgapi("sendPhoto"),
            data={"chat_id": CHANNEL_ID, "caption": caption, "parse_mode": "HTML"},
            files={"photo": ("card.jpg", buf, "image/jpeg")},
            timeout=httpx.Timeout(20.0))
        return r.status_code == 200 and r.json().get("ok", False)
    except Exception as e:
        log.warning(f"TG photo: {e}"); return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIL Ú©Ø§Ø±Øª Ø®Ø¨Ø±ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BG_DARK  = (14, 16, 22)
BG_BAR   = (22, 26, 34)
FG_WHITE = (235, 237, 242)
FG_GREY  = (120, 132, 148)
ACCENT_MAP = {
    "ğŸ‡®ğŸ‡·":(180,40,40), "ğŸ‡®ğŸ‡±":(30,90,180), "ğŸ‡ºğŸ‡¸":(40,80,160),
    "ğŸ”":(60,130,80), "ğŸŒ":(100,60,130), "ğŸ›ï¸":(140,100,40),
}
ICON_BG = {
    "ğŸ’€":(140,20,20),"ğŸ”´":(180,30,30),"ğŸ’¥":(190,80,10),
    "âœˆï¸":(20,90,160),"ğŸš€":(100,20,160),"â˜¢ï¸":(0,130,50),
    "ğŸš¢":(10,80,140),"ğŸ•µï¸":(60,55,70),"ğŸ›¡ï¸":(20,110,80),
    "ğŸ”¥":(180,60,0),"ğŸ’°":(130,110,0),"âš ï¸":(160,110,0),
    "ğŸ¤":(20,120,100),"ğŸ“œ":(60,80,100),"ğŸ“°":(45,58,72),
}

def _get_accent(src, urgent):
    if urgent: return (210, 40, 40)
    for k, v in ACCENT_MAP.items():
        if src.startswith(k) or k in src: return v
    return (80, 110, 140)

def _wrap(text, chars):
    words, lines_out, cur = text.split(), [], ""
    for w in words:
        if len(cur) + len(w) + 1 <= chars: cur = (cur + " " + w).strip()
        else:
            if cur: lines_out.append(cur)
            cur = w
    if cur: lines_out.append(cur)
    return lines_out

def _fonts():
    try:
        bold = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
        reg  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        sm   = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 13)
        return bold, reg, sm
    except:
        d = ImageFont.load_default(); return d, d, d

def make_news_card(headline, fa_text, src, dt_str,
                   urgent=False, sentiment_icons=None):
    if not PIL_OK: return None
    try:
        W, H = 960, 310
        acc = _get_accent(src, urgent)
        img = Image.new("RGB", (W, H), BG_DARK)
        drw = ImageDraw.Draw(img)
        F_H, F_B, F_sm = _fonts()

        drw.rectangle([(0,0),(W,5)], fill=acc)
        drw.rectangle([(0,5),(W,58)], fill=BG_BAR)
        drw.rectangle([(0,58),(W,61)], fill=acc)
        drw.text((18,18), src[:55],     font=F_sm, fill=acc)
        drw.text((W-170,18), dt_str[:25], font=F_sm, fill=FG_GREY)

        display = fa_text if (fa_text and len(fa_text) > 5) else headline
        y = 72
        for line in _wrap(display, 50)[:4]:
            drw.text((W-18, y), line, font=F_H, fill=FG_WHITE, anchor="ra")
            y += 30

        drw.rectangle([(0,H-56),(W,H)], fill=BG_BAR)
        drw.rectangle([(0,H-58),(W,H-56)], fill=acc)
        x_pos = 16
        for ico in (sentiment_icons or ["ğŸ“°"])[:4]:
            bg = ICON_BG.get(ico, (50,65,75))
            drw.rounded_rectangle([(x_pos-2,H-52),(x_pos+38,H-6)], radius=7, fill=bg)
            drw.text((x_pos+2,H-50), ico, font=F_H, fill=(255,255,255))
            x_pos += 50

        if urgent: drw.rectangle([(0,61),(5,H-58)], fill=acc)

        buf = io.BytesIO()
        img.save(buf, "JPEG", quality=85)
        buf.seek(0)
        return buf
    except Exception as e:
        log.debug(f"card: {e}"); return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² Ø³Ø§ÛŒØª (og:image ØªØµÙˆÛŒØ± Ù…Ù‚Ø§Ù„Ù‡ â€” Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± (Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ â€” Ø¹Ú©Ø³ Ù…Ù‚Ø§Ù„Ù‡)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ (Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ â€” Ø¹Ú©Ø³ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# URLâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒ Ù„ÙˆÚ¯Ùˆ Ø¯Ø§Ø±Ù†Ø¯
_SKIP_IMG_PATTERNS = [
    "logo","icon","favicon","sprite","avatar","placeholder",
    "default","blank","spacer","1x1","pixel","brand","masthead",
    "no-image","no-photo","profile","author","byline","signature",
    "/ad/","/ads/","banner","promo","subscribe","newsletter",
]
# CSS selector Ù‡Ø§ÛŒ ordered Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±
_IMG_SELECTORS = [
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ article Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    "article figure img",
    "article .featured-image img",
    "article .hero-image img",
    "[class*='article-image'] img",
    "[class*='news-image'] img",
    "[class*='story-image'] img",
    "[class*='featured-img'] img",
    "[class*='lead-image'] img",
    "[class*='post-image'] img",
    "[class*='entry-image'] img",
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
    ".detail-media img",
    ".news-photo img",
    ".content-media img",
    ".article-img img",
    ".body img",
    # Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØªØ±
    "figure img",
    "picture source",
    "picture img",
    ".content img",
    "article img",
]

async def fetch_article_image(client: httpx.AsyncClient, url: str) -> "io.BytesIO | None":
    """
    ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡:
    Û±. CSS selectors Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† ØªØµÙˆÛŒØ± Ø®Ø¨Ø± Ø¯Ø± Ù…ØªÙ† Ù…Ù‚Ø§Ù„Ù‡
    Û². og:image / twitter:image ÙÙ‚Ø· Ø§Ú¯Ù‡ Ø¹Ø±Ø¶ â‰¥ Û¶Û°Û° Ø¨Ø§Ø´Ø¯
    Û³. ÙÛŒÙ„ØªØ± Ù„ÙˆÚ¯Ùˆ: Ø­Ø¬Ù… < Û±ÛµKB ÛŒØ§ Ø§Ø¨Ø¹Ø§Ø¯ < ÛµÛ°Û°Ã—Û²Û¸Û° ÛŒØ§ ratio < 1.3 â†’ Ø±Ø¯
    """
    if not url or len(url) < 10:
        return None
    skip_domains = ("t.me", "twitter.com", "x.com", "google.com/rss",
                    "feeds.reuters", "feeds.bbci", "feed.", "rss.")
    if any(d in url for d in skip_domains):
        return None

    try:
        r = await client.get(url,
            timeout=httpx.Timeout(10.0),
            headers={**COMMON_UA,
                     "Accept": "text/html,*/*;q=0.8",
                     "Sec-Fetch-Dest": "document"},
            follow_redirects=True)
        if r.status_code != 200:
            return None

        soup = BeautifulSoup(r.text, "html.parser")

        # â”€â”€ Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        candidates: list[tuple[str, int]] = []  # (url, priority)

        # Priority 1: CSS selector Ù‡Ø§ÛŒ article/news
        for sel in _IMG_SELECTORS:
            for el in soup.select(sel)[:3]:
                src = None
                if el.name == "source":
                    src = el.get("srcset", "").split(" ")[0]
                else:
                    # srcset â†’ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ†
                    ss = el.get("srcset", "")
                    if ss:
                        parts = [p.strip().split(" ") for p in ss.split(",") if p.strip()]
                        best = sorted(parts, key=lambda x: int(x[1].rstrip("w")) if len(x)>1 and x[1].rstrip("w").isdigit() else 0, reverse=True)
                        if best: src = best[0][0]
                    if not src:
                        src = el.get("src") or el.get("data-src") or el.get("data-lazy-src")
                if src and not src.startswith("data:"):
                    candidates.append((src, 10))

        # Priority 2: og:image
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            candidates.append((og["content"], 5))

        # og:image:width Ø¨Ø±Ø±Ø³ÛŒ
        og_w = soup.find("meta", property="og:image:width")
        if og_w:
            try:
                w = int(og_w.get("content", 0))
                if w < 500 and candidates:
                    # og:image Ú©ÙˆÚ†Ú© Ø§Ø³Øª â†’ Ø§ÙˆÙ„ÙˆÛŒØª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
                    candidates = [(u, p-3 if u == og.get("content") else p) for u, p in candidates]
            except: pass

        # Priority 3: twitter:image
        for name in ("twitter:image", "twitter:image:src"):
            tw = soup.find("meta", attrs={"name": name})
            if tw and tw.get("content"):
                candidates.append((tw["content"], 4)); break

        if not candidates:
            return None

        # â”€â”€ ÙÛŒÙ„ØªØ± Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        from urllib.parse import urlparse
        base_p = urlparse(r.url)  # URL Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø¹Ø¯ Ø§Ø² redirect)

        # Ù…Ø±ØªØ¨ Ø§Ø² Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
        candidates.sort(key=lambda x: -x[1])
        tried_urls = set()

        for img_url, _ in candidates[:8]:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ URL
            if img_url.startswith("//"):
                img_url = "https:" + img_url
            elif img_url.startswith("/"):
                img_url = f"{base_p.scheme}://{base_p.netloc}{img_url}"
            elif not img_url.startswith("http"):
                continue

            # Ø­Ø°Ù query string Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
            clean_url = img_url.lower().split("?")[0]

            # ÙÛŒÙ„ØªØ± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù„ÙˆÚ¯Ùˆ Ø¯Ø± URL
            if any(p in clean_url for p in _SKIP_IMG_PATTERNS):
                log.debug(f"ğŸ–¼ skip-url: {img_url[:60]}")
                continue

            if img_url in tried_urls:
                continue
            tried_urls.add(img_url)

            # Ø¯Ø§Ù†Ù„ÙˆØ¯
            try:
                ir = await client.get(img_url,
                    timeout=httpx.Timeout(12.0),
                    headers={**COMMON_UA, "Accept": "image/*,*/*;q=0.5"},
                    follow_redirects=True)
                if ir.status_code != 200:
                    continue
            except Exception as de:
                log.debug(f"ğŸ–¼ dl-err: {de}"); continue

            raw   = ir.content
            ctype = ir.headers.get("content-type", "")

            # Ø­Ø¬Ù… Ú©Ù… â†’ Ù„ÙˆÚ¯Ùˆ
            if len(raw) < 15_000:
                log.debug(f"ğŸ–¼ skip-small: {len(raw)}B")
                continue

            # Ú†Ú© Ù†ÙˆØ¹ ØªØµÙˆÛŒØ±
            is_img = (
                ctype.startswith("image/") or
                raw[:3]  == b'\xff\xd8\xff' or
                raw[:8]  == b'\x89PNG\r\n\x1a\n' or
                raw[:6]  in (b'GIF87a', b'GIF89a') or
                raw[:4]  == b'RIFF' or
                raw[:4]  == b'WEBP'
            )
            if not is_img:
                continue

            # PIL: Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ resize
            if PIL_OK:
                try:
                    tmp = Image.open(io.BytesIO(raw))
                    w, h = tmp.size
                    # Ø¹Ø±Ø¶ < ÛµÛ°Û° ÛŒØ§ Ø§Ø±ØªÙØ§Ø¹ < Û²Û¸Û° â†’ Ù„ÙˆÚ¯Ùˆ/Ø¨Ù†Ø±
                    if w < 500 or h < 280:
                        log.debug(f"ğŸ–¼ skip-dim: {w}Ã—{h}")
                        continue
                    # Ù†Ø³Ø¨Øª < 1.3 â†’ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù…Ø±Ø¨Ø¹ ÛŒØ§ Ø¹Ù…ÙˆØ¯ÛŒ = Ù„ÙˆÚ¯Ùˆ
                    ratio = w / max(h, 1)
                    if ratio < 1.3:
                        log.debug(f"ğŸ–¼ skip-ratio: {ratio:.2f} ({w}Ã—{h})")
                        continue
                    img_rgb = tmp.convert("RGB")
                    if w > 1600 or h > 1000:
                        img_rgb.thumbnail((1600, 1000), Image.LANCZOS)
                    out = io.BytesIO()
                    img_rgb.save(out, "JPEG", quality=88, optimize=True)
                    out.seek(0)
                    log.info(f"ğŸ–¼ âœ… {w}Ã—{h} r={ratio:.1f}  {img_url[:55]}")
                    return out
                except Exception as pe:
                    log.debug(f"ğŸ–¼ PIL-err: {pe}"); continue
            else:
                buf = io.BytesIO(raw); buf.seek(0)
                return buf

        return None

    except Exception as e:
        log.debug(f"fetch_img {url[:55]}: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    global _TW_SEMA
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID Ù†ÛŒØ³Øª!"); return

    # â”€â”€ semaphore Twitter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _TW_SEMA = asyncio.Semaphore(20)  # Û²Û° handle Ù‡Ù…Ø²Ù…Ø§Ù†

    # â”€â”€ cutoff Ù‡ÙˆØ´Ù…Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # = Ø¢Ø®Ø±ÛŒÙ† Ø§Ø¬Ø±Ø§ - BUFFER â†’ ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± ÙˆØ§Ù‚Ø¹Ø§Ù‹ ØªØ§Ø²Ù‡
    last_run   = load_run_state()
    cutoff     = last_run - timedelta(minutes=CUTOFF_BUFFER_MIN)
    # Ø­Ø¯Ø§Ú©Ø«Ø± MAX_LOOKBACK_MIN Ø¨Ù‡ Ø¹Ù‚Ø¨ (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ / Ø¨Ø¹Ø¯ Ø§Ø² crash)
    max_cutoff = datetime.now(timezone.utc) - timedelta(minutes=MAX_LOOKBACK_MIN)
    cutoff     = max(cutoff, max_cutoff)

    seen    = load_seen()
    stories = load_stories()

    log.info("=" * 65)
    log.info(f"ğŸš€ WarBot v18  |  {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')}")
    log.info(f"   ğŸ“¡ {len(ALL_RSS_FEEDS)} RSS  ğŸ“¢ {len(TELEGRAM_CHANNELS)} TG  ğ• {len(TWITTER_HANDLES)} TW")
    log.info(f"   PIL:{'âœ…' if PIL_OK else 'âŒ'}  seen:{len(seen)}")
    log.info(f"   â± cutoff={cutoff.astimezone(TEHRAN_TZ).strftime('%H:%M')} ØªÙ‡Ø±Ø§Ù†"
             f"  (last_run={last_run.astimezone(TEHRAN_TZ).strftime('%H:%M')})")
    log.info("=" * 65)

    limits = httpx.Limits(max_connections=100, max_keepalive_connections=30)
    async with httpx.AsyncClient(follow_redirects=True, limits=limits) as client:

        # â”€â”€ ADS-B + fetch Ù…ÙˆØ§Ø²ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        flight_task = asyncio.create_task(fetch_military_flights(client))
        raw_task    = asyncio.create_task(fetch_all(client, cutoff))
        (flight_msgs, flight_aircraft), raw = await asyncio.gather(flight_task, raw_task)
        log.info(f"ğŸ“¥ {len(raw)} Ø¢ÛŒØªÙ… Ø®Ø§Ù…  âœˆï¸ {len(flight_msgs)} ØªØ­Ø±Ú© ({len(flight_aircraft)} Ø¨Ø§ Ù…ÙˆÙ‚Ø¹ÛŒØª)")

        # â”€â”€ Ù¾Ø±Ø¯Ø§Ø²Ø´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        collected = []
        sent_ids  = set()
        cnt_old = cnt_irrel = cnt_url = cnt_story = 0

        for entry, src_name, src_type, is_emb in raw:
            eid = make_id(entry)

            # Ù„Ø§ÛŒÙ‡ Û±: Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ØŸ
            if eid in seen:
                cnt_url += 1; continue

            # Ù„Ø§ÛŒÙ‡ Û²: Ø¯Ø± Ù¾Ù†Ø¬Ø±Ù‡ Ø²Ù…Ø§Ù†ÛŒØŸ (TG Ù‚Ø¨Ù„Ø§Ù‹ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ØŒ RSS Ø§ÛŒÙ†Ø¬Ø§)
            if not is_fresh(entry, cutoff):
                cnt_old += 1; continue

            # Ù„Ø§ÛŒÙ‡ Û³: Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¬Ù†Ú¯ØŸ
            t    = clean_html(entry.get("title", ""))
            s    = clean_html(entry.get("summary") or entry.get("description") or "")
            full = f"{t} {s}"
            if not is_war_relevant(full, is_embassy=is_emb,
                                   is_tg=(src_type=="tg"), is_tw=(src_type=="tw")):
                cnt_irrel += 1; continue

            # Ù„Ø§ÛŒÙ‡ Û´: story ØªÚ©Ø±Ø§Ø±ÛŒØŸ
            if is_story_dup(t, stories):
                seen.add(eid)   # story-dup â†’ Ø¨Ù‡ seen Ø§Ø¶Ø§ÙÙ‡ (Ø¨Ø±Ø§ÛŒ Ù‡Ø± run ØªÚ©Ø±Ø§Ø± Ù†Ø´Ù‡)
                cnt_story += 1; continue

            collected.append((eid, entry, src_name, src_type, is_emb))
            stories = register_story(t, stories)

        log.info(
            f"ğŸ“Š Ù‚Ø¯ÛŒÙ…ÛŒ:{cnt_old}  Ù†Ø§Ù…Ø±ØªØ¨Ø·:{cnt_irrel}  "
            f"dup:{cnt_url}  story-dup:{cnt_story}  âœ… {len(collected)} Ø®Ø¨Ø±"
        )

        # Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø§ÙˆÙ„ØŒ Ø­Ø¯Ø§Ú©Ø«Ø± MAX_NEW_PER_RUN
        collected = list(reversed(collected))
        if len(collected) > MAX_NEW_PER_RUN:
            log.warning(f"âš ï¸ {len(collected)} â†’ {MAX_NEW_PER_RUN} (Ø¨Ø±Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯)")
            collected = collected[-MAX_NEW_PER_RUN:]

        # â”€â”€ ADS-B â€” Ù†Ù‚Ø´Ù‡ + Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if flight_aircraft:
            # Ù†Ù‚Ø´Ù‡ PIL Ø¨Ø§ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§
            map_buf = make_flight_map(flight_aircraft)
            if map_buf:
                regions = set(a["region"] for a in flight_aircraft)
                cap_parts = [f"âœˆï¸ <b>ØªØ­Ø±Ú©Ø§Øª Ù‡ÙˆØ§ÛŒÛŒ Ù†Ø¸Ø§Ù…ÛŒ â€” {' | '.join(regions)}</b>"]
                for ac in flight_aircraft[:8]:
                    cap_parts.append(
                        f"â€¢ <code>{ac['callsign']}</code> ({ac['type']}) "
                        f"Ø§Ø±ØªÙØ§Ø¹:{int(ac['alt'])//1000 if ac['alt'] else '?'}k ft "
                        f"Ø³Ø±Ø¹Øª:{ac['gs']} kt â€” {ac['region']}"
                    )
                cap_parts.append(f"\nğŸ• {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')}")
                await tg_send_photo(client, map_buf, "\n".join(cap_parts))
                await asyncio.sleep(0.8)
            else:
                # Ø¨Ø¯ÙˆÙ† PIL: Ø§Ø±Ø³Ø§Ù„ Ù…ØªÙ†ÛŒ
                for msg in flight_msgs[:5]:
                    await tg_send_text(client, msg)
                    await asyncio.sleep(0.5)
        elif flight_msgs:
            for msg in flight_msgs[:3]:
                await tg_send_text(client, msg)
                await asyncio.sleep(0.5)

        if not collected:
            log.info("ğŸ’¤ Ø®Ø¨Ø± Ø¬Ù†Ú¯ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ù†ÛŒØ³Øª")
            save_seen(seen); save_stories(stories); save_run_state()
            return

        # â”€â”€ ØªØ±Ø¬Ù…Ù‡ â€” Ù‡Ù…ÛŒØ´Ù‡ (Gemini ÛŒØ§ MyMemory) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        arts_in = [
            (trim(clean_html(e.get("title", "")), 400),
             trim(clean_html(e.get("summary") or e.get("description") or ""), 600))
            for _, e, _, _, _ in collected
        ]
        log.info(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡ {len(arts_in)} Ø®Ø¨Ø±...")
        translations = await translate_batch(client, arts_in)

        # â”€â”€ Ø§Ø±Ø³Ø§Ù„ â€” ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ â€” ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        sent = 0
        for i, (eid, entry, src_name, stype, is_emb) in enumerate(collected):
            fa_title, fa_body = translations[i]
            en_title          = arts_in[i][0]
            link              = entry.get("link", "")
            dt_str            = format_dt(entry)

            # â”€â”€ Ø¨Ø±Ø±Ø³ÛŒ: Ø¢ÛŒØ§ ØªØ±Ø¬Ù…Ù‡ ÙØ§Ø±Ø³ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Ø§Ú¯Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù‡Ù†ÙˆØ² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ø³Øª â†’ skip (Ø§Ø±Ø³Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù…Ù…Ù†ÙˆØ¹)
            title_is_fa = _is_farsi(fa_title) if fa_title else False
            orig_is_fa  = _is_farsi(en_title)   # Ø®Ø¨Ø± Ø§ØµÙ„ ÙØ§Ø±Ø³ÛŒ Ø¨ÙˆØ¯ØŸ

            if not title_is_fa and not orig_is_fa:
                # ØªØ±Ø¬Ù…Ù‡ fail Ú©Ø±Ø¯ Ùˆ Ø®Ø¨Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ø³Øª â†’ skip
                log.info(f"  â­ skip (no FA): {en_title[:55]}")
                # Ø¨Ù‡ seen Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù† ØªØ§ Ø¯ÙØ¹Ù‡ Ø¨Ø¹Ø¯ retry Ø´ÙˆØ¯
                continue

            # Ø¹Ù†ÙˆØ§Ù† Ù†Ù‡Ø§ÛŒÛŒ: Ø§Ú¯Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ØŒ ÙˆÚ¯Ø±Ù†Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§ØµÙ„ÛŒ
            display = fa_title.strip() if title_is_fa else en_title.strip()

            # â”€â”€ Ù…ØªÙ† ØªÙˆØ¶ÛŒØ­ â€” ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            body_fa = ""
            if fa_body and _is_farsi(fa_body) and len(fa_body) > 15:
                body_fa = fa_body.strip()
            elif not body_fa and _is_farsi(en_title) and arts_in[i][1]:
                # Ø®Ø¨Ø± Ø§ØµÙ„ ÙØ§Ø±Ø³ÛŒ Ø¨ÙˆØ¯ â†’ Ø§Ø² body Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                orig_body = arts_in[i][1]
                if _is_farsi(orig_body):
                    body_fa = orig_body.strip()

            urgent = any(w in (fa_title + fa_body + en_title).lower() for w in [
                "attack","strike","killed","bomb","explosion","nuclear","missile",
                "Ø­Ù…Ù„Ù‡","Ú©Ø´ØªÙ‡","Ø§Ù†ÙØ¬Ø§Ø±","Ù…ÙˆØ´Ú©","Ø´Ù‡ÛŒØ¯","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","ÙÙˆØ±ÛŒ","Ø§Ø¹Ù„Ø§Ù… Ø¬Ù†Ú¯","Ø¨Ù…Ø¨",
            ])
            sentiment_icons = analyze_sentiment(f"{fa_title} {fa_body} {en_title}")
            s_bar           = sentiment_bar(sentiment_icons)
            importance      = calc_importance(en_title, "", sentiment_icons, stype)
            log.info(f"  â†’ [{stype}] imp={importance}  {display[:65]}")

            # â”€â”€ caption â€” ÙØ§Ø±Ø³ÛŒ Ø®Ø§Ù„ØµØŒ Ø¨Ø¯ÙˆÙ† Ù„ÛŒÙ†Ú©ØŒ Ø¨Ø¯ÙˆÙ† Ù…Ù†Ø¨Ø¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            cap_parts = [s_bar, f"<b>{esc(display)}</b>"]
            if body_fa and body_fa[:60] not in display[:60]:
                cap_parts += ["", esc(trim(body_fa, 800))]
            if dt_str:
                cap_parts.append(f"\nğŸ• {dt_str}")
            caption = "\n".join(cap_parts)

            card_sent = False

            # â”€â”€ ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø§Ø² Ø³Ø§ÛŒØª Ø®Ø¨Ø± (RSS only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if link and stype == "rss":
                img_buf = await fetch_article_image(client, link)
                if img_buf:
                    ok = await tg_send_photo(client, img_buf, caption[:1024])
                    if ok:
                        card_sent = True
                        log.info("    âœ… ØªØµÙˆÛŒØ± Ø®Ø¨Ø± + ÙØ§Ø±Ø³ÛŒ")

            # â”€â”€ fallback: Ù…ØªÙ† Ø®Ø§Ù„Øµ ÙØ§Ø±Ø³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if not card_sent:
                ok = await tg_send_text(client, caption)
                if ok:
                    card_sent = True
                    log.info("    âœ… Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ")

            if card_sent:
                sent_ids.add(eid)
                seen.add(eid)   # ÙÙˆØ±ÛŒ Ø¨Ù‡ seen Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† ØªØ§ Ø§Ø±Ø³Ø§Ù„ Ù…Ø¬Ø¯Ø¯ Ù†Ø´ÙˆØ¯
                sent += 1
            await asyncio.sleep(SEND_DELAY)

        # ÙÙ‚Ø· Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ seen
        seen.update(sent_ids)
        save_seen(seen); save_stories(stories); save_run_state()
        log.info(f"ğŸ {sent}/{len(collected)} Ø®Ø¨Ø±  seen:{len(seen)}")


if __name__ == "__main__":
    asyncio.run(main())
