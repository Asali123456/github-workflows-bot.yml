"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ›¡ï¸ Military Intel Bot â€” Translated Edition                      â•‘
â•‘     Iran Â· Israel Â· USA  |  RSS + Google News + Twitter/X (Nitter)      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os, json, hashlib, time, re, logging, asyncio
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz
from deep_translator import GoogleTranslator

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN   = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID  = os.environ.get("CHANNEL_ID", "")
SEEN_FILE   = "seen.json"
MAX_NEW_PER_RUN = 50          # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ù‡ ÛµÛ° Ø¨Ø±Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ù†Ø¯Ø§Ø¯Ù† Ø®Ø¨Ø±Ù‡Ø§
SEND_DELAY  = 3               
MAX_MSG_LEN = 4000
TEHRAN_TZ   = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ (Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ + ØªÙˆÛŒÛŒØªØ± + Ú¯ÙˆÚ¯Ù„ Ù†ÛŒÙˆØ²)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [
    {"name": "ğŸŒ Axios NatSec",       "url": "https://api.axios.com/feed/national-security"},
    {"name": "ğŸŒ Axios World",        "url": "https://api.axios.com/feed/world"},
    {"name": "ğŸŒ Reuters Defense",    "url": "https://feeds.reuters.com/reuters/worldNews"},
    {"name": "ğŸŒ CNN Middle East",    "url": "http://rss.cnn.com/rss/edition_meast.rss"},
    {"name": "ğŸŒ Fox News World",     "url": "https://moxie.foxnews.com/google-publisher/world.xml"},
    {"name": "ğŸŒ Al Jazeera",         "url": "https://www.aljazeera.com/xml/rss/all.xml"},
    {"name": "ğŸŒ Politico Defense",   "url": "https://rss.politico.com/defense.xml"},
    {"name": "ğŸŒ AP Defense",         "url": "https://apnews.com/hub/military-and-defense?format=rss"},
    {"name": "ğŸ‡ºğŸ‡¸ Pentagon",          "url": "https://www.defense.gov/DesktopModules/ArticleCS/RSS.ashx?ContentType=1&Site=945&max=10"},
    {"name": "ğŸ‡ºğŸ‡¸ CENTCOM",           "url": "https://www.centcom.mil/RSS/"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",  "url": "https://breakingdefense.com/feed/"},
    {"name": "ğŸ‡®ğŸ‡± IDF Official",      "url": "https://www.idf.il/en/mini-sites/idf-spokesperson-english/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Jerusalem Post",    "url": "https://www.jpost.com/rss/rssfeedsmilitary.aspx"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",   "url": "https://www.timesofisrael.com/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Haaretz",          "url": "https://www.haaretz.com/cmlink/1.4455099"},
    {"name": "ğŸ‡®ğŸ‡· Iran International","url": "https://www.iranintl.com/en/rss"},
    {"name": "ğŸ‡®ğŸ‡· Radio Farda",       "url": "https://www.radiofarda.com/api/zmqpqopvp"},
    {"name": "ğŸŒ Middle East Eye",    "url": "https://www.middleeasteye.net/rss"},
    {"name": "ğŸŒ ISW (Institute)",    "url": "https://www.understandingwar.org/rss.xml"},
]

GOOGLE_NEWS_QUERIES = [
    ("ğŸ“° Axios Iran",              "site:axios.com Iran Israel military attack"),
    ("ğŸ“° Reuters Iran Israel",     "site:reuters.com Iran Israel military strike"),
    ("âš”ï¸ Iran Israel War",          "Iran Israel war attack strike military"),
    ("âš”ï¸ US Forces Middle East",    "US forces CENTCOM Iraq Syria base attack Iran"),
    ("âš”ï¸ Hezbollah IRGC",           "Hezbollah IRGC proxy militia Lebanon strike"),
]

def google_news_url(query: str) -> str:
    q = query.replace(" ", "+")
    return f"https://news.google.com/rss/search?q={q}&hl=en-US&gl=US&ceid=US:en&num=10"

GOOGLE_FEEDS = [{"name": name, "url": google_news_url(q), "is_google": True} for name, q in GOOGLE_NEWS_QUERIES]

TWITTER_ACCOUNTS = [
    ("ğŸ“° Barak Ravid (Axios)",      "BarakRavid"),
    ("ğŸ“° Natasha Bertrand (CNN)",   "NatashaBertrand"),
    ("ğŸ“° Idrees Ali (Reuters)",     "idreesali114"),
    ("ğŸ“° Lucas Tomlinson (Fox)",    "LucasFoxNews"),
    ("ğŸ“° Farnaz Fassihi (NYT)",     "farnazfassihi"),
    ("ğŸ” OSINT Defender",    "OSINTdefender"),
    ("ğŸ” Intel Crab",        "IntelCrab"),
    ("ğŸ‡®ğŸ‡± IDF Official",    "IDF"),
    ("ğŸ‡ºğŸ‡¸ CENTCOM",         "CENTCOM"),
]

NITTER_MIRRORS = [
    "https://nitter.poast.org",
    "https://nitter.privacydev.net",
    "https://nitter.1d4.us",
]

def get_nitter_feeds() -> list[dict]:
    feeds = []
    for name, handle in TWITTER_ACCOUNTS:
        for mirror in NITTER_MIRRORS:
            feeds.append({"name": f"ğ• {name}", "url": f"{mirror}/{handle}/rss", "nitter_handle": handle})
            break 
    return feeds

NITTER_FEEDS = get_nitter_feeds()
ALL_FEEDS = RSS_FEEDS + GOOGLE_FEEDS + NITTER_FEEDS

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙˆØ§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ÙÛŒÙ„ØªØ± (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¬Ø§ Ù…Ø§Ù†Ø¯Ù† Ø®Ø¨Ø±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def is_recent(entry: dict, hours: int = 48) -> bool:
    """ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø°Ù Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø®Ø¨Ø±Ù‡Ø§ Ø¨Ø®Ø§Ø·Ø± Ù…Ù†Ø·Ù‚Ù‡ Ø²Ù…Ø§Ù†ÛŒØŒ Û´Û¸ Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ± Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… """
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t: return True
        dt = datetime(*t[:6], tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        return (now - dt) <= timedelta(hours=hours)
    except:
        return True

def is_relevant(entry: dict, is_twitter: bool = False) -> bool:
    text = " ".join([
        str(entry.get("title", "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()
    
    if is_twitter:
        if any(kw in text for kw in ["iran", "israel", "us ", "strike", "war", "gaza", "lebanon", "irgc", "idf", "military", "attack", "missile"]):
            return True
        return False
        
    KEYWORDS = ["iran", "irgc", "tehran", "khamenei", "israel", "idf", "mossad", "tel aviv", "netanyahu",
                "us forces", "centcom", "pentagon", "american base", "strike", "airstrike", "Ø³Ù¾Ø§Ù‡", "Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„", "Ø­Ù…Ù„Ù‡"]
    return any(kw in text for kw in KEYWORDS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…ÙˆØªÙˆØ± ØªØ±Ø¬Ù…Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def translate_to_fa(text: str) -> str:
    if not text or len(text.strip()) < 3:
        return ""
    try:
        translated = GoogleTranslator(source='auto', target='fa').translate(text)
        return translated
    except Exception as e:
        log.error(f"Translation Error: {e}")
        return text  # Ø¯Ø± ØµÙˆØ±Øª Ù‚Ø·Ø¹ÛŒ Ù…ØªØ±Ø¬Ù…ØŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø±Ùˆ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡

def clean_html(text: str) -> str:
    if not text: return ""
    return BeautifulSoup(str(text), "html.parser").get_text(" ", strip=True)

def truncate(text: str, n: int = 300) -> str:
    if len(text) <= n: return text
    return text[:n].rsplit(" ", 1)[0] + "â€¦"

def make_id(entry: dict) -> str:
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def format_dt(entry: dict) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except:
        pass
    return ""

def escape_html(text: str) -> str:
    return text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def build_message(entry: dict, source: str, is_twitter: bool = False) -> str:
    en_title   = clean_html(entry.get("title", "No Title")).strip()
    en_summary = clean_html(entry.get("summary") or entry.get("description") or "")
    link       = entry.get("link", "")
    dt         = format_dt(entry)

    # ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† Ùˆ Ø®Ù„Ø§ØµÙ‡
    fa_title = escape_html(translate_to_fa(en_title))
    fa_summary_short = escape_html(translate_to_fa(truncate(en_summary, 300)))
    en_title_escaped = escape_html(en_title)

    icon = "ğ•" if is_twitter else "ğŸ“¡"

    lines = [f"ğŸ”´ <b>{fa_title}</b>", ""]
    
    # Ø§Ú¯Ø± Ø®Ù„Ø§ØµÙ‡ Ø®Ø¨Ø± Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† ÙØ±Ù‚ Ø¯Ø§Ø´Øª (ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨ÙˆØ¯)ØŒ Ø®Ù„Ø§ØµÙ‡â€ŒÛŒ ÙØ§Ø±Ø³ÛŒ Ø±Ùˆ Ù‡Ù… Ù…ÛŒØ°Ø§Ø±ÛŒÙ…
    if fa_summary_short and fa_summary_short.lower() not in fa_title.lower():
        lines += [f"ğŸ”¹ <i>{fa_summary_short}</i>", ""]
        
    lines += [
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
        f"ğŸ‡ºğŸ‡¸ <b>Ù…ØªÙ† Ø§ØµÙ„ÛŒ:</b>",
        f"<blockquote expandable>{en_title_escaped}</blockquote>"
    ]

    if dt: lines.append(dt)
    lines.append(f"{icon} <b>{source}</b>")
    if link: lines.append(f'ğŸ”— <a href="{link}">Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±</a>')

    return "\n".join(lines)

def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen: set):
    recent = list(seen)[-8000:]
    with open(SEEN_FILE, "w") as f: json.dump(recent, f)

def fetch_feed(cfg: dict) -> list:
    handle = cfg.get("nitter_handle")
    mirrors = NITTER_MIRRORS if handle else [None]

    for i, mirror in enumerate(mirrors):
        url = f"{mirror}/{handle}/rss" if handle else cfg["url"]
        try:
            parsed = feedparser.parse(url, request_headers={"User-Agent": "Mozilla/5.0 MilNewsBot/4.0"})
            if parsed.entries: return parsed.entries
        except Exception:
            pass
    return []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client: httpx.AsyncClient, text: str) -> bool:
    for attempt in range(4):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID,
                "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML",
                "disable_web_page_preview": True, # Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù„ÛŒÙ†Ú© Ø®Ø§Ù…ÙˆØ´ Ø´Ø¯ ØªØ§ Ù¾Ø³Øª Ù…Ø±ØªØ¨â€ŒØªØ± Ø¨Ø§Ø´Ø¯
            }, timeout=25)
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                wait = data.get("parameters", {}).get("retry_after", 30)
                await asyncio.sleep(wait)
            else:
                return False
        except Exception:
            await asyncio.sleep(8)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø¨Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ ØªÙˆÚ©Ù† Ø¨Ø§Øª ÛŒØ§ Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        return

    seen = load_seen()
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        collected: list[tuple] = [] 

        for cfg in ALL_FEEDS:
            is_tw = bool(cfg.get("nitter_handle"))
            entries = fetch_feed(cfg)
            
            for entry in entries:
                eid = make_id(entry)
                
                if eid in seen:
                    continue
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø²Ù…Ø§Ù†: ØªØ§ Û´Û¸ Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ Ø±Ùˆ Ú†Ú© Ù…ÛŒÚ©Ù†Ù‡ ØªØ§ Ú†ÛŒØ²ÛŒ Ø¬Ø§ Ù†Ù…ÙˆÙ†Ù‡
                if not is_recent(entry, hours=48):
                    seen.add(eid)
                    continue
                
                # ÙÛŒÙ„ØªØ± Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¬Ù†Ú¯ÛŒ
                if not is_relevant(entry, is_twitter=is_tw):
                    seen.add(eid)
                    continue
                    
                collected.append((eid, entry, cfg, is_tw))

        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø®Ø¨Ø±Ù‡Ø§ Ø§Ø² Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
        collected = collected[::-1]

        if len(collected) > MAX_NEW_PER_RUN:
            collected = collected[-MAX_NEW_PER_RUN:]

        sent = 0
        for eid, entry, cfg, is_tw in collected:
            msg = build_message(entry, cfg["name"], is_tw)
            if await tg_send(client, msg):
                seen.add(eid)
                sent += 1
                log.info(f"  âœ… [{cfg['name']}] ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"âœ”ï¸ Ù¾Ø§ÛŒØ§Ù† | {sent} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")

if __name__ == "__main__":
    asyncio.run(main())
