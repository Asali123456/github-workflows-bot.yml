"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸ›¡ï¸ Military Intel Bot v11 â€” FULLY REBUILT                        â•‘
â•‘                                                                          â•‘
â•‘  âœ… Fix1: Nitter Ø§Ø² status.d420.de â€” 8 instance Ú©Ø§Ø±â€ŒÚ©Ø±Ø¯Ù‡               â•‘
â•‘  âœ… Fix2: Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ (IRNA, Tasnim, Mehr, Fars, PressTV...)   â•‘
â•‘  âœ… Fix3: ÙÛŒÙ„ØªØ± Ø³Ø®Øª â€” ÙÙ‚Ø· Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†-Ø¢Ù…Ø±ÛŒÚ©Ø§-Ø§Ø³Ø±Ø§ÛŒÛŒÙ„                     â•‘
â•‘  âœ… Fix4: ØªØ±Ø¬Ù…Ù‡ Ø¹Ø§Ù…ÛŒØ§Ù†Ù‡ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ Ø³Ø¨Ú© ØªÙ„Ú¯Ø±Ø§Ù…                           â•‘
â•‘  âœ… Fix5: ØªÙˆÛŒÛŒØª Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ùˆ Ø³ÛŒØ§Ø³ØªÙ…Ø¯Ø§Ø±Ø§Ù† Ø§ÛŒØ±Ø§Ù†ÛŒ                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os, json, hashlib, asyncio, logging, re
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S")
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE         = "seen.json"
GEMINI_STATE_FILE = "gemini_state.json"
MAX_NEW_PER_RUN   = 20
MAX_MSG_LEN       = 4096
SEND_DELAY        = 2
TEHRAN_TZ         = pytz.timezone("Asia/Tehran")
CUTOFF_HOURS      = 6

def get_cutoff() -> datetime:
    return datetime.now(timezone.utc) - timedelta(hours=CUTOFF_HOURS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix2: Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ + Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [

    # â•â• Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ â•â•
    {"name": "ğŸ‡®ğŸ‡· IRNA",              "url": "https://www.irna.ir/rss/",                            "lang": "fa"},
    {"name": "ğŸ‡®ğŸ‡· IRNA English",      "url": "https://en.irna.ir/rss/",                             "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡· Tasnim",            "url": "https://www.tasnimnews.com/fa/rss/feed/0/8/0",        "lang": "fa"},
    {"name": "ğŸ‡®ğŸ‡· Tasnim English",    "url": "https://www.tasnimnews.com/en/rss/feed/0/8/0",        "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡· Mehr Agency",       "url": "https://en.mehrnews.com/rss",                         "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡· Mehr ÙØ§Ø±Ø³ÛŒ",        "url": "https://www.mehrnews.com/rss",                        "lang": "fa"},
    {"name": "ğŸ‡®ğŸ‡· Fars Agency",       "url": "https://www.farsnews.ir/rss",                         "lang": "fa"},
    {"name": "ğŸ‡®ğŸ‡· Fars English",      "url": "https://en.farsnews.ir/rss",                          "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡· Press TV",          "url": "https://www.presstv.ir/rss",                          "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡· Iran International","url": "https://www.iranintl.com/en/rss",                     "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡· Radio Farda",       "url": "https://www.radiofarda.com/api/zoyqvpemr",            "lang": "fa"},
    {"name": "ğŸ‡®ğŸ‡· Nour News",         "url": "https://www.nournews.ir/fa/rss/",                     "lang": "fa"},

    # â•â• Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â•â•
    {"name": "ğŸ‡®ğŸ‡± Jerusalem Post",    "url": "https://www.jpost.com/rss/rssfeedsheadlines.aspx",    "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",   "url": "https://www.timesofisrael.com/feed/",                 "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡± Israel Hayom",      "url": "https://www.israelhayom.com/feed/",                   "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡± Arutz Sheva",       "url": "https://www.israelnationalnews.com/rss.aspx",         "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡± i24 News",          "url": "https://www.i24news.tv/en/rss",                       "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡± Haaretz (GNews)",   "url": "https://news.google.com/rss/search?q=site:haaretz.com+iran+israel+war+military&hl=en-US&gl=US&ceid=US:en", "lang": "en"},
    {"name": "ğŸ‡®ğŸ‡± IDF (GNews)",       "url": "https://news.google.com/rss/search?q=IDF+Israel+Defense+Forces+operation+strike+iran&hl=en-US&gl=US&ceid=US:en", "lang": "en"},

    # â•â• Ø¢Ù…Ø±ÛŒÚ©Ø§/Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ â•â•
    {"name": "ğŸŒ Reuters World",      "url": "https://feeds.reuters.com/reuters/worldNews",         "lang": "en"},
    {"name": "ğŸŒ AP Military",        "url": "https://apnews.com/hub/military-and-defense?format=rss", "lang": "en"},
    {"name": "ğŸŒ AP World",           "url": "https://feeds.apnews.com/rss/apf-WorldNews",          "lang": "en"},
    {"name": "ğŸŒ BBC Middle East",    "url": "https://feeds.bbci.co.uk/news/world/middle_east/rss.xml", "lang": "en"},
    {"name": "ğŸŒ Al Jazeera",         "url": "https://www.aljazeera.com/xml/rss/all.xml",           "lang": "en"},
    {"name": "ğŸŒ CNN Middle East",    "url": "http://rss.cnn.com/rss/edition_meast.rss",            "lang": "en"},
    {"name": "ğŸŒ Fox News World",     "url": "https://moxie.foxnews.com/google-publisher/world.xml", "lang": "en"},
    {"name": "ğŸŒ Middle East Eye",    "url": "https://www.middleeasteye.net/rss",                   "lang": "en"},
    {"name": "ğŸŒ Bloomberg Politics", "url": "https://feeds.bloomberg.com/politics/news.rss",       "lang": "en"},
    {"name": "ğŸŒ Foreign Policy",     "url": "https://foreignpolicy.com/feed/",                     "lang": "en"},
    {"name": "ğŸŒ Politico NatSec",   "url": "https://rss.politico.com/defense.xml",                "lang": "en"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",  "url": "https://breakingdefense.com/feed/",                   "lang": "en"},
    {"name": "ğŸ‡ºğŸ‡¸ USNI News",         "url": "https://news.usni.org/feed",                          "lang": "en"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense News",      "url": "https://www.defensenews.com/arc/outboundfeeds/rss/",  "lang": "en"},
    {"name": "ğŸ‡ºğŸ‡¸ The War Zone",      "url": "https://www.twz.com/feed",                            "lang": "en"},
    {"name": "ğŸ” Long War Journal",   "url": "https://www.longwarjournal.org/feed",                 "lang": "en"},
    {"name": "ğŸ” Bellingcat",         "url": "https://www.bellingcat.com/feed/",                    "lang": "en"},
    {"name": "ğŸ” OSINT Defender",     "url": "https://osintdefender.com/feed/",                     "lang": "en"},
    {"name": "ğŸ‡¸ğŸ‡¦ Al-Monitor ME",     "url": "https://news.google.com/rss/search?q=site:al-monitor.com+iran+israel+us+military&hl=en-US&gl=US&ceid=US:en", "lang": "en"},
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Google News â€” Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù‡Ø¯ÙÙ…Ù†Ø¯ Ø¬Ù†Ú¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WAR_QUERIES = [
    ("âš”ï¸ Iran Israel War",     "Iran Israel war attack strike"),
    ("âš”ï¸ Iran US Military",    "United States Iran military attack"),
    ("âš”ï¸ IRGC Strike",         "IRGC Revolutionary Guard attack strike"),
    ("âš”ï¸ IDF Iran",            "IDF airstrike Iran nuclear"),
    ("âš”ï¸ Iran Nuclear",        "Iran nuclear IAEA uranium fordo natanz"),
    ("âš”ï¸ Iran Missile",        "Iran ballistic missile drone attack"),
    ("âš”ï¸ Hezbollah War",       "Hezbollah IDF Lebanon war"),
    ("âš”ï¸ US Base Attack",      "US military base Iraq Syria Iran attack"),
    ("âš”ï¸ Strait Hormuz",       "Strait Hormuz tanker seized navy"),
    ("âš”ï¸ Red Sea Houthis",     "Red Sea Houthi Yemen attack ship"),
    ("âš”ï¸ Iran Sanctions War",  "Iran sanctions military war escalation"),
    ("âš”ï¸ Gaza War 2026",       "Gaza Hamas IDF war 2026"),
    ("âš”ï¸ Iran Proxy",          "Iran proxy militia attack US Israel"),
    ("âš”ï¸ Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù† Ø§Ø³Ø±Ø§ÛŒÛŒÙ„",   "Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù† Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø­Ù…Ù„Ù‡"),
    ("âš”ï¸ Iran Trump War",      "Trump Iran Israel military war threat"),
]

def gnews(q):
    return f"https://news.google.com/rss/search?q={q.replace(' ','+')}&hl=en-US&gl=US&ceid=US:en&num=15"

WAR_FEEDS = [{"name": n, "url": gnews(q)} for n, q in WAR_QUERIES]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix1: Nitter â€” 8 instance Ú©Ø§Ø±â€ŒÚ©Ø±Ø¯Ù‡ Ø§Ø² status.d420.de
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ù†Ø¨Ø¹: https://status.d420.de (ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Û²Û² ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶)
NITTER_INSTANCES = [
    "https://xcancel.com",               # âœ… US â€” Ø¨Ù‡ØªØ±ÛŒÙ†
    "https://nitter.poast.org",          # âœ… US
    "https://nitter.privacyredirect.com",# âœ… Finland
    "https://nitter.tiekoetter.com",     # âœ… Germany
    "https://lightbrd.com",              # âœ… Turkey
    "https://nitter.space",              # âœ… US
    "https://nuku.trabun.org",           # âœ… Chile
    "https://nitter.catsarch.com",       # âœ… US/Germany
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix5: Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ùˆ Ø³ÛŒØ§Ø³ØªÙ…Ø¯Ø§Ø±Ø§Ù† â€” Ø§ÛŒØ±Ø§Ù† + Ø¢Ù…Ø±ÛŒÚ©Ø§ + Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_ACCOUNTS = [

    # â”€â”€ Ø§ÛŒØ±Ø§Ù† â€” Ø±Ø³Ù…ÛŒ Ùˆ Ø³ÛŒØ§Ø³ÛŒ â”€â”€
    ("ğŸ‡®ğŸ‡· Ø¨Ø§Ù‚Ø±ÛŒâ€ŒÚ©Ù†ÛŒ",           "Bagheri_Kani",    "ir"),   # Ù…Ø°Ø§Ú©Ø±Ù‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ
    ("ğŸ‡®ğŸ‡· Ø³Ø®Ù†Ú¯ÙˆÛŒ Ø³Ù¾Ø§Ù‡",         "IRGC_PRGC",       "ir"),   # Ø³Ù¾Ø§Ù‡
    ("ğŸ‡®ğŸ‡· ÙˆØ²Ø§Ø±Øª Ø®Ø§Ø±Ø¬Ù‡ Ø§ÛŒØ±Ø§Ù†",   "IRIMFA",          "ir"),   # ÙˆØ²Ø§Ø±Øª Ø®Ø§Ø±Ø¬Ù‡
    ("ğŸ‡®ğŸ‡· Press TV",             "PressTV",         "ir"),
    ("ğŸ‡®ğŸ‡· IRNA English",         "IrnaEnglish",     "ir"),
    ("ğŸ‡®ğŸ‡· Tasnim News",          "tasnimna",        "ir"),
    ("ğŸ‡®ğŸ‡· Iran Intl English",    "IranIntl_En",     "ir"),
    ("ğŸ‡®ğŸ‡· Nour News",            "NourNews_Ir",     "ir"),
    ("ğŸ‡®ğŸ‡· Ø±Ø¶Ø§ Ù†ØµØ±ÛŒ",             "rezanasri",       "ir"),   # ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ù‚ÙˆÙ‚ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„
    ("ğŸ‡®ğŸ‡· Ù‡ÙˆØ´Ù†Ú¯ Ø§Ù…ÛŒØ±Ø§Ø­Ù…Ø¯ÛŒ",     "hosseinami",      "ir"),   # Ø§Ø³ØªØ§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„

    # â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â€” Ø±Ø³Ù…ÛŒ Ùˆ Ù†Ø¸Ø§Ù…ÛŒ â”€â”€
    ("ğŸ‡®ğŸ‡± IDF Official",         "IDF",             "il"),
    ("ğŸ‡®ğŸ‡± ÛŒÙˆØ¢Ùˆ Ú¯Ø§Ù„Ø§Ù†Øª",          "yoavgallant",    "il"),   # ÙˆØ²ÛŒØ± Ø¯ÙØ§Ø¹ Ø³Ø§Ø¨Ù‚
    ("ğŸ‡®ğŸ‡± Ø³Øª ÙØ±Ø§Ù†ØªØ²Ù…Ù†",          "sfrantzman",      "il"),   # Jerusalem Post Ø¯ÙØ§Ø¹
    ("ğŸ‡®ğŸ‡± ÛŒÙˆØ³ÛŒ Ù…Ù„Ù…Ù†",            "yossi_melman",    "il"),   # Ù…ÙˆØ³Ø§Ø¯/Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    ("ğŸ‡®ğŸ‡± Ø¢ÙˆÛŒ Ø§ÛŒØ³Ø§Ø®Ø§Ø±ÙˆÙ",        "AviIssacharoff",  "il"),   # ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù†Ø¸Ø§Ù…ÛŒ
    ("ğŸ‡®ğŸ‡± Ø¨Ù† Ú©Ø§Ø³Ù¾ÛŒØª",            "BenCaspit",       "il"),   # ØªØ­Ù„ÛŒÙ„Ú¯Ø±

    # â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ â€” Ø±Ø³Ù…ÛŒ â”€â”€
    ("ğŸ‡ºğŸ‡¸ CENTCOM",              "CENTCOM",         "us"),
    ("ğŸ‡ºğŸ‡¸ Dept of Defense",      "DeptofDefense",   "us"),
    ("ğŸ‡ºğŸ‡¸ NatashaBertrand",      "NatashaBertrand", "us"),   # CNN Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ
    ("ğŸ‡ºğŸ‡¸ Helene Cooper",        "helenecooper",    "us"),   # NYT Pentagon
    ("ğŸ‡ºğŸ‡¸ Farnaz Fassihi",       "farnazfassihi",   "us"),   # NYT Ø§ÛŒØ±Ø§Ù†
    ("ğŸ‡ºğŸ‡¸ Barak Ravid",          "BarakRavid",      "us"),   # Axios Ø§Ø³Ø±Ø§ÛŒÛŒÙ„

    # â”€â”€ OSINT / Ø§Ø·Ù„Ø§Ø¹Ø§Øª â”€â”€
    ("ğŸ” OSINT Defender",        "OSINTdefender",   "osint"),
    ("ğŸ” Intel Crab",            "IntelCrab",       "osint"),
    ("ğŸ” War Monitor",           "WarMonitor3",     "osint"),
    ("ğŸ” Conflicts.media",       "Conflicts",       "osint"),
    ("ğŸ” Aurora Intel",          "AuroraIntel",     "osint"),
    ("ğŸ” GeoConfirmed",          "GeoConfirmed",    "osint"),

    # â”€â”€ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ø¨Ø±ØªØ± â”€â”€
    ("ğŸ“° Idrees Ali (Reuters)",  "idreesali114",    "reporter"),  # Pentagon
    ("ğŸ“° Phil Stewart (Reuters)","phil_stewart_",   "reporter"),
    ("ğŸ“° Jack Detsch (FP)",      "JackDetsch",      "reporter"),
    ("ğŸ“° Joyce Karam (Arab News)","Joyce_Karam",    "reporter"),

    # â”€â”€ Ù‡Ø´Ø¯Ø§Ø± â”€â”€
    ("âš ï¸ DEFCON Level",          "DEFCONLevel",     "alert"),
    ("âš ï¸ Arms Control Wonk",     "ArmsControlWonk", "alert"),
]

def get_nitter_feeds() -> list[dict]:
    feeds = []
    for name, handle, country in TWITTER_ACCOUNTS:
        # Ù‡Ù…Ù‡ instances Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† fallback
        urls = [f"{inst}/{handle}/rss" for inst in NITTER_INSTANCES]
        feeds.append({
            "name":    f"ğ• {name}",
            "handle":  handle,
            "country": country,
            "urls":    urls,
            "is_twitter": True,
        })
    return feeds

NITTER_FEEDS = get_nitter_feeds()
ALL_FEEDS    = RSS_FEEDS + WAR_FEEDS

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix3: ÙÛŒÙ„ØªØ± Ø³Ø®Øª â€” ÙÙ‚Ø· Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†-Ø¢Ù…Ø±ÛŒÚ©Ø§-Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø§Ø² Ù‡Ø± Ú¯Ø±ÙˆÙ‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯

# Ú¯Ø±ÙˆÙ‡ Ø§ÛŒØ±Ø§Ù†
IRAN_KW = {"iran","irgc","tehran","khamenei","sepah","Ø³Ù¾Ø§Ù‡","Ø§ÛŒØ±Ø§Ù†","ØªÙ‡Ø±Ø§Ù†","Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ",
           "revolutionary guard","Ù…Ù‚Ø§ÙˆÙ…Øª","Ø­Ø´Ø¯Ø§Ù„Ø´Ø¹Ø¨ÛŒ","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡","Ø­ÙˆØ«ÛŒ",
           "Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","enrichment","natanz","fordo","iaea","nuclear",
           "hezbollah","houthi","ansarallah","hamas","ÙÙ„Ø³Ø·ÛŒÙ†"}

# Ú¯Ø±ÙˆÙ‡ Ø¢Ù…Ø±ÛŒÚ©Ø§
US_KW = {"us ","usa","america","american","pentagon","centcom","us forces","us military",
         "us base","trump","rubio","Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø§Ù…Ø±ÛŒÚ©Ø§","ÙˆØ§Ø´Ù†Ú¯ØªÙ†","Ù¾Ù†ØªØ§Ú¯ÙˆÙ†","ØªØ±Ø§Ù…Ù¾",
         "white house","state department","secretary"}

# Ú¯Ø±ÙˆÙ‡ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
ISRAEL_KW = {"israel","idf","mossad","netanyahu","tel aviv","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„","Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ",
             "mos","haifa","jerusalem","Ø§ÙˆØ±Ø´Ù„ÛŒÙ…","ØªÙ„â€ŒØ¢ÙˆÛŒÙˆ","ØµÙ‡ÛŒÙˆÙ†ÛŒØ³Øª","ØµÙ‡ÛŒÙˆÙ†ÛŒØ³Ù…"}

# Ú©Ù„Ù…Ø§Øª Ø¬Ù†Ú¯ÛŒ â€” Ø­ØªÙ…Ø§Ù‹ Ø¨Ø§ÛŒØ¯ ÛŒÚ©ÛŒ Ø§Ø² Ø§ÛŒÙ†Ù‡Ø§ Ø¨Ø§Ø´Ø¯
WAR_KW = {"attack","strike","airstrike","missile","bomb","war","military","operation",
          "sanction","nuclear","drone","uav","Ø­Ù…Ù„Ù‡","Ø­Ù…Ù„Ù‡ Ù‡ÙˆØ§ÛŒÛŒ","Ù…ÙˆØ´Ú©","Ø¬Ù†Ú¯","Ø¹Ù…Ù„ÛŒØ§Øª",
          "Ø¨Ù…Ø¨","ØªØ­Ø±ÛŒÙ…","Ù†Ø¸Ø§Ù…ÛŒ","Ù¾Ù‡Ù¾Ø§Ø¯","explosion","kill","assassin","Ù†Ø¸Ø§Ù…ÛŒ","artillery",
          "escalat","deploy","troops","force","rocket","shell","invasion","blockade",
          "threat","ultimatum","siege","seized","seized","intercept","intercept"}

def is_war_relevant(entry: dict, is_twitter: bool = False) -> bool:
    """ÙÛŒÙ„ØªØ± Ø³Ø®Øª: Ø¨Ø§ÛŒØ¯ Ø¬Ù†Ú¯ Ø¨ÛŒÙ† Ø§ÛŒØ±Ø§Ù†-Ø¢Ù…Ø±ÛŒÚ©Ø§-Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø¨Ø§Ø´Ø¯"""
    text = " ".join([
        str(entry.get("title",   "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()

    # Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ù‡Ø± Ø·Ø±Ù
    has_iran   = any(k in text for k in IRAN_KW)
    has_us     = any(k in text for k in US_KW)
    has_israel = any(k in text for k in ISRAEL_KW)
    has_war    = any(k in text for k in WAR_KW)

    if not has_war:
        return False

    # Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ Û² Ø·Ø±Ù Ø§Ø² Û³ Ø¯Ø±Ú¯ÛŒØ± Ø¨Ø§Ø´Ù†Ø¯
    sides = sum([has_iran, has_us, has_israel])
    if sides >= 2:
        return True

    # ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø·Ø±Ù Ø¨Ø§ Ú©Ù„Ù…Ù‡ Ø¬Ù†Ú¯ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ
    if is_twitter and has_iran and has_war:
        return True
    if sides == 1 and has_war:
        # Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒØ±Ø§Ù†ÛŒ/Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ÛŒØŒ ÛŒÚ© Ø·Ø±Ù Ú©Ø§ÙÛŒÙ‡
        source_name = entry.get("_source_name", "")
        if "Ø§ÛŒØ±Ø§Ù†" in source_name or "Iran" in source_name or "Israel" in source_name:
            return True

    return False

def is_fresh(entry: dict) -> bool:
    cutoff = get_cutoff()
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t: return False
        return datetime(*t[:6], tzinfo=timezone.utc) >= cutoff
    except:
        return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª RSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_one_rss(client: httpx.AsyncClient, cfg: dict) -> list:
    try:
        r = await client.get(cfg["url"], timeout=httpx.Timeout(12.0),
                             headers={"User-Agent": "Mozilla/5.0 MilNewsBot/11.0"})
        if r.status_code == 200:
            entries = feedparser.parse(r.text).entries
            for e in entries:
                e["_source_name"] = cfg["name"]
            return entries or []
    except: pass
    return []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix1: Nitter Ø¨Ø§ 8 instance fallback
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_one_nitter(client: httpx.AsyncClient, cfg: dict) -> list:
    """ØªØ³Øª Ù‡Ø± instance Ø¨Ù‡ ØªØ±ØªÛŒØ¨ â€” Ø§ÙˆÙ„ÛŒ Ú©Ù‡ Ø¬ÙˆØ§Ø¨ Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒÙ‡"""
    for url in cfg["urls"]:
        try:
            r = await client.get(url, timeout=httpx.Timeout(10.0),
                                 headers={"User-Agent": "Mozilla/5.0 MilNewsBot/11.0"})
            if r.status_code == 200:
                entries = feedparser.parse(r.text).entries
                if entries:
                    log.debug(f"  ğ• {cfg['handle']} â† {url.split('/')[2]}")
                    for e in entries:
                        e["_source_name"] = cfg["name"]
                    return entries
        except: continue
    return []

async def fetch_all(client: httpx.AsyncClient) -> list:
    # RSS + GNews Ù‡Ù…Ø²Ù…Ø§Ù†
    rss_tasks = [fetch_one_rss(client, cfg) for cfg in ALL_FEEDS]
    rss_results = await asyncio.gather(*rss_tasks, return_exceptions=True)

    out = []
    for i, res in enumerate(rss_results):
        if isinstance(res, list):
            for entry in res:
                out.append((entry, ALL_FEEDS[i], False))

    # Nitter Ù‡Ù…Ø²Ù…Ø§Ù†
    tw_tasks = [fetch_one_nitter(client, cfg) for cfg in NITTER_FEEDS]
    tw_results = await asyncio.gather(*tw_tasks, return_exceptions=True)

    tw_ok = 0
    for i, res in enumerate(tw_results):
        if isinstance(res, list) and res:
            tw_ok += 1
            for entry in res:
                out.append((entry, NITTER_FEEDS[i], True))

    log.info(f"  ğ• Nitter: {tw_ok}/{len(NITTER_FEEDS)} Ø§Ú©Ø§Ù†Øª Ù…ÙˆÙÙ‚")
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix4: Gemini â€” Ø®Ù„Ø§ØµÙ‡ Ø¹Ø§Ù…ÛŒØ§Ù†Ù‡ + multi-model rotation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_BASE = "https://generativelanguage.googleapis.com/v1beta/models"

GEMINI_MODEL_POOL = [
    {"id": "gemini-2.5-flash-lite",               "rpm": 15, "rpd": 1000, "tier": 1, "label": "Lite"},
    {"id": "gemini-2.5-flash-lite-preview-09-2025","rpm": 15, "rpd": 1000, "tier": 1, "label": "Lite-Preview"},
    {"id": "gemini-2.5-flash",                     "rpm": 10, "rpd":  250, "tier": 2, "label": "Flash"},
    {"id": "gemini-2.5-flash-preview-09-2025",     "rpm": 10, "rpd":  250, "tier": 2, "label": "Flash-Preview"},
    {"id": "gemini-3-flash-preview",               "rpm": 10, "rpd":  100, "tier": 3, "label": "G3-Flash"},
    {"id": "gemini-2.5-pro",                       "rpm":  5, "rpd":  100, "tier": 3, "label": "Pro"},
    {"id": "gemini-3-pro-preview",                 "rpm":  5, "rpd":   50, "tier": 3, "label": "G3-Pro"},
]

def load_gstate() -> dict:
    try:
        if Path(GEMINI_STATE_FILE).exists():
            s = json.load(open(GEMINI_STATE_FILE))
            today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
            if s.get("date") != today:
                return _fresh_gstate(today)
            return s
    except: pass
    return _fresh_gstate(datetime.now(timezone.utc).strftime("%Y-%m-%d"))

def _fresh_gstate(today):
    return {"date": today, "usage": {m["id"]: 0 for m in GEMINI_MODEL_POOL},
            "failures": {m["id"]: 0 for m in GEMINI_MODEL_POOL}}

def save_gstate(s):
    with open(GEMINI_STATE_FILE, "w") as f: json.dump(s, f)

def pick_models(state: dict) -> list:
    ordered = []
    for tier in [1, 2, 3]:
        for m in GEMINI_MODEL_POOL:
            if m["tier"] == tier:
                rem = m["rpd"] - state["usage"].get(m["id"], 0)
                fails = state["failures"].get(m["id"], 0)
                if rem > 0 and fails < 3:
                    ordered.append(m)
    return ordered or GEMINI_MODEL_POOL

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âœ… Fix4: Prompt Ø¬Ø¯ÛŒØ¯ â€” Ø®Ù„Ø§ØµÙ‡ Ø¹Ø§Ù…ÛŒØ§Ù†Ù‡ Ø¨Ù‡ Ø³Ø¨Ú© ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø¯Ø§ÛŒØª Ù…Ø¯Ù„:
PROMPT_EXAMPLES = """
Ù…Ø«Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ:
  TITLE: IDF strikes Iranian weapons depot in Syria, killing 3 IRGC advisors
  BODY: Israeli forces carried out a series of airstrikes...

Ù…Ø«Ø§Ù„ Ø®Ø±ÙˆØ¬ÛŒ:
  Ø¹Ù†ÙˆØ§Ù†: ğŸ’¥ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø§Ù†Ø¨Ø§Ø± ØªØ³Ù„ÛŒØ­Ø§ØªÛŒ Ø³Ù¾Ø§Ù‡ Ø¯Ø± Ø³ÙˆØ±ÛŒÙ‡ Ø±Ùˆ Ø²Ø¯ØŒ Û³ Ù…Ø³ØªØ´Ø§Ø± Ú©Ø´ØªÙ‡ Ø´Ø¯Ù†
  Ø®Ø¨Ø±: Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø§Ù…Ø´Ø¨ Ú†Ù†Ø¯ ØªØ§ Ù‡ÙˆØ§ÛŒÛŒ Ø²Ø¯ ØªÙˆ Ø³ÙˆØ±ÛŒÙ‡ Ùˆ ÛŒÙ‡ Ø§Ù†Ø¨Ø§Ø± Ø§Ø³Ù„Ø­Ù‡ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø³Ù¾Ø§Ù‡ Ø±Ùˆ Ù…Ù†Ù‡Ø¯Ù… Ú©Ø±Ø¯. Û³ Ù†ÙØ± Ø§Ø² Ù…Ø³ØªØ´Ø§Ø±Ø§Ù† Ø³Ù¾Ø§Ù‡ Ú©Ø´ØªÙ‡ Ø´Ø¯Ù†. Ø§ÛŒÙ† Ø§ØªÙØ§Ù‚ Ø¨Ø¹Ø¯ Ø§Ø²...
"""

def build_prompt(articles: list[tuple[str, str]]) -> str:
    items = ""
    for i, (title, body) in enumerate(articles):
        items += f"###ITEM_{i}###\nTITLE: {title[:350]}\nBODY: {body[:450]}\n"

    return f"""ØªÙˆ ÛŒÙ‡ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ø§ÛŒØ±Ø§Ù†ÛŒ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†-Ø¢Ù…Ø±ÛŒÚ©Ø§-Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù…ÛŒ Ø®Ù„Ø§ØµÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒ.

Ø³Ø¨Ú© Ù†ÙˆØ´ØªØ§Ø±:
- Ø²Ø¨Ø§Ù† Ø¹Ø§Ù…ÛŒØ§Ù†Ù‡ Ùˆ Ø±ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ (Ù†Ù‡ Ø±Ø³Ù…ÛŒ Ùˆ Ø³Ù†Ú¯ÛŒÙ†)
- Ù…Ø«Ù„ ÛŒÙ‡ Ø¯ÙˆØ³Øª Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ú©Ù‡ Ø¯Ø§Ø±ÛŒ Ø¨Ù‡Ø´ Ù¾ÛŒØ§Ù… Ù…ÛŒâ€ŒØ¯ÛŒ
- Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ…ØŒ Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ù‡â€ŒÚ†ÛŒÙ†ÛŒ
- Ø§Ø³Ø§Ù…ÛŒ Ù…Ù‡Ù… Ø±Ùˆ Ø­ÙØ¸ Ú©Ù†: Ù†ØªØ§Ù†ÛŒØ§Ù‡ÙˆØŒ Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒØŒ ØªØ±Ø§Ù…Ù¾ØŒ Ø³Ù¾Ø§Ù‡ØŒ Ù†Ø§ØªÙˆØŒ Ø³Ù†ØªÚ©Ø§Ù…...
- Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø¢Ù…Ø§Ø± Ù…Ù‡Ù… Ø±Ùˆ Ø°Ú©Ø± Ú©Ù†

{PROMPT_EXAMPLES}

Ø­Ø§Ù„Ø§ {len(articles)} Ø®Ø¨Ø± Ø²ÛŒØ± Ø±Ùˆ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†:
ÙØ±Ù…Øª Ø¯Ù‚ÛŒÙ‚:
###ITEM_0###
Ø¹Ù†ÙˆØ§Ù†: [Ø¹Ù†ÙˆØ§Ù† Ú©ÙˆØªØ§Ù‡ Ø¨Ø§ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù…Ù†Ø§Ø³Ø¨]
Ø®Ø¨Ø±: [Ø®Ù„Ø§ØµÙ‡ Û²-Û³ Ø¬Ù…Ù„Ù‡ Ø¹Ø§Ù…ÛŒØ§Ù†Ù‡]
###ITEM_1###
Ø¹Ù†ÙˆØ§Ù†: [...]
Ø®Ø¨Ø±: [...]

===
{items}"""

async def summarize_batch(client: httpx.AsyncClient,
                          articles: list[tuple[str,str]]) -> list[tuple[str,str]]:
    if not GEMINI_API_KEY or not articles:
        return articles

    prompt  = build_prompt(articles)
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.3, "maxOutputTokens": 8192}
    }

    state      = load_gstate()
    candidates = pick_models(state)

    for model in candidates:
        mid   = model["id"]
        label = model["label"]
        used  = state["usage"].get(mid, 0)
        rem   = model["rpd"] - used
        url   = f"{GEMINI_BASE}/{mid}:generateContent?key={GEMINI_API_KEY}"

        log.info(f"ğŸŒ Gemini [{label}] â€” quota: {used}/{model['rpd']} ({rem} Ù…Ø§Ù†Ø¯Ù‡)")

        for attempt in range(2):
            try:
                r = await client.post(url, json=payload, timeout=httpx.Timeout(90.0))

                if r.status_code == 200:
                    raw    = r.json()["candidates"][0]["content"]["parts"][0]["text"]
                    result = _parse_summary(raw, articles)
                    ok     = sum(1 for i,x in enumerate(result) if x != articles[i])
                    log.info(f"âœ… [{label}]: {ok}/{len(articles)} Ø®Ø¨Ø± Ø®Ù„Ø§ØµÙ‡ Ø´Ø¯")
                    state["usage"][mid]    = used + 1
                    state["failures"][mid] = 0
                    save_gstate(state)
                    return result

                elif r.status_code == 429:
                    retry = r.headers.get("Retry-After","")
                    wait  = int(retry) if retry.isdigit() else 20
                    log.warning(f"â³ [{label}] 429 â€” {wait}s â†’ Ù…Ø¯Ù„ Ø¨Ø¹Ø¯ÛŒ")
                    state["failures"][mid] = state["failures"].get(mid,0) + 1
                    await asyncio.sleep(min(wait, 15))
                    break

                elif r.status_code in (500, 503):
                    await asyncio.sleep(10)

                else:
                    log.warning(f"[{label}] HTTP {r.status_code}")
                    break

            except asyncio.TimeoutError:
                log.warning(f"â³ [{label}] timeout")
                break
            except Exception as e:
                log.debug(f"[{label}]: {e}")
                break

    save_gstate(state)
    log.warning("âš ï¸ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø´Ú©Ø³Øª â€” Ù…ØªÙ† Ø§ØµÙ„ÛŒ")
    return articles

def _parse_summary(raw: str, fallback: list) -> list:
    results = list(fallback)
    pattern = re.compile(
        r'###ITEM_(\d+)###\s*\n'
        r'(?:Ø¹Ù†ÙˆØ§Ù†|title)\s*:\s*(.+?)\s*\n'
        r'(?:Ø®Ø¨Ø±|body|text|Ù…ØªÙ†)\s*:\s*(.+?)(?=###ITEM_|\Z)',
        re.IGNORECASE | re.DOTALL
    )
    for m in pattern.finditer(raw):
        idx  = int(m.group(1))
        fa_t = m.group(2).strip().replace("**","").replace("*","")
        fa_s = m.group(3).strip().replace("**","").replace("*","")
        if 0 <= idx < len(results) and fa_t:
            results[idx] = (nfa(fa_t), nfa(fa_s))
    return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(t):
    if not t: return ""
    return BeautifulSoup(str(t), "html.parser").get_text(" ", strip=True)

def make_id(entry):
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode()).hexdigest()

def make_title_id(title):
    t = re.sub(r'[^a-z0-9\u0600-\u06FF]', '', title.lower())
    return "t:" + hashlib.md5(t[:200].encode()).hexdigest()

def format_dt(entry):
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ).strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except: pass
    return ""

def esc(t): return (t or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")
def trim(t, n):
    t = re.sub(r'\s+', ' ', t).strip()
    return t if len(t)<=n else t[:n].rsplit(" ",1)[0]+"â€¦"

def load_seen():
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen):
    with open(SEEN_FILE,"w") as f: json.dump(list(seen)[-15000:], f)

TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client, text):
    for _ in range(4):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID, "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML", "disable_web_page_preview": True,
            }, timeout=httpx.Timeout(15.0))
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                await asyncio.sleep(data.get("parameters",{}).get("retry_after",20))
            elif data.get("error_code") in (400,403):
                log.error(f"TG: {data.get('description')}"); return False
            else: await asyncio.sleep(5)
        except Exception as e:
            log.warning(f"TG: {e}"); await asyncio.sleep(8)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡!"); return

    seen   = load_seen()
    cutoff = get_cutoff()
    log.info(f"ğŸš€ {len(ALL_FEEDS)} RSS/GNews + {len(NITTER_FEEDS)} Nitter")
    log.info(f"ğŸ“… Cutoff: {CUTOFF_HOURS}h ({cutoff.astimezone(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')} Ø¨Ù‡ Ø¨Ø¹Ø¯)")
    log.info(f"ğŸ’¾ Ø­Ø§ÙØ¸Ù‡: {len(seen)} Ø®Ø¨Ø± Ù‚Ø¨Ù„ÛŒ")

    async with httpx.AsyncClient(follow_redirects=True) as client:

        # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¯Ø±ÛŒØ§ÙØª
        log.info("â¬ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù†...")
        raw = await fetch_all(client)
        log.info(f"ğŸ“¥ {len(raw)} Ø¢ÛŒØªÙ… Ø®Ø§Ù…")

        # Ù…Ø±Ø­Ù„Ù‡ Û²: ÙÛŒÙ„ØªØ± Ø³Ø®Øª
        collected  = []
        title_seen = set()
        old_cnt = irrel_cnt = dup_cnt = 0

        for entry, cfg, is_tw in raw:
            eid = make_id(entry)
            if eid in seen: continue

            if not is_fresh(entry):
                seen.add(eid); old_cnt += 1; continue

            if not is_war_relevant(entry, is_twitter=is_tw):
                seen.add(eid); irrel_cnt += 1; continue

            raw_title = clean_html(entry.get("title",""))
            tid = make_title_id(raw_title)
            if tid in title_seen:
                seen.add(eid); dup_cnt += 1; continue

            title_seen.add(tid)
            collected.append((eid, entry, cfg, is_tw))

        log.info(f"ğŸ“Š {old_cnt} Ù‚Ø¯ÛŒÙ…ÛŒ | {irrel_cnt} Ù†Ø§Ù…Ø±ØªØ¨Ø· | {dup_cnt} ØªÚ©Ø±Ø§Ø±ÛŒ | âœ… {len(collected)} Ø¬Ù†Ú¯ÛŒ")

        collected = list(reversed(collected))
        if len(collected) > MAX_NEW_PER_RUN:
            collected = collected[-MAX_NEW_PER_RUN:]

        if not collected:
            log.info("ğŸ’¤ Ù‡ÛŒÚ† Ø®Ø¨Ø± Ø¬Ù†Ú¯ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ù†ÛŒØ³Øª")
            save_seen(seen); return

        # Ù…Ø±Ø­Ù„Ù‡ Û³: Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        articles_in = []
        for eid, entry, cfg, is_tw in collected:
            en_t = trim(clean_html(entry.get("title","")), 300)
            en_s = trim(clean_html(entry.get("summary") or entry.get("description") or ""), 500)
            articles_in.append((en_t, en_s))

        log.info(f"ğŸ“ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ {len(articles_in)} Ø®Ø¨Ø±...")
        summaries = await summarize_batch(client, articles_in)

        # Ù…Ø±Ø­Ù„Ù‡ Û´: Ø§Ø±Ø³Ø§Ù„
        sent = 0
        for i, (eid, entry, cfg, is_tw) in enumerate(collected):
            en_title      = articles_in[i][0]
            fa_title, fa_body = summaries[i]
            link = entry.get("link","")
            dt   = format_dt(entry)
            icon = "ğ•" if is_tw else "ğŸ“¡"

            lines = [f"ğŸ”´ <b>{esc(fa_title)}</b>", ""]
            if fa_body and len(fa_body)>10:
                lines += [esc(fa_body), ""]
            lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            if dt:   lines.append(dt)
            lines.append(f"{icon} <b>{cfg['name']}</b>")
            if link: lines.append(f'ğŸ”— <a href="{link}">Ù…Ù†Ø¨Ø¹</a>')

            if await tg_send(client, "\n".join(lines)):
                seen.add(eid); sent += 1
                log.info(f"  âœ… {fa_title[:55]}")
            else:
                log.error("  âŒ Ø§Ø±Ø³Ø§Ù„ Ù†Ø§Ù…ÙˆÙÙ‚")
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"ğŸ Ù¾Ø§ÛŒØ§Ù† | {sent}/{len(collected)} Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")

if __name__ == "__main__":
    asyncio.run(main())
