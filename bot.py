import os, json, hashlib, asyncio, logging, re, io
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_OK = True
except ImportError:
    PIL_OK = False

try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("WarBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE         = "seen.json"
STORIES_FILE      = "stories.json"
GEMINI_STATE_FILE = "gemini_state.json"
FLIGHT_ALERT_FILE = "flight_alerts.json"
RUN_STATE_FILE    = "run_state.json"
NITTER_CACHE_FILE = "nitter_cache.json"

# â”€â”€ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† ØªØºÛŒÛŒØ± v17: cutoff = last_run - BUFFER_MIN
# Ù‡Ø± Ø§Ø¬Ø±Ø§ ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± ØªØ§Ø²Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯
CUTOFF_BUFFER_MIN  = 3    # buffer Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² miss Ø´Ø¯Ù†
MAX_LOOKBACK_MIN   = 15   # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ø±Ú¯Ø´Øª Ø¨Ù‡ Ø¹Ù‚Ø¨ (Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§ ÛŒØ§ Ø¨Ø¹Ø¯ crash)
SEEN_TTL_HOURS     = 6    # seen.json ÙÙ‚Ø· Û¶ Ø³Ø§Ø¹Øª Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù‡
NITTER_CACHE_TTL   = 900  # Û±Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡ (Ú©ÙˆØªØ§Ù‡â€ŒØªØ± â†’ Ù†ÙˆØ³Ø§Ø²ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±)

MAX_NEW_PER_RUN    = 25   # Ø­Ø¯Ø§Ú©Ø«Ø± Ø®Ø¨Ø± per run
MAX_MSG_LEN        = 4096
SEND_DELAY         = 0.6  # Ø«Ø§Ù†ÛŒÙ‡ Ø¨ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
JACCARD_THRESHOLD  = 0.38
RSS_TIMEOUT        = 7.0
TG_TIMEOUT         = 10.0
TW_TIMEOUT         = 5.0  # Ú©ÙˆØªØ§Ù‡â€ŒØªØ± â†’ fail faster â†’ handle Ø¨Ø¹Ø¯ÛŒ
RICH_CARD_THRESHOLD = 7

TEHRAN_TZ = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ù†Ø§Ø¨Ø¹ RSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IRAN_FEEDS = [
    {"n":"ğŸ‡®ğŸ‡· IRNA English",       "u":"https://en.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Mehr News EN",        "u":"https://en.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tasnim News EN",      "u":"https://www.tasnimnews.com/en/rss"},
    {"n":"ğŸ‡®ğŸ‡· Fars News EN",        "u":"https://www.farsnews.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Press TV",            "u":"https://www.presstv.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· ISNA English",        "u":"https://en.isna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tehran Times",        "u":"https://www.tehrantimes.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran International", "u":"https://www.iranintl.com/en/rss"},
    {"n":"ğŸ‡®ğŸ‡· Radio Farda",         "u":"https://www.radiofarda.com/api/zoyqvpemr"},
    {"n":"ğŸ‡®ğŸ‡· Iran Wire EN",        "u":"https://iranwire.com/en/feed/"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ ØªØ³Ù†ÛŒÙ…",      "u":"https://www.tasnimnews.com/fa/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ù…Ù‡Ø±",         "u":"https://www.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø§ÛŒØ±Ù†Ø§",       "u":"https://www.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ ÙØ§Ø±Ø³",        "u":"https://www.farsnews.ir/rss/fa"},
    {"n":"ğŸ‡®ğŸ‡· Ù…Ø´Ø±Ù‚ Ù†ÛŒÙˆØ²",             "u":"https://www.mashreghnews.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø¯ÙØ§Ø¹ Ù¾Ø±Ø³",             "u":"https://www.defapress.ir/fa/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø³Ù¾Ø§Ù‡ Ù†ÛŒÙˆØ²",             "u":"https://www.sepahnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· GNews IRGC EN",        "u":"https://news.google.com/rss/search?q=IRGC+Iran+Israel+attack+war&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"ğŸ‡®ğŸ‡· GNews Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†",      "u":"https://news.google.com/rss/search?q=Ø§ÛŒØ±Ø§Ù†+Ø§Ø³Ø±Ø§ÛŒÛŒÙ„+Ø¬Ù†Ú¯+Ø­Ù…Ù„Ù‡&hl=fa&gl=IR&ceid=IR:fa&num=15"},
    {"n":"ğŸ‡®ğŸ‡· GNews Ø³Ù¾Ø§Ù‡ Ù…ÙˆØ´Ú©",      "u":"https://news.google.com/rss/search?q=Ø³Ù¾Ø§Ù‡+Ù…ÙˆØ´Ú©+Ø­Ù…Ù„Ù‡+Ø§Ø³Ø±Ø§ÛŒÛŒÙ„&hl=fa&gl=IR&ceid=IR:fa&num=15"},
    {"n":"ğŸ‡®ğŸ‡· GNews Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ",        "u":"https://news.google.com/rss/search?q=Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ+Ø¨ÛŒØ§Ù†ÛŒÙ‡+Ø¬Ù†Ú¯&hl=fa&gl=IR&ceid=IR:fa&num=10"},
]
ISRAEL_FEEDS = [
    {"n":"ğŸ‡®ğŸ‡± Jerusalem Post",       "u":"https://www.jpost.com/rss/rssfeedsheadlines.aspx"},
    {"n":"ğŸ‡®ğŸ‡± Times of Israel",      "u":"https://www.timesofisrael.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± TOI Iran",             "u":"https://www.timesofisrael.com/topic/iran/feed/"},
    {"n":"ğŸ‡®ğŸ‡± Israel Hayom EN",      "u":"https://www.israelhayom.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± Arutz Sheva",          "u":"https://www.israelnationalnews.com/rss.aspx"},
    {"n":"ğŸ‡®ğŸ‡± i24 News",             "u":"https://www.i24news.tv/en/rss"},
    {"n":"ğŸ‡®ğŸ‡± Israel Defense",       "u":"https://www.israeldefense.co.il/en/rss.xml"},
    {"n":"ğŸ‡®ğŸ‡± Netanyahu Iran GNews", "u":"https://news.google.com/rss/search?q=Netanyahu+Iran+attack+order+war&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"ğŸ‡®ğŸ‡± IDF Iran GNews",       "u":"https://news.google.com/rss/search?q=IDF+operation+Iran+strike+missile&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"ğŸ‡®ğŸ‡± Iron Dome GNews",      "u":"https://news.google.com/rss/search?q=Iron+Dome+Arrow+missile+intercept+Iran&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"ğŸ‡®ğŸ‡± Mossad Iran GNews",    "u":"https://news.google.com/rss/search?q=Mossad+Iran+covert+operation&hl=en-US&gl=US&ceid=US:en&num=15"},
]
USA_FEEDS = [
    {"n":"ğŸ‡ºğŸ‡¸ AP Top News",          "u":"https://feeds.apnews.com/rss/apf-topnews"},
    {"n":"ğŸ‡ºğŸ‡¸ AP World",             "u":"https://feeds.apnews.com/rss/apf-WorldNews"},
    {"n":"ğŸ‡ºğŸ‡¸ Reuters World",        "u":"https://feeds.reuters.com/reuters/worldNews"},
    {"n":"ğŸ‡ºğŸ‡¸ Reuters Middle East",  "u":"https://feeds.reuters.com/reuters/MEonlineHeadlines"},
    {"n":"ğŸ‡ºğŸ‡¸ CNN Middle East",      "u":"http://rss.cnn.com/rss/edition_meast.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ Pentagon DoD",         "u":"https://www.defense.gov/DesktopModules/ArticleCS/RSS.ashx?ContentType=1&Site=945&max=10"},
    {"n":"ğŸ‡ºğŸ‡¸ USNI News",            "u":"https://news.usni.org/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Breaking Defense",     "u":"https://breakingdefense.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ The War Zone",         "u":"https://www.twz.com/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Foreign Policy",       "u":"https://foreignpolicy.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ Defense News",         "u":"https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"n":"ğŸ‡ºğŸ‡¸ Military Times",       "u":"https://www.militarytimes.com/arc/outboundfeeds/rss/"},
    {"n":"ğŸ‡ºğŸ‡¸ US Strike Iran GNews", "u":"https://news.google.com/rss/search?q=United+States+strike+bomb+Iran+military&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"ğŸ‡ºğŸ‡¸ US Navy Iran GNews",   "u":"https://news.google.com/rss/search?q=US+Navy+carrier+Iran+Persian+Gulf&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"ğŸ‡ºğŸ‡¸ CENTCOM GNews",        "u":"https://news.google.com/rss/search?q=CENTCOM+Iran+Iraq+military+operation&hl=en-US&gl=US&ceid=US:en"},
    {"n":"ğŸ” Long War Journal",      "u":"https://www.longwarjournal.org/feed"},
    {"n":"âš ï¸ IAEA Iran GNews",       "u":"https://news.google.com/rss/search?q=IAEA+Iran+nuclear+uranium&hl=en-US&gl=US&ceid=US:en&num=15"},
    {"n":"âš ï¸ Red Sea Houthi GNews",  "u":"https://news.google.com/rss/search?q=Houthi+Iran+Red+Sea+attack+US&hl=en-US&gl=US&ceid=US:en&num=15"},
]
EMBASSY_FEEDS = [
    {"n":"ğŸ›ï¸ US Virtual Embassy",   "u":"https://ir.usembassy.gov/feed/"},
    {"n":"ğŸ›ï¸ US State Travel",      "u":"https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html.rss"},
    {"n":"ğŸ›ï¸ UK FCDO Iran",         "u":"https://www.gov.uk/foreign-travel-advice/iran.atom"},
    {"n":"ğŸ›ï¸ Embassy Evacuations",  "u":"https://news.google.com/rss/search?q=embassy+evacuation+Iran+Tehran+warning&hl=en-US&gl=US&ceid=US:en&num=10"},
]
INTL_FEEDS = [
    {"n":"ğŸŒ BBC Middle East",  "u":"https://feeds.bbci.co.uk/news/world/middle_east/rss.xml"},
    {"n":"ğŸŒ Al Jazeera",       "u":"https://www.aljazeera.com/xml/rss/all.xml"},
    {"n":"ğŸŒ Middle East Eye",  "u":"https://www.middleeasteye.net/rss"},
]

ALL_RSS_FEEDS = IRAN_FEEDS + ISRAEL_FEEDS + USA_FEEDS + EMBASSY_FEEDS + INTL_FEEDS
EMBASSY_SET   = {id(f) for f in EMBASSY_FEEDS}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X handles
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_HANDLES = [
    # â”€â”€â”€ OSINT / Breaking â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ” OSINTdefender",        "OSINTdefender"),
    ("ğŸ” IntelCrab",            "IntelCrab"),
    ("ğŸ” GeoConfirmed",         "GeoConfirmed"),
    ("ğŸ” WarMonitor",           "WarMonitor3"),
    ("ğŸ” AuroraIntel",          "AuroraIntel"),
    ("ğŸ” Faytuks",              "Faytuks"),
    ("ğŸ” Clash Report",         "clashreport"),
    ("ğŸ” Megatron",             "Megatron_Ron"),
    ("ğŸ” ELINT News",           "ELINTNews"),
    ("ğŸ” War Zone TW",          "TheWarZoneTW"),
    # â”€â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¯ÙˆÙ„ØªÛŒ / Ù†Ø¸Ø§Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡ºğŸ‡¸ CENTCOM",              "CENTCOM"),
    ("ğŸ‡ºğŸ‡¸ DoD",                  "DeptofDefense"),
    ("ğŸ‡ºğŸ‡¸ Natasha Bertrand",     "NatashaBertrand"),
    ("ğŸ‡ºğŸ‡¸ Barak Ravid",          "BarakRavid"),
    ("ğŸ‡ºğŸ‡¸ Idrees Ali",           "idreesali114"),
    ("ğŸ‡ºğŸ‡¸ Jack Detsch",          "JackDetsch"),
    ("ğŸ‡ºğŸ‡¸ Lara Seligman",        "laraseligman"),
    ("ğŸ‡ºğŸ‡¸ Jim Sciutto",          "jimsciutto"),
    # â”€â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡± IDF",                  "IDF"),
    ("ğŸ‡®ğŸ‡± Israeli PM",           "IsraeliPM"),
    ("ğŸ‡®ğŸ‡± Yossi Melman",         "yossi_melman"),
    ("ğŸ‡®ğŸ‡± Seth Frantzman",       "sfrantzman"),
    ("ğŸ‡®ğŸ‡± Emanuel Fabian",       "manniefabian"),
    ("ğŸ‡®ğŸ‡± Anna Ahronheim",       "AAhronheim"),
    # â”€â”€â”€ Ø§ÛŒØ±Ø§Ù† / Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡· IranIntl EN",          "IranIntl_En"),
    ("ğŸ‡®ğŸ‡· IRNA EN",              "IRNA_English"),
    ("ğŸ‡®ğŸ‡· Press TV",             "PressTV"),
    ("ğŸ‡®ğŸ‡· Farnaz Fassihi",       "farnazfassihi"),
    ("ğŸ‡®ğŸ‡· Kasra Aarabi",         "KasraAarabi"),
    # â”€â”€â”€ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Brk",       "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ AFP News",              "AFPnews"),
    # â”€â”€â”€ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ” Ian Bremmer",           "ianbremmer"),
    ("ğŸ” Ellie Geranmayeh",      "EllieGeranmayeh"),
    ("ğŸ” Michael Knights",       "Mikeknightsiraq"),
    ("ğŸ” Aric Toler",            "AricToler"),
    ("âš ï¸ DEFCONLevel",           "DEFCONLevel"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TELEGRAM_CHANNELS = [
    # OSINT â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
    ("ğŸ”´ Middle East Spectator", "Middle_East_Spectator"),
    ("ğŸ”´ Intel Slava Z",         "intelslava"),
    ("ğŸ”´ ELINT News",            "ELINTNews"),
    ("ğŸ”´ Megatron OSINT",        "Megatron_Ron"),
    ("ğŸ”´ Disclose TV",           "disclosetv"),
    ("ğŸ” OSINTtechnical",        "Osinttechnical"),
    ("ğŸ” Iran OSINT",            "IranOSINT"),
    ("ğŸ” Aurora Intel",          "Aurora_Intel"),
    ("ğŸ” War Monitor",           "WarMonitor3"),
    # Ø§ÛŒØ±Ø§Ù† ÙØ§Ø±Ø³ÛŒ
    ("ğŸ‡®ğŸ‡· Iran Intl Persian",   "IranIntlPersian"),
    ("ğŸ‡®ğŸ‡· ØªØ³Ù†ÛŒÙ… ÙØ§Ø±Ø³ÛŒ",          "tasnimnewsfa"),
    ("ğŸ‡®ğŸ‡· Ù…Ù‡Ø± ÙØ§Ø±Ø³ÛŒ",             "mehrnews_fa"),
    ("ğŸ‡®ğŸ‡· Ø§ÛŒØ±Ù†Ø§ ÙØ§Ø±Ø³ÛŒ",           "irnafarsi"),
    ("ğŸ‡®ğŸ‡· Press TV",              "PressTVnews"),
    # Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    ("ğŸ‡®ğŸ‡± Kann News",            "kann_news"),
    ("ğŸ‡®ğŸ‡± Times of Israel",      "timesofisrael"),
    # Ù…Ù†Ø·Ù‚Ù‡
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Breaking",  "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸ‡¾ğŸ‡² Masirah TV",           "AlMasirahNet"),
    ("ğŸ‡±ğŸ‡§ Naharnet",             "Naharnet"),
    # Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ OSINTdefender",         "OSINTdefender"),
    ("ğŸŒ GeoConfirmed",          "GeoConfirmed"),
    ("ğŸŒ IntelCrab",             "IntelCrab"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÙÛŒÙ„ØªØ± Ù…ÙˆØ¶ÙˆØ¹ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IRAN_KEYWORDS = [
    "iran","iranian","irgc","islamic republic","khamenei","tehran","persian",
    "sepah","basij","quds force","rouhani","raisi","pezeshkian",
    "Ø§ÛŒØ±Ø§Ù†","Ø³Ù¾Ø§Ù‡","Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ","ØªÙ‡Ø±Ø§Ù†","Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ","Ù¾Ø²Ø´Ú©ÛŒØ§Ù†",
]
OPPONENT_KEYWORDS = [
    "israel","israeli","idf","netanyahu","us military","united states","pentagon",
    "centcom","nato","hamas","hezbollah","houthi","saudi","uae",
    "Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ø¢Ù…Ø±ÛŒÚ©Ø§","Ù¾Ù†ØªØ§Ú¯ÙˆÙ†","Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ","Ø­Ù…Ø§Ø³","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡","Ø­ÙˆØ«ÛŒ",
]
ACTION_KEYWORDS = [
    "attack","strike","missile","bomb","war","kill","dead","casualties","nuclear",
    "sanction","threat","intercept","drone","explosion","airstrike","operation",
    "deploy","troops","invasion","retaliat","escalat","alert","warning",
    "Ø­Ù…Ù„Ù‡","Ù…ÙˆØ´Ú©","Ú©Ø´ØªÙ‡","Ø¬Ù†Ú¯","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","ØªØ­Ø±ÛŒÙ…","ØªÙ‡Ø¯ÛŒØ¯","Ø¹Ù…Ù„ÛŒØ§Øª",
    "Ø§Ù†ÙØ¬Ø§Ø±","Ù¾Ù‡Ù¾Ø§Ø¯","Ù¾Ø¯Ø§ÙÙ†Ø¯","Ø±Ù‡Ú¯ÛŒØ±ÛŒ","ØªØ¬Ø§ÙˆØ²","Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ¨Ø§Ø´","Ø§Ø¹Ù„Ø§Ù… Ø¬Ù†Ú¯",
]
HARD_EXCLUDE = [
    "football","soccer","basketball","olympic","sports","cooking",
    "fashion","celebrity","entertainment","music","award",
    "ÙÙˆØªØ¨Ø§Ù„","Ø³ÛŒÙ†Ù…Ø§","Ù…ÙˆØ³ÛŒÙ‚ÛŒ","ÙˆØ±Ø²Ø´",
]
EMBASSY_OVERRIDE = [
    "evacuate","leave immediately","travel warning","security alert","emergency",
    "ØªØ®Ù„ÛŒÙ‡","ÙÙˆØ±ÛŒ ØªØ±Ú©","Ù‡Ø´Ø¯Ø§Ø±","Ø§Ø¶Ø·Ø±Ø§Ø±",
]

def is_war_relevant(text, is_embassy=False, is_tg=False, is_tw=False):
    txt = text.lower()
    if is_embassy and any(k in txt for k in EMBASSY_OVERRIDE): return True
    if any(k in txt for k in HARD_EXCLUDE): return False
    hi = any(k in txt for k in IRAN_KEYWORDS)
    ho = any(k in txt for k in OPPONENT_KEYWORDS)
    ha = any(k in txt for k in ACTION_KEYWORDS)
    if is_tg or is_tw: return (hi or ho) and ha
    return hi and ho and ha

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X â€” Nitter + RSSHub
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Nitter instances â€” Ù…Ø¹ØªØ¨Ø±ØªØ±ÛŒÙ† Ø¯Ø± Û²Û°Û²Û¶
# xcancel.com Ø¨ÛŒØ´ØªØ±ÛŒÙ† uptime Ø¯Ø§Ø±Ø¯
NITTER_INSTANCES = [
    "https://xcancel.com",
    "https://nitter.poast.org",
    "https://nitter.privacyredirect.com",
    "https://lightbrd.com",
    "https://nitter.tiekoetter.com",
    "https://nitter.space",
    "https://n.ramle.be",
    "https://nitter.catsarch.com",
]
# RSSHub instances â€” fallback
RSSHUB_INSTANCES = [
    "https://rsshub.rss.now.sh",
    "https://rss.shab.fun",
    "https://rsshub.moeyy.xyz",
]

NITTER_HDR = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0",
    "Accept": "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Cache-Control": "no-cache",
}
COMMON_UA = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

_nitter_pool: list[str]  = []
_rsshub_pool: list[str]  = []
_TW_SEMA: asyncio.Semaphore | None = None

def _load_nitter_cache() -> tuple[list, list, float]:
    try:
        if Path(NITTER_CACHE_FILE).exists():
            d = json.load(open(NITTER_CACHE_FILE))
            return d.get("nitter", []), d.get("rsshub", []), d.get("ts", 0.0)
    except: pass
    return [], [], 0.0

def _save_nitter_cache(nitter, rsshub):
    json.dump({"nitter": nitter, "rsshub": rsshub,
               "ts": datetime.now(timezone.utc).timestamp()},
              open(NITTER_CACHE_FILE, "w"))

def _is_rss(body: str, ct: str) -> bool:
    return ("xml" in ct) or ("<rss" in body[:400]) or body.lstrip()[:6].startswith("<?xml")

async def _try_rss(client: httpx.AsyncClient, url: str, timeout: float = TW_TIMEOUT) -> list:
    """GET ÛŒÚ© URL RSS â€” Ø®Ø±ÙˆØ¬ÛŒ: list Ø§Ø² entries ÛŒØ§ []"""
    try:
        r = await client.get(url, headers=NITTER_HDR,
                             timeout=httpx.Timeout(connect=3.0, read=timeout,
                                                   write=3.0, pool=3.0))
        if r.status_code != 200: return []
        if not _is_rss(r.text, r.headers.get("content-type", "")): return []
        entries = feedparser.parse(r.text).entries
        return [e for e in entries if len(e.get("title", "").strip()) > 5]
    except: return []

async def _probe_nitter(client: httpx.AsyncClient, inst: str) -> tuple | None:
    t0 = asyncio.get_running_loop().time()
    e  = await _try_rss(client, f"{inst}/CENTCOM/rss", timeout=4.0)
    if e:
        return inst, (asyncio.get_running_loop().time() - t0) * 1000
    return None

async def _probe_rsshub(client: httpx.AsyncClient, inst: str) -> tuple | None:
    t0 = asyncio.get_running_loop().time()
    e  = await _try_rss(client, f"{inst}/twitter/user/CENTCOM", timeout=5.0)
    if e:
        return inst, (asyncio.get_running_loop().time() - t0) * 1000
    return None

async def build_twitter_pools(client: httpx.AsyncClient):
    """
    Probe Nitter + RSSHub Ù…ÙˆØ§Ø²ÛŒ â€” Ù†ØªÛŒØ¬Ù‡ Ø¯Ø± Ú©Ø´ Û±Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡â€ŒØ§ÛŒ
    """
    global _nitter_pool, _rsshub_pool
    if _nitter_pool or _rsshub_pool:
        return

    cached_n, cached_r, ts = _load_nitter_cache()
    age = datetime.now(timezone.utc).timestamp() - ts
    if age < NITTER_CACHE_TTL and (cached_n or cached_r):
        _nitter_pool = cached_n
        _rsshub_pool = cached_r
        log.info(f"ğ• pool Ø§Ø² cache: Nitter={len(_nitter_pool)} RSSHub={len(_rsshub_pool)}")
        return

    log.info(f"ğ• Probing {len(NITTER_INSTANCES)} Nitter + {len(RSSHUB_INSTANCES)} RSSHub...")
    sema = asyncio.Semaphore(8)
    async def sp(coro):
        async with sema:
            try: return await coro
            except: return None

    n = len(NITTER_INSTANCES)
    results = await asyncio.gather(
        *[sp(_probe_nitter(client, u)) for u in NITTER_INSTANCES],
        *[sp(_probe_rsshub(client, u)) for u in RSSHUB_INSTANCES],
    )
    nok = sorted([r for r in results[:n] if r], key=lambda x: x[1])
    rok = sorted([r for r in results[n:] if r], key=lambda x: x[1])

    _nitter_pool = [u for u, _ in nok]  or NITTER_INSTANCES[:3]
    _rsshub_pool = [u for u, _ in rok]

    if nok: log.info(f"  Nitter best: {nok[0][0].split('//')[-1]} ({nok[0][1]:.0f}ms)")
    if rok: log.info(f"  RSSHub best: {rok[0][0].split('//')[-1]} ({rok[0][1]:.0f}ms)")
    log.info(f"ğ• Nitter:{len(_nitter_pool)} RSSHub:{len(_rsshub_pool)}")
    _save_nitter_cache(_nitter_pool, _rsshub_pool)

async def fetch_twitter(client: httpx.AsyncClient, label: str, handle: str) -> list:
    """
    Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ ÛŒÚ© handle â€” Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡:
    1. Nitter (Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† instance Ø§Ø² probe)
    2. RSSHub
    3. xcancel.com Ù…Ø³ØªÙ‚ÛŒÙ…
    semaphore Ú©Ù„ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² flood Ø¨Ù‡ instances
    """
    sema = _TW_SEMA or asyncio.Semaphore(20)
    async with sema:
        # Ù…Ø±Ø­Ù„Ù‡ Û±: Nitter
        pool = _nitter_pool or NITTER_INSTANCES
        start = abs(hash(handle)) % len(pool)
        for inst in (pool * 2)[start: start + min(3, len(pool))]:
            e = await _try_rss(client, f"{inst}/{handle}/rss")
            if e:
                log.debug(f"ğ• {handle} â† Nitter/{inst.split('//')[-1]}")
                return [(x, f"ğ• {label}", "tw", False) for x in e]

        # Ù…Ø±Ø­Ù„Ù‡ Û²: RSSHub
        for inst in (_rsshub_pool or RSSHUB_INSTANCES[:1]):
            e = await _try_rss(client, f"{inst}/twitter/user/{handle}")
            if e:
                log.debug(f"ğ• {handle} â† RSSHub/{inst.split('//')[-1]}")
                return [(x, f"ğ• {label}", "tw", False) for x in e]

        # Ù…Ø±Ø­Ù„Ù‡ Û³: xcancel.com Ù…Ø³ØªÙ‚ÛŒÙ…
        e = await _try_rss(client, f"https://xcancel.com/{handle}/rss")
        if e:
            log.debug(f"ğ• {handle} â† xcancel direct")
            return [(x, f"ğ• {label}", "tw", False) for x in e]

    log.debug(f"ğ• {handle}: Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ fail")
    return []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADS-B
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ADSB_API     = "https://api.adsb.one/v2"
ADSB_REGIONS = [
    ("Ø§ÛŒØ±Ø§Ù†",          32.4, 53.7, 250),
    ("Ø®Ù„ÛŒØ¬â€ŒÙØ§Ø±Ø³",     26.5, 52.0, 250),
    ("Ø§Ø³Ø±Ø§ÛŒÛŒÙ„/Ù„Ø¨Ù†Ø§Ù†", 32.1, 35.2, 200),
    ("Ø¹Ø±Ø§Ù‚",           33.3, 44.4, 250),
]
_MIL_TYPES    = {"B52","B2","B1","F15","F16","F22","F35","F18","E3","E8","RC135","U2","P8","MQ9","RQ4","C17","KC135"}
_CALLSIGN_PFX = ["DOOM","BONE","BUCK","CIAO","JAKE","TORC","GRIM","HAVOC","GHOST"]
_ADSB_SEEN    = set()

async def fetch_military_flights(client: httpx.AsyncClient) -> list:
    global _ADSB_SEEN
    msgs = []
    try:
        try:
            if Path(FLIGHT_ALERT_FILE).exists():
                _ADSB_SEEN = set(json.load(open(FLIGHT_ALERT_FILE)).get("seen", []))
        except: pass
        for region, lat, lon, radius in ADSB_REGIONS:
            try:
                r = await client.get(f"{ADSB_API}/point/{lat}/{lon}/{radius}",
                                     timeout=httpx.Timeout(7.0),
                                     headers={"Accept": "application/json"})
                if r.status_code != 200: continue
                for ac in (r.json().get("ac") or []):
                    hex_id   = (ac.get("hex") or ac.get("icao","")).upper()
                    callsign = (ac.get("flight") or ac.get("callsign","")).strip()
                    cat      = (ac.get("category") or "").upper()
                    t        = (ac.get("t") or ac.get("type","")).upper()
                    is_mil   = (any(t.startswith(m) for m in _MIL_TYPES)
                                or any(callsign.startswith(p) for p in _CALLSIGN_PFX)
                                or "A5" in cat)
                    if not is_mil: continue
                    uid = f"{hex_id}_{callsign}"
                    if uid in _ADSB_SEEN: continue
                    _ADSB_SEEN.add(uid)
                    alt = ac.get("alt_baro") or ac.get("alt", 0)
                    gs  = ac.get("gs") or ac.get("speed", 0)
                    msgs.append(f"âœˆï¸ <b>ØªØ­Ø±Ú© Ù†Ø¸Ø§Ù…ÛŒ â€” {region}</b>\n"
                                f"Ù†ÙˆØ¹: <code>{t or '?'}</code>  Ú©Ø§Ù„â€ŒØ³Ø§ÛŒÙ†: <code>{callsign or hex_id}</code>\n"
                                f"Ø§Ø±ØªÙØ§Ø¹: {alt:,} ft  Ø³Ø±Ø¹Øª: {gs} kt")
            except Exception as e:
                log.debug(f"ADS-B {region}: {e}")
        json.dump({"seen": list(_ADSB_SEEN)[-300:]}, open(FLIGHT_ALERT_FILE, "w"))
    except Exception as e:
        log.warning(f"ADS-B: {e}")
    return msgs

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS + Telegram fetch
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_rss(client: httpx.AsyncClient, feed: dict) -> list:
    """RSS Ø¨Ø§ conditional GET (ETag/If-Modified-Since)"""
    try:
        hdrs = dict(COMMON_UA)
        hdrs["Accept"] = "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8"
        if feed.get("_etag"):      hdrs["If-None-Match"]     = feed["_etag"]
        if feed.get("_last_mod"):  hdrs["If-Modified-Since"] = feed["_last_mod"]
        r = await client.get(feed["u"], timeout=httpx.Timeout(RSS_TIMEOUT), headers=hdrs)
        if r.status_code == 304: return []
        if r.status_code != 200: return []
        if r.headers.get("ETag"):          feed["_etag"]     = r.headers["ETag"]
        if r.headers.get("Last-Modified"): feed["_last_mod"] = r.headers["Last-Modified"]
        entries = feedparser.parse(r.text).entries or []
        is_emb  = id(feed) in EMBASSY_SET
        return [(e, feed["n"], "rss", is_emb) for e in entries]
    except: return []

async def fetch_telegram_channel(client: httpx.AsyncClient, label: str,
                                  handle: str, cutoff: datetime) -> list:
    """
    scrape t.me/s/{handle} â€” ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² cutoff
    """
    url  = f"https://t.me/s/{handle}"
    hdrs = {
        "User-Agent": "TelegramBot (like TwitterBot)",
        "Accept": "text/html,application/xhtml+xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Cache-Control": "no-cache",
    }
    try:
        r = await client.get(url, timeout=httpx.Timeout(TG_TIMEOUT), headers=hdrs)
        if r.status_code not in (200, 301, 302): return []
        soup = BeautifulSoup(r.text, "html.parser")
        msgs = soup.select(".tgme_widget_message_wrap")
        if not msgs: return []
        results = []
        for msg in msgs[-30:]:
            txt_el = msg.select_one(".tgme_widget_message_text")
            text   = txt_el.get_text(" ", strip=True) if txt_el else ""
            if not text or len(text) < 15: continue
            time_el  = msg.select_one("time")
            dt_str   = time_el.get("datetime", "") if time_el else ""
            entry_dt = None
            if dt_str:
                try: entry_dt = datetime.fromisoformat(dt_str.replace("Z","+00:00"))
                except: pass
            # ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØªØ§Ø²Ù‡â€ŒØªØ± Ø§Ø² cutoff
            if entry_dt and entry_dt < cutoff: continue
            link_el = msg.select_one("a.tgme_widget_message_date")
            link    = link_el.get("href","") if link_el else f"https://t.me/{handle}"
            results.append(({
                "title":   text[:300],
                "summary": text[:800],
                "link":    link,
                "_tg_dt":  entry_dt,
            }, label, "tg", False))
        return results
    except Exception as e:
        log.debug(f"TG {handle}: {e}"); return []

async def fetch_all(client: httpx.AsyncClient, cutoff: datetime) -> list:
    """
    ÙˆØ§Ú©Ø´ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹
    cutoff Ø¨Ø±Ø§ÛŒ Telegram Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡ (RSS Ø§Ø² is_fresh Ø¯Ø± main ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´Ù‡)
    """
    await build_twitter_pools(client)

    rss_t = [fetch_rss(client, f) for f in ALL_RSS_FEEDS]
    tg_t  = [fetch_telegram_channel(client, l, h, cutoff) for l, h in TELEGRAM_CHANNELS]
    tw_t  = [fetch_twitter(client, l, h) for l, h in TWITTER_HANDLES]

    all_res = await asyncio.gather(*rss_t, *tg_t, *tw_t, return_exceptions=True)

    out = []; rss_ok = tg_ok = tw_ok = 0
    n_rss = len(ALL_RSS_FEEDS); n_tg = len(TELEGRAM_CHANNELS)
    for i, res in enumerate(all_res):
        if not isinstance(res, list): continue
        out.extend(res)
        if   i < n_rss:          rss_ok += bool(res)
        elif i < n_rss + n_tg:   tg_ok  += bool(res)
        else:                     tw_ok  += bool(res)

    log.info(f"  ğŸ“¡ RSS:{rss_ok}/{len(ALL_RSS_FEEDS)} "
             f" ğŸ“¢ TG:{tg_ok}/{len(TELEGRAM_CHANNELS)} "
             f" ğ•:{tw_ok}/{len(TWITTER_HANDLES)}")
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø± Ù…ØªÙ†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(t): return re.sub(r"<[^>]+>", " ", t or "").strip()
def trim(t, n):
    t = t.strip()
    return t if len(t) <= n else t[:n-1] + "â€¦"
def make_id(entry):
    k = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(k.encode()).hexdigest()
def esc(t):
    return re.sub(r"([<>&])", lambda m: {"<":"&lt;",">":"&gt;","&":"&amp;"}[m.group()], t)

def format_dt(entry) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
        tg_dt = entry.get("_tg_dt")
        if tg_dt:
            return tg_dt.astimezone(TEHRAN_TZ).strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
    except: pass
    return ""

def is_fresh(entry, cutoff: datetime) -> bool:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t: return datetime(*t[:6], tzinfo=timezone.utc) >= cutoff
        tg_dt = entry.get("_tg_dt")
        if tg_dt: return tg_dt >= cutoff
        return True  # Ø¨Ø¯ÙˆÙ† timestamp â†’ Ù¾Ø§Ø³ Ø¨Ø¯Ù‡ (seen.json ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ù‡)
    except: return True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dedup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_VIOLENCE_CODES  = {"MSL","AIR","ATK","KIA","DEF","EXP"}
_POLITICAL_CODES = {"THR","DIP","SAN","NUC","SPY","STM"}

def _stem(word):
    w = word.lower()
    for suf in ("ing","ed","tion","ment","er","Ù‡Ø§","Ù‡Ø§ÛŒ","\u200cÙ‡Ø§"):
        if w.endswith(suf) and len(w) > len(suf)+3: return w[:-len(suf)]
    return w

def _bag(text):
    return {_stem(w) for w in re.findall(r"[\w\u0600-\u06FF]{3,}", text.lower())}

def _entity_triple(title):
    txt = title.lower()
    actors = (
        ["iran","irgc","khamenei","Ø³Ù¾Ø§Ù‡","Ø§ÛŒØ±Ø§Ù†"],
        ["israel","idf","netanyahu","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„"],
        ["us ","usa","centcom","pentagon","Ø¢Ù…Ø±ÛŒÚ©Ø§"],
        ["hamas","Ø­Ù…Ø§Ø³"], ["hezbollah","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡"], ["houthi","Ø­ÙˆØ«ÛŒ"],
    )
    action_cats = {
        "MSL": ["missile","rocket","ballistic","Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯"],
        "AIR": ["airstrike","bombing","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†"],
        "ATK": ["attack","strike","Ø­Ù…Ù„Ù‡"],
        "KIA": ["killed","dead","casualties","Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯"],
        "DEF": ["intercept","iron dome","Ø±Ù‡Ú¯ÛŒØ±ÛŒ"],
        "EXP": ["explosion","blast","Ø§Ù†ÙØ¬Ø§Ø±"],
        "THR": ["threat","warn","ØªÙ‡Ø¯ÛŒØ¯"],
        "SAN": ["sanction","ØªØ­Ø±ÛŒÙ…"],
        "NUC": ["nuclear","uranium","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ"],
    }
    actor1, actor2, act = "", "", ""
    for i, grp in enumerate(actors):
        if any(a in txt for a in grp):
            if not actor1: actor1 = str(i)
            elif not actor2: actor2 = str(i)
    for code, kws in action_cats.items():
        if any(k in txt for k in kws): act = code; break
    return actor1, actor2, act

def is_story_dup(title: str, stories: list) -> bool:
    bag1 = _bag(title)
    if not bag1: return False
    a1, a2, act1 = _entity_triple(title)
    for prev_t, prev_bag, prev_triple in stories:
        pa, pb, pact = prev_triple
        if act1 and pact and act1 in _VIOLENCE_CODES and pact in _VIOLENCE_CODES:
            if a1 == pa and a2 == pb: return True
        if act1 and pact and act1 in _POLITICAL_CODES and pact in _POLITICAL_CODES:
            if a1 == pa: return True
        union = bag1 | prev_bag
        if union and len(bag1 & prev_bag) / len(union) >= JACCARD_THRESHOLD:
            return True
    return False

def register_story(title, stories):
    stories.append((title, _bag(title), _entity_triple(title)))
    return stories[-300:]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# seen.json â€” Ø¨Ø§ TTL â€” ÙÙ‚Ø· Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    cutoff_ts = datetime.now(timezone.utc).timestamp() - SEEN_TTL_HOURS * 3600
    try:
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                return {k for k, v in raw.items() if v > cutoff_ts}
            elif isinstance(raw, list):
                # migrate Ø§Ø² ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… â€” ÙÙ‚Ø· ÛµÛ°Û° ØªØ§ Ø¢Ø®Ø±
                return set(raw[-500:])
    except: pass
    return set()

def save_seen(seen: set):
    now_ts    = datetime.now(timezone.utc).timestamp()
    cutoff_ts = now_ts - SEEN_TTL_HOURS * 3600
    try:
        existing = {}
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                existing = {k: v for k, v in raw.items() if v > cutoff_ts}
    except: existing = {}
    for eid in seen:
        if eid not in existing: existing[eid] = now_ts
    if len(existing) > 5000:
        existing = dict(sorted(existing.items(), key=lambda x: x[1], reverse=True)[:5000])
    json.dump(existing, open(SEEN_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# run_state â€” last_run Ø¨Ø±Ø§ÛŒ cutoff Ù‡ÙˆØ´Ù…Ù†Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_run_state() -> datetime:
    """Ø¢Ø®Ø±ÛŒÙ† Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ â€” Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ cutoff"""
    try:
        if Path(RUN_STATE_FILE).exists():
            d   = json.load(open(RUN_STATE_FILE))
            ts  = d.get("last_run", 0)
            if ts:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
    except: pass
    # Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§: MAX_LOOKBACK_MIN Ø¨Ù‡ Ø¹Ù‚Ø¨
    return datetime.now(timezone.utc) - timedelta(minutes=MAX_LOOKBACK_MIN)

def save_run_state():
    existing = {}
    try:
        if Path(RUN_STATE_FILE).exists():
            existing = json.load(open(RUN_STATE_FILE))
    except: pass
    existing["last_run"] = datetime.now(timezone.utc).timestamp()
    json.dump(existing, open(RUN_STATE_FILE, "w"))

def load_stories() -> list:
    try:
        if Path(STORIES_FILE).exists(): return json.load(open(STORIES_FILE))
    except: pass
    return []

def save_stories(stories):
    json.dump(stories[-300:], open(STORIES_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Gemini ØªØ±Ø¬Ù…Ù‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_MODELS = [
    "gemini-2.0-flash",
    "gemini-1.5-flash",
    "gemini-1.5-flash-8b",
]

TRANSLATE_PROMPT = """ØªÙˆ ÛŒÙ‡ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ø¬Ù†Ú¯ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒ. Ø§ÛŒÙ† Ø®Ø¨Ø±Ù‡Ø§ Ø±Ùˆ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†.

Ù‚ÙˆØ§Ù†ÛŒÙ†:
Û±. ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ú©Ø§Ù…Ù„ â€” Ù‡ÛŒÚ† Ú†ÛŒØ² Ø­Ø°Ù Ù†Ø´ÙˆØ¯
Û². Ù†Ù‚Ù„â€ŒÙ‚ÙˆÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¹ÛŒÙ†â€ŒØ§Ù„Ø¹ÛŒÙ† Ø¨Ø§ Ú¯ÛŒÙˆÙ…Ù‡: Â«Ø¬Ù…Ù„Ù‡ Ú¯ÙØªÙ‡â€ŒØ´Ø¯Ù‡Â»
Û³. Ø§Ø³Ø§Ù…ÛŒ Ø¯Ù‚ÛŒÙ‚: Netanyahu=Ù†ØªØ§Ù†ÛŒØ§Ù‡ÙˆØŒ Khamenei=Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒØŒ IRGC=Ø³Ù¾Ø§Ù‡ØŒ IDF=Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
Û´. Ø¢Ù…Ø§Ø± Ùˆ ØªØ§Ø±ÛŒØ® Ø±Ø§ Ø­ÙØ¸ Ú©Ù†
Ûµ. Ø§Ú¯Ù‡ Ø®Ø¨Ø± ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª: ÙÙ‚Ø· ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù† Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù…Ø­ØªÙˆØ§

ÙØ±Ù…Øª Ø®Ø±ÙˆØ¬ÛŒ:
###ITEM_0###
[ØªØ±Ø¬Ù…Ù‡ ÙØ§Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„]
###ITEM_1###
[ØªØ±Ø¬Ù…Ù‡ ÙØ§Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„]

===Ø®Ø¨Ø±Ù‡Ø§===
{items}"""

async def translate_batch(client: httpx.AsyncClient, articles: list) -> list:
    if not GEMINI_API_KEY or not articles: return articles
    items_txt = "".join(
        f"###ITEM_{i}###\nTITLE: {t[:400]}\nBODY: {s[:600]}\n"
        for i, (t, s) in enumerate(articles)
    )
    state = {}
    try:
        if Path(GEMINI_STATE_FILE).exists():
            state = json.load(open(GEMINI_STATE_FILE))
    except: pass
    models = state.get("models_order", GEMINI_MODELS)
    base   = "https://generativelanguage.googleapis.com/v1beta/models"
    for model in models:
        try:
            r = await client.post(
                f"{base}/{model}:generateContent?key={GEMINI_API_KEY}",
                json={
                    "contents": [{"parts": [{"text": TRANSLATE_PROMPT.format(items=items_txt)}]}],
                    "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
                },
                timeout=httpx.Timeout(30.0)
            )
            if r.status_code in (429, 503): continue
            if r.status_code != 200: continue
            text_out = r.json()["candidates"][0]["content"]["parts"][0]["text"]
            results  = list(articles)
            for i, (orig_t, orig_s) in enumerate(articles):
                m = re.search(rf"###ITEM_{i}###\s*(.*?)(?=###ITEM_|\Z)", text_out, re.DOTALL)
                if m:
                    tr = m.group(1).strip()
                    if len(tr) > 10: results[i] = (tr, orig_s)
            return results
        except Exception as e:
            log.debug(f"Gemini {model}: {e}"); continue
    return articles

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Sentiment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BREAKING_KEYWORDS = [
    "breaking","urgent","alert","just in","explosion","airstrike","killed","dead",
    "war","attack","strike","nuclear","bomb","missile","invasion",
    "Ø­Ù…Ù„Ù‡","Ú©Ø´ØªÙ‡","Ø§Ù†ÙØ¬Ø§Ø±","Ø´Ù‡ÛŒØ¯","Ù…ÙˆØ´Ú©","ÙÙˆØ±ÛŒ","Ø®Ø¨Ø± ÙÙˆØ±ÛŒ","Ø§Ø¹Ù„Ø§Ù… Ø¬Ù†Ú¯",
]
IMPORTANCE_BOOST = {
    "ğŸ’€":4, "ğŸ”´":3, "ğŸ’¥":3, "ğŸš€":3, "â˜¢ï¸":3,
    "âœˆï¸":2, "ğŸš¢":2, "ğŸ›¡ï¸":2, "ğŸ•µï¸":2,
    "ğŸ”¥":1, "ğŸ’°":1, "âš ï¸":1,
}

SENTIMENT_RULES = [
    ("ğŸ’€", ["killed","dead","casualties","fatalities","wounded","martyred","massacre"],
           ["Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯","ØªÙ„ÙØ§Øª","Ú©Ø´ØªØ§Ø±","Ù…Ø¬Ø±ÙˆØ­"]),
    ("ğŸ”´", ["attack","struck","assault","launched attack","opened fire","bombed","targeted"],
           ["Ø­Ù…Ù„Ù‡","Ø¶Ø±Ø¨Ù‡","Ù…ÙˆØ±Ø¯ Ù‡Ø¯Ù","Ø­Ù…Ù„Ù‡ Ú©Ø±Ø¯"]),
    ("ğŸ’¥", ["explosion","blast","detonation","explode","blew up"],
           ["Ø§Ù†ÙØ¬Ø§Ø±","Ù…Ù†ÙØ¬Ø±","ØªØ±Ú©ÛŒØ¯"]),
    ("âœˆï¸", ["airstrike","air strike","air raid","warplane","f-35","f-15","b-52","f-16"],
           ["Ø­Ù…Ù„Ù‡ Ù‡ÙˆØ§ÛŒÛŒ","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†","Ø¬Ù†Ú¯Ù†Ø¯Ù‡"]),
    ("ğŸš€", ["missile","rocket","ballistic","cruise missile","drone strike","hypersonic"],
           ["Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯","Ù…ÙˆØ´Ú© Ø¨Ø§Ù„Ø³ØªÛŒÚ©","Ø±Ø§Ú©Øª"]),
    ("â˜¢ï¸", ["nuclear","uranium","enrichment","natanz","fordow","centrifuge","iaea"],
           ["Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…","ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ","Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ø³Ø§Ù†ØªØ±ÛŒÙÛŒÙˆÚ˜"]),
    ("ğŸš¢", ["navy","naval","warship","aircraft carrier","strait of hormuz","red sea"],
           ["Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ","Ù†Ø§Ùˆ","ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²","Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø®"]),
    ("ğŸ•µï¸", ["intelligence","mossad","cia","spy","covert","assassination","sabotage","cyber"],
           ["Ø¬Ø§Ø³ÙˆØ³ÛŒ","Ù…ÙˆØ³Ø§Ø¯","Ø®Ø±Ø§Ø¨Ú©Ø§Ø±ÛŒ","ØªØ±ÙˆØ±","Ø³Ø§ÛŒØ¨Ø±ÛŒ"]),
    ("ğŸ›¡ï¸", ["intercept","shot down","iron dome","air defense","patriot"],
           ["Ø±Ù‡Ú¯ÛŒØ±ÛŒ","Ù¾Ø¯Ø§ÙÙ†Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†","Ø³Ø±Ù†Ú¯ÙˆÙ†"]),
    ("ğŸ”¥", ["escalat","tension","brink of war","retaliat","provocation"],
           ["ØªØ´Ø¯ÛŒØ¯","ØªÙ†Ø´","ØªÙ„Ø§ÙÛŒ","Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯"]),
    ("ğŸ’°", ["sanction","embargo","swift","freeze assets"],
           ["ØªØ­Ø±ÛŒÙ…","Ù…Ø­Ø§ØµØ±Ù‡ Ø§Ù‚ØªØµØ§Ø¯ÛŒ"]),
    ("âš ï¸", ["threat","warn","warning","ultimatum","red line","will respond"],
           ["ØªÙ‡Ø¯ÛŒØ¯","Ù‡Ø´Ø¯Ø§Ø±","Ø®Ø· Ù‚Ø±Ù…Ø²","Ø§ÙˆÙ„ØªÛŒÙ…Ø§ØªÙˆÙ…"]),
    ("ğŸ¤", ["negotiation","talks","deal","diplomacy","ceasefire","agreement"],
           ["Ù…Ø°Ø§Ú©Ø±Ù‡","ØªÙˆØ§ÙÙ‚","Ø¢ØªØ´â€ŒØ¨Ø³","Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ"]),
    ("ğŸ“œ", ["statement","declared","announced","press conference","spokesperson"],
           ["Ø¨ÛŒØ§Ù†ÛŒÙ‡","Ø§Ø¹Ù„Ø§Ù…","Ù†Ø´Ø³Øª Ø®Ø¨Ø±ÛŒ","Ø³Ø®Ù†Ú¯Ùˆ"]),
]

def analyze_sentiment(text: str) -> list:
    txt = text.lower()
    found = []
    for icon, en_kws, fa_kws in SENTIMENT_RULES:
        if any(kw in txt for kw in en_kws) or any(kw in txt for kw in fa_kws):
            found.append(icon)
        if len(found) >= 3: break
    return found or ["ğŸ“°"]

def calc_importance(title: str, body: str, icons: list, stype: str) -> int:
    txt = (title + " " + body).lower()
    score = sum(IMPORTANCE_BOOST.get(ic, 0) for ic in icons)
    if any(k in txt for k in BREAKING_KEYWORDS): score += 2
    if stype == "tw" and score > 0: score += 1
    return min(score, 10)

def sentiment_bar(icons): return "  ".join(icons)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Telegram Ø§Ø±Ø³Ø§Ù„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _tgapi(path: str) -> str:
    return f"https://api.telegram.org/bot{BOT_TOKEN}/{path}"

async def tg_send_text(client: httpx.AsyncClient, text: str) -> bool:
    text = text[:MAX_MSG_LEN]
    for attempt in range(3):
        try:
            r = await client.post(_tgapi("sendMessage"),
                json={"chat_id": CHANNEL_ID, "text": text,
                      "parse_mode": "HTML", "disable_web_page_preview": False},
                timeout=httpx.Timeout(15.0))
            d = r.json()
            if r.status_code == 200 and d.get("ok"): return True
            if d.get("error_code") == 429:
                wait = d.get("parameters", {}).get("retry_after", 20)
                await asyncio.sleep(wait)
            elif attempt < 2:
                await asyncio.sleep(3)
        except Exception as e:
            log.warning(f"TG send: {e}")
            if attempt < 2: await asyncio.sleep(5)
    return False

async def tg_send_photo(client: httpx.AsyncClient, buf: io.BytesIO,
                         caption: str) -> bool:
    caption = caption[:1024]
    try:
        buf.seek(0)
        r = await client.post(_tgapi("sendPhoto"),
            data={"chat_id": CHANNEL_ID, "caption": caption, "parse_mode": "HTML"},
            files={"photo": ("card.jpg", buf, "image/jpeg")},
            timeout=httpx.Timeout(20.0))
        return r.status_code == 200 and r.json().get("ok", False)
    except Exception as e:
        log.warning(f"TG photo: {e}"); return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIL Ú©Ø§Ø±Øª Ø®Ø¨Ø±ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BG_DARK  = (14, 16, 22)
BG_BAR   = (22, 26, 34)
FG_WHITE = (235, 237, 242)
FG_GREY  = (120, 132, 148)
ACCENT_MAP = {
    "ğŸ‡®ğŸ‡·":(180,40,40), "ğŸ‡®ğŸ‡±":(30,90,180), "ğŸ‡ºğŸ‡¸":(40,80,160),
    "ğŸ”":(60,130,80), "ğŸŒ":(100,60,130), "ğŸ›ï¸":(140,100,40),
}
ICON_BG = {
    "ğŸ’€":(140,20,20),"ğŸ”´":(180,30,30),"ğŸ’¥":(190,80,10),
    "âœˆï¸":(20,90,160),"ğŸš€":(100,20,160),"â˜¢ï¸":(0,130,50),
    "ğŸš¢":(10,80,140),"ğŸ•µï¸":(60,55,70),"ğŸ›¡ï¸":(20,110,80),
    "ğŸ”¥":(180,60,0),"ğŸ’°":(130,110,0),"âš ï¸":(160,110,0),
    "ğŸ¤":(20,120,100),"ğŸ“œ":(60,80,100),"ğŸ“°":(45,58,72),
}

def _get_accent(src, urgent):
    if urgent: return (210, 40, 40)
    for k, v in ACCENT_MAP.items():
        if src.startswith(k) or k in src: return v
    return (80, 110, 140)

def _wrap(text, chars):
    words, lines_out, cur = text.split(), [], ""
    for w in words:
        if len(cur) + len(w) + 1 <= chars: cur = (cur + " " + w).strip()
        else:
            if cur: lines_out.append(cur)
            cur = w
    if cur: lines_out.append(cur)
    return lines_out

def _fonts():
    try:
        bold = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
        reg  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        sm   = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 13)
        return bold, reg, sm
    except:
        d = ImageFont.load_default(); return d, d, d

def make_news_card(headline, fa_text, src, dt_str,
                   urgent=False, sentiment_icons=None):
    if not PIL_OK: return None
    try:
        W, H = 960, 310
        acc = _get_accent(src, urgent)
        img = Image.new("RGB", (W, H), BG_DARK)
        drw = ImageDraw.Draw(img)
        F_H, F_B, F_sm = _fonts()

        drw.rectangle([(0,0),(W,5)], fill=acc)
        drw.rectangle([(0,5),(W,58)], fill=BG_BAR)
        drw.rectangle([(0,58),(W,61)], fill=acc)
        drw.text((18,18), src[:55],     font=F_sm, fill=acc)
        drw.text((W-170,18), dt_str[:25], font=F_sm, fill=FG_GREY)

        display = fa_text if (fa_text and len(fa_text) > 5) else headline
        y = 72
        for line in _wrap(display, 50)[:4]:
            drw.text((W-18, y), line, font=F_H, fill=FG_WHITE, anchor="ra")
            y += 30

        drw.rectangle([(0,H-56),(W,H)], fill=BG_BAR)
        drw.rectangle([(0,H-58),(W,H-56)], fill=acc)
        x_pos = 16
        for ico in (sentiment_icons or ["ğŸ“°"])[:4]:
            bg = ICON_BG.get(ico, (50,65,75))
            drw.rounded_rectangle([(x_pos-2,H-52),(x_pos+38,H-6)], radius=7, fill=bg)
            drw.text((x_pos+2,H-50), ico, font=F_H, fill=(255,255,255))
            x_pos += 50

        if urgent: drw.rectangle([(0,61),(5,H-58)], fill=acc)

        buf = io.BytesIO()
        img.save(buf, "JPEG", quality=85)
        buf.seek(0)
        return buf
    except Exception as e:
        log.debug(f"card: {e}"); return None

# Article fetcher Ø¨Ø±Ø§ÛŒ Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù…Ù‡Ù…
_ARTICLE_SEL = [
    "article","[class*='article-body']","[class*='story-body']",
    ".entry-content",".post-content","[itemprop='articleBody']",
]

async def fetch_article_text(client: httpx.AsyncClient, url: str) -> str:
    if not url or "t.me" in url: return ""
    try:
        r = await client.get(url, timeout=httpx.Timeout(7.0), headers=COMMON_UA,
                             follow_redirects=True)
        if r.status_code != 200: return ""
        soup = BeautifulSoup(r.text, "html.parser")
        for tag in soup.find_all(["script","style","nav","header","footer","aside"]):
            tag.decompose()
        for sel in _ARTICLE_SEL:
            el = soup.select_one(sel)
            if el:
                txt = el.get_text(" ", strip=True)
                if len(txt) > 150: return txt[:1000]
        paras = [p.get_text(" ", strip=True) for p in soup.find_all("p") if len(p.get_text()) > 60]
        return " ".join(paras)[:1000] if paras else ""
    except: return ""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    global _TW_SEMA
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID Ù†ÛŒØ³Øª!"); return

    # â”€â”€ semaphore Twitter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _TW_SEMA = asyncio.Semaphore(20)  # Û²Û° handle Ù‡Ù…Ø²Ù…Ø§Ù†

    # â”€â”€ cutoff Ù‡ÙˆØ´Ù…Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # = Ø¢Ø®Ø±ÛŒÙ† Ø§Ø¬Ø±Ø§ - BUFFER â†’ ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± ÙˆØ§Ù‚Ø¹Ø§Ù‹ ØªØ§Ø²Ù‡
    last_run   = load_run_state()
    cutoff     = last_run - timedelta(minutes=CUTOFF_BUFFER_MIN)
    # Ø­Ø¯Ø§Ú©Ø«Ø± MAX_LOOKBACK_MIN Ø¨Ù‡ Ø¹Ù‚Ø¨ (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ / Ø¨Ø¹Ø¯ Ø§Ø² crash)
    max_cutoff = datetime.now(timezone.utc) - timedelta(minutes=MAX_LOOKBACK_MIN)
    cutoff     = max(cutoff, max_cutoff)

    seen    = load_seen()
    stories = load_stories()

    log.info("=" * 65)
    log.info(f"ğŸš€ WarBot v17  |  {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')}")
    log.info(f"   ğŸ“¡ {len(ALL_RSS_FEEDS)} RSS  ğŸ“¢ {len(TELEGRAM_CHANNELS)} TG  ğ• {len(TWITTER_HANDLES)} TW")
    log.info(f"   PIL:{'âœ…' if PIL_OK else 'âŒ'}  seen:{len(seen)}")
    log.info(f"   â± cutoff={cutoff.astimezone(TEHRAN_TZ).strftime('%H:%M')} ØªÙ‡Ø±Ø§Ù†"
             f"  (last_run={last_run.astimezone(TEHRAN_TZ).strftime('%H:%M')})")
    log.info("=" * 65)

    limits = httpx.Limits(max_connections=100, max_keepalive_connections=30)
    async with httpx.AsyncClient(follow_redirects=True, limits=limits) as client:

        # â”€â”€ ADS-B + fetch Ù…ÙˆØ§Ø²ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        flight_task = asyncio.create_task(fetch_military_flights(client))
        raw_task    = asyncio.create_task(fetch_all(client, cutoff))
        flight_msgs, raw = await asyncio.gather(flight_task, raw_task)
        log.info(f"ğŸ“¥ {len(raw)} Ø¢ÛŒØªÙ… Ø®Ø§Ù…  âœˆï¸ {len(flight_msgs)} ØªØ­Ø±Ú©")

        # â”€â”€ Ù¾Ø±Ø¯Ø§Ø²Ø´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        collected = []
        sent_ids  = set()
        cnt_old = cnt_irrel = cnt_url = cnt_story = 0

        for entry, src_name, src_type, is_emb in raw:
            eid = make_id(entry)

            # Ù„Ø§ÛŒÙ‡ Û±: Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ØŸ
            if eid in seen:
                cnt_url += 1; continue

            # Ù„Ø§ÛŒÙ‡ Û²: Ø¯Ø± Ù¾Ù†Ø¬Ø±Ù‡ Ø²Ù…Ø§Ù†ÛŒØŸ (TG Ù‚Ø¨Ù„Ø§Ù‹ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ØŒ RSS Ø§ÛŒÙ†Ø¬Ø§)
            if not is_fresh(entry, cutoff):
                cnt_old += 1; continue

            # Ù„Ø§ÛŒÙ‡ Û³: Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¬Ù†Ú¯ØŸ
            t    = clean_html(entry.get("title", ""))
            s    = clean_html(entry.get("summary") or entry.get("description") or "")
            full = f"{t} {s}"
            if not is_war_relevant(full, is_embassy=is_emb,
                                   is_tg=(src_type=="tg"), is_tw=(src_type=="tw")):
                cnt_irrel += 1; continue

            # Ù„Ø§ÛŒÙ‡ Û´: story ØªÚ©Ø±Ø§Ø±ÛŒØŸ
            if is_story_dup(t, stories):
                seen.add(eid)   # story-dup â†’ Ø¨Ù‡ seen Ø§Ø¶Ø§ÙÙ‡ (Ø¨Ø±Ø§ÛŒ Ù‡Ø± run ØªÚ©Ø±Ø§Ø± Ù†Ø´Ù‡)
                cnt_story += 1; continue

            collected.append((eid, entry, src_name, src_type, is_emb))
            stories = register_story(t, stories)

        log.info(
            f"ğŸ“Š Ù‚Ø¯ÛŒÙ…ÛŒ:{cnt_old}  Ù†Ø§Ù…Ø±ØªØ¨Ø·:{cnt_irrel}  "
            f"dup:{cnt_url}  story-dup:{cnt_story}  âœ… {len(collected)} Ø®Ø¨Ø±"
        )

        # Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø§ÙˆÙ„ØŒ Ø­Ø¯Ø§Ú©Ø«Ø± MAX_NEW_PER_RUN
        collected = list(reversed(collected))
        if len(collected) > MAX_NEW_PER_RUN:
            log.warning(f"âš ï¸ {len(collected)} â†’ {MAX_NEW_PER_RUN} (Ø¨Ø±Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯)")
            collected = collected[-MAX_NEW_PER_RUN:]

        # â”€â”€ ADS-B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for msg in flight_msgs[:3]:
            await tg_send_text(client, msg)
            await asyncio.sleep(0.5)

        if not collected:
            log.info("ğŸ’¤ Ø®Ø¨Ø± Ø¬Ù†Ú¯ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ù†ÛŒØ³Øª")
            save_seen(seen); save_stories(stories); save_run_state()
            return

        # â”€â”€ ØªØ±Ø¬Ù…Ù‡ Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        arts_in = [
            (trim(clean_html(e.get("title", "")), 400),
             trim(clean_html(e.get("summary") or e.get("description") or ""), 600))
            for _, e, _, _, _ in collected
        ]
        if GEMINI_API_KEY:
            log.info(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡ {len(arts_in)} Ø®Ø¨Ø±...")
            translations = await translate_batch(client, arts_in)
        else:
            translations = arts_in

        # â”€â”€ Ø§Ø±Ø³Ø§Ù„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        sent = 0
        for i, (eid, entry, src_name, stype, is_emb) in enumerate(collected):
            fa, _    = translations[i]
            en_title = arts_in[i][0]
            en_body  = arts_in[i][1]
            link     = entry.get("link", "")
            dt_str   = format_dt(entry)
            display  = fa if (fa and fa != en_title and len(fa) > 5) else en_title
            urgent   = any(w in (fa + en_title).lower() for w in [
                "attack","strike","killed","bomb","explosion","nuclear",
                "Ø­Ù…Ù„Ù‡","Ú©Ø´ØªÙ‡","Ø§Ù†ÙØ¬Ø§Ø±","Ù…ÙˆØ´Ú©","Ø´Ù‡ÛŒØ¯","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ",
            ])

            sentiment_icons = analyze_sentiment(f"{fa} {en_title} {en_body}")
            s_bar      = sentiment_bar(sentiment_icons)
            importance = calc_importance(en_title, en_body, sentiment_icons, stype)
            src_icon   = "ğŸ›ï¸" if is_emb else ("ğ•" if stype=="tw" else ("ğŸ“¢" if stype=="tg" else "ğŸ“¡"))

            log.info(f"  â†’ [{stype}] imp={importance}  {en_title[:60]}")

            card_sent = False
            if PIL_OK:
                buf = make_news_card(en_title,
                                     fa if (fa and fa != en_title) else "",
                                     src_name, dt_str, urgent, sentiment_icons)
                if buf:
                    cap  = f"{s_bar}\n\n<b>{esc(display)}</b>"
                    # body ÙÙ‚Ø· Ø§Ú¯Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ø¯Ø§Ø±Ø¯
                    if en_body and len(en_body) > 40 and en_body.lower() not in en_title.lower():
                        cap += f"\n\n<i>{esc(trim(en_body, 600))}</i>"
                    cap += f"\n\n{src_icon} <b>{esc(src_name)}</b>  {dt_str}"
                    if await tg_send_photo(client, buf, cap):
                        card_sent = True

            if not card_sent:
                parts = [s_bar, f"<b>{esc(display)}</b>"]
                if en_body and len(en_body) > 40 and en_body.lower() not in en_title.lower():
                    parts += ["", f"<i>{esc(trim(en_body, 700))}</i>"]
                parts += ["", f"â”€â”€â”€ {src_icon} <b>{esc(src_name)}</b>  {dt_str}"]
                if await tg_send_text(client, "\n".join(parts)):
                    card_sent = True

            if card_sent:
                sent_ids.add(eid); sent += 1
                log.info(f"    âœ… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")
            await asyncio.sleep(SEND_DELAY)

        # ÙÙ‚Ø· Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ seen
        seen.update(sent_ids)
        save_seen(seen); save_stories(stories); save_run_state()
        log.info(f"ğŸ {sent}/{len(collected)} Ø®Ø¨Ø±  seen:{len(seen)}")


if __name__ == "__main__":
    asyncio.run(main())
