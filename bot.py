"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸ›¡ï¸ Military Intel Bot v10 â€” ALL BUGS FIXED                        â•‘
â•‘                                                                          â•‘
â•‘  âœ… Fix1: Twitter/RSSHub Ù…Ø±Ø¯Ù‡ â†’ Google News journalist search            â•‘
â•‘  âœ… Fix2: Gemini 429 â†’ dual-model fallback + exponential backoff         â•‘
â•‘  âœ… Fix3: URLÙ‡Ø§ÛŒ Ù…Ø±Ø¯Ù‡ Ø­Ø°Ù/Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù†Ø¯                                   â•‘
â•‘  âœ… Fix4: Cutoff 2h â†’ 6h (96% Ø®Ø¨Ø±Ù‡Ø§ Ù‚Ø¯ÛŒÙ…ÛŒ ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯)               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os, json, hashlib, asyncio, logging, re
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S")
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE       = "seen.json"
MAX_NEW_PER_RUN = 20
MAX_MSG_LEN     = 4096
SEND_DELAY      = 2
TEHRAN_TZ       = pytz.timezone("Asia/Tehran")

# âœ… Fix4: Ù¾Ù†Ø¬Ø±Ù‡ Û¶ Ø³Ø§Ø¹Øª â€” Ù‚Ø¨Ù„Ø§Ù‹ Û² Ø³Ø§Ø¹Øª Ø¨ÙˆØ¯ Ùˆ 96% Ø®Ø¨Ø±Ù‡Ø§ Ø±Ø¯ Ù…ÛŒâ€ŒØ´Ø¯
CUTOFF_HOURS = 6

def get_cutoff() -> datetime:
    return datetime.now(timezone.utc) - timedelta(hours=CUTOFF_HOURS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ Û±. ÙÛŒØ¯Ù‡Ø§ÛŒ RSS â€” ÙÙ‚Ø· URLÙ‡Ø§ÛŒ Ú©Ø§Ø±â€ŒÚ©Ø±Ø¯Ù‡ (Ø§Ø² Ù„Ø§Ú¯ ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [
    # â•â• Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ â•â•
    {"name": "ğŸŒ Reuters World",      "url": "https://feeds.reuters.com/reuters/worldNews"},
    {"name": "ğŸŒ Reuters Top",        "url": "https://feeds.reuters.com/reuters/topNews"},
    {"name": "ğŸŒ AP Top",             "url": "https://feeds.apnews.com/rss/apf-topnews"},
    {"name": "ğŸŒ AP World",           "url": "https://feeds.apnews.com/rss/apf-WorldNews"},
    {"name": "ğŸŒ AP Military",        "url": "https://apnews.com/hub/military-and-defense?format=rss"},
    {"name": "ğŸŒ Bloomberg Politics", "url": "https://feeds.bloomberg.com/politics/news.rss"},
    {"name": "ğŸŒ WSJ World",          "url": "https://feeds.a.dj.com/rss/RSSWorldNews.xml"},
    # âœ… NYT â†’ Ø§Ø² Google News (RSS 404 Ø§Ø³Øª)
    {"name": "ğŸŒ NYT (GNews)",        "url": "https://news.google.com/rss/search?q=site:nytimes.com+iran+israel+military&hl=en-US&gl=US&ceid=US:en"},
    # âœ… CNN â€” Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    {"name": "ğŸŒ CNN Middle East",    "url": "http://rss.cnn.com/rss/edition_meast.rss"},
    {"name": "ğŸŒ CNN World",          "url": "http://rss.cnn.com/rss/edition_world.rss"},
    # âœ… BBC â€” Ø¨Ø§ https Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    {"name": "ğŸŒ BBC Middle East",    "url": "https://feeds.bbci.co.uk/news/world/middle_east/rss.xml"},
    {"name": "ğŸŒ BBC World",          "url": "https://feeds.bbci.co.uk/news/world/rss.xml"},
    {"name": "ğŸŒ Al Jazeera",         "url": "https://www.aljazeera.com/xml/rss/all.xml"},
    {"name": "ğŸŒ Fox News World",     "url": "https://moxie.foxnews.com/google-publisher/world.xml"},
    {"name": "ğŸŒ Politico NatSec",   "url": "https://rss.politico.com/defense.xml"},
    {"name": "ğŸŒ Politico Politics", "url": "https://rss.politico.com/politics-news.xml"},
    # âœ… The Hill â€” Ø¨Ø§ redirect Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    {"name": "ğŸŒ The Hill",           "url": "https://thehill.com/news/feed/"},
    {"name": "ğŸŒ Foreign Policy",     "url": "https://foreignpolicy.com/feed/"},
    {"name": "ğŸŒ Foreign Affairs",    "url": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "ğŸŒ The Intercept",      "url": "https://theintercept.com/feed/?rss=1"},
    {"name": "ğŸŒ Middle East Eye",    "url": "https://www.middleeasteye.net/rss"},

    # â•â• Ø§Ú©Ø³ÛŒÙˆØ³ â€” Ø§Ø² Google News â•â•
    {"name": "ğŸ“° Axios (GNews)",      "url": "https://news.google.com/rss/search?q=site:axios.com+iran+israel+military+national+security&hl=en-US&gl=US&ceid=US:en"},

    # â•â• Ø¢Ù…Ø±ÛŒÚ©Ø§ Ù†Ø¸Ø§Ù…ÛŒ â€” ÙÙ‚Ø· URLÙ‡Ø§ÛŒ Ú©Ø§Ø±â€ŒÚ©Ø±Ø¯Ù‡ â•â•
    {"name": "ğŸ‡ºğŸ‡¸ Pentagon",          "url": "https://www.defense.gov/DesktopModules/ArticleCS/RSS.ashx?ContentType=1&Site=945&max=10"},
    {"name": "ğŸ‡ºğŸ‡¸ CENTCOM (GNews)",   "url": "https://news.google.com/rss/search?q=CENTCOM+military+operation+Iran+Iraq&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ‡ºğŸ‡¸ USNI News",         "url": "https://news.usni.org/feed"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",  "url": "https://breakingdefense.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense News",      "url": "https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"name": "ğŸ‡ºğŸ‡¸ Military Times",    "url": "https://www.militarytimes.com/arc/outboundfeeds/rss/"},
    # âœ… Stars & Stripes â†’ GNews (feed 404)
    {"name": "ğŸ‡ºğŸ‡¸ Stars & Stripes",   "url": "https://news.google.com/rss/search?q=site:stripes.com+iran+israel+military&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ‡ºğŸ‡¸ C4ISRNET",          "url": "https://www.c4isrnet.com/arc/outboundfeeds/rss/"},
    # âœ… The War Zone â€” URL ØµØ­ÛŒØ­ (twz.com Ú©Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
    {"name": "ğŸ‡ºğŸ‡¸ The War Zone",      "url": "https://www.twz.com/feed"},
    {"name": "ğŸ‡ºğŸ‡¸ War on Rocks",      "url": "https://warontherocks.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Task & Purpose",    "url": "https://taskandpurpose.com/feed/"},

    # â•â• Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â•â•
    {"name": "ğŸ‡®ğŸ‡± IDF (GNews)",       "url": "https://news.google.com/rss/search?q=IDF+Israel+Defense+Forces+operation+strike&hl=en-US&gl=US&ceid=US:en"},
    # âœ… JP All headlines (Military 404 Ø§Ø³Øª)
    {"name": "ğŸ‡®ğŸ‡± Jerusalem Post",    "url": "https://www.jpost.com/rss/rssfeedsheadlines.aspx"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",   "url": "https://www.timesofisrael.com/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Haaretz (GNews)",   "url": "https://news.google.com/rss/search?q=site:haaretz.com+iran+israel+war+military&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ‡®ğŸ‡± Israel Hayom",      "url": "https://www.israelhayom.com/feed/"},
    # âœ… Ynetnews â†’ GNews (feed 404)
    {"name": "ğŸ‡®ğŸ‡± Ynetnews (GNews)",  "url": "https://news.google.com/rss/search?q=site:ynetnews.com+iran+israel+military&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ‡®ğŸ‡± i24 News",          "url": "https://www.i24news.tv/en/rss"},
    # âœ… Arutz Sheva â€” Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    {"name": "ğŸ‡®ğŸ‡± Arutz Sheva",       "url": "https://www.israelnationalnews.com/rss.aspx"},

    # â•â• Ø§ÛŒØ±Ø§Ù† â•â•
    {"name": "ğŸ‡®ğŸ‡· Iran International","url": "https://www.iranintl.com/en/rss"},
    # âœ… Radio Farda â€” URL Ø¬Ø¯ÛŒØ¯ RFE/RL
    {"name": "ğŸ‡®ğŸ‡· Radio Farda",       "url": "https://www.radiofarda.com/api/zoyqvpemr"},

    # â•â• ØªØ­Ù„ÛŒÙ„ÛŒ / OSINT â•â•
    {"name": "ğŸ” ISW (GNews)",        "url": "https://news.google.com/rss/search?q=site:understandingwar.org+iran+israel&hl=en-US&gl=US&ceid=US:en"},
    {"name": "ğŸ” Long War Journal",   "url": "https://www.longwarjournal.org/feed"},
    {"name": "ğŸ” Bellingcat",         "url": "https://www.bellingcat.com/feed/"},
    {"name": "ğŸ” OSINT Defender",     "url": "https://osintdefender.com/feed/"},
    # âœ… RAND â†’ GNews (XML 404)
    {"name": "ğŸ” RAND (GNews)",       "url": "https://news.google.com/rss/search?q=site:rand.org+iran+israel+military+nuclear&hl=en-US&gl=US&ceid=US:en"},
    # âœ… Lawfare â†’ GNews (403)
    {"name": "ğŸ” Lawfare (GNews)",    "url": "https://news.google.com/rss/search?q=site:lawfaremedia.org+iran+israel&hl=en-US&gl=US&ceid=US:en"},
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ Û². Google News â€” Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TOPIC_QUERIES = [
    ("âš”ï¸ Iran Israel War",      "Iran Israel war attack strike"),
    ("âš”ï¸ Iran Airstrike",       "Iran airstrike bomb explosion"),
    ("âš”ï¸ US Iran Military",     "United States Iran military IRGC"),
    ("âš”ï¸ IDF Operation",        "IDF military operation strike Gaza"),
    ("âš”ï¸ Iran Nuclear",         "Iran nuclear IAEA uranium enrichment"),
    ("âš”ï¸ Iran Missile Drone",   "Iran ballistic missile drone attack"),
    ("âš”ï¸ Hezbollah IDF",        "Hezbollah IDF Lebanon border strike"),
    ("âš”ï¸ Strait Hormuz",        "Strait Hormuz tanker navy seized"),
    ("âš”ï¸ IRGC Attack",          "IRGC Revolutionary Guard attack base"),
    ("âš”ï¸ Israel Airstrike",     "Israel airstrike Syria Iraq Iran"),
    ("âš”ï¸ Mossad Operation",     "Mossad CIA covert operation"),
    ("âš”ï¸ US Navy Gulf",         "US carrier strike group Persian Gulf"),
    ("âš”ï¸ Iran Sanctions",       "Iran sanctions oil SWIFT 2026"),
    ("âš”ï¸ Red Sea Houthis",      "Red Sea Houthi attack ship missile"),
    ("âš”ï¸ Gaza Deal",            "Gaza ceasefire Hamas IDF deal"),
    ("âš”ï¸ Iran Proxy",           "Iran proxy militia Iraq Syria US base"),
    ("âš”ï¸ Nuclear Escalation",   "nuclear military escalation Middle East"),
    ("âš”ï¸ Trump Iran Israel",    "Trump Iran Israel military policy"),
    ("âš”ï¸ Khamenei Netanyahu",   "Khamenei Netanyahu threat war"),
    ("âš”ï¸ Iron Dome",            "Iron Dome Patriot Arrow missile intercept"),
]

def gnews(q):
    return f"https://news.google.com/rss/search?q={q.replace(' ','+')}&hl=en-US&gl=US&ceid=US:en&num=15"

TOPIC_FEEDS = [{"name": n, "url": gnews(q)} for n, q in TOPIC_QUERIES]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€ âœ… Fix1: Twitter â†’ Google News journalist search
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSSHub Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ù„Ø§Ú© Ø´Ø¯Ù‡ (rsshub.app â†’ google.com/404)
# Ø±Ø§Ù‡â€ŒØ­Ù„: Google News Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ø§Ù… Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± + Ù…ÙˆØ¶ÙˆØ¹ = Ù‡Ù…Ø§Ù† Ø®Ø¨Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
JOURNALIST_QUERIES = [
    # OSINT
    ("ğŸ” OSINTdefender",       "OSINTdefender iran israel military"),
    ("ğŸ” Intel Crab",          "IntelCrab military attack strike"),
    ("ğŸ” War Monitor",         "WarMonitor conflict strike attack"),
    ("ğŸ” Aurora Intel",        "AuroraIntel military intelligence"),
    ("ğŸ” GeoConfirmed",        "GeoConfirmed military conflict"),
    # Axios
    ("ğŸ“° Barak Ravid",         "Barak Ravid Iran Israel Axios"),
    ("ğŸ“° Alex Ward",           "Alex Ward national security Axios"),
    # Reuters
    ("ğŸ“° Idrees Ali",          "Idrees Ali Pentagon Reuters"),
    ("ğŸ“° Phil Stewart",        "Phil Stewart military Reuters"),
    # NYT
    ("ğŸ“° Farnaz Fassihi",      "Farnaz Fassihi Iran NYT"),
    ("ğŸ“° Eric Schmitt",        "Eric Schmitt military national security NYT"),
    # WaPo
    ("ğŸ“° Dan Lamothe",         "Dan Lamothe military Washington Post"),
    # Politico / FP
    ("ğŸ“° Lara Seligman",       "Lara Seligman defense Politico"),
    ("ğŸ“° Jack Detsch",         "Jack Detsch Pentagon Foreign Policy"),
    # Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    ("ğŸ‡®ğŸ‡± Yossi Melman",       "Yossi Melman Mossad Israel intelligence"),
    ("ğŸ‡®ğŸ‡± Seth Frantzman",     "Seth Frantzman Israel defense"),
    # Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ
    ("ğŸŒ Joyce Karam",         "Joyce Karam Middle East national security"),
    ("ğŸŒ Ragip Soylu",         "Ragip Soylu Middle East Turkey"),
    # Ù‡Ø´Ø¯Ø§Ø±
    ("âš ï¸ DEFCON",              "DEFCON nuclear alert military escalation"),
    ("âš ï¸ Arms Control",        "arms control nuclear Iran missile"),
]

JOURNALIST_FEEDS = [
    {"name": f"ğ• {n}", "url": gnews(q), "is_journalist": True}
    for n, q in JOURNALIST_QUERIES
]

ALL_FEEDS = RSS_FEEDS + TOPIC_FEEDS + JOURNALIST_FEEDS

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÙÛŒÙ„ØªØ±Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEYWORDS = [
    "Ø³Ù¾Ø§Ù‡","Ù…ÙˆØ´Ú©","Ø¬Ù†Ú¯","Ø­Ù…Ù„Ù‡","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø§ÛŒØ±Ø§Ù†","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ù¾Ù‡Ù¾Ø§Ø¯","Ù†Ø¸Ø§Ù…ÛŒ",
    "iran","irgc","khamenei","tehran","revolutionary guard","nuclear",
    "israel","idf","mossad","netanyahu","hamas","hezbollah","houthi",
    "pentagon","centcom","us forces","us military","us base",
    "strike","airstrike","missile","ballistic","drone",
    "attack","bomb","explosion","assassination","operation",
    "warship","carrier","navy","air force",
    "persian gulf","strait of hormuz","red sea","middle east",
    "iron dome","arrow","patriot","hypersonic",
    "uranium","enrichment","natanz","fordo","iaea",
    "intelligence","cia","covert","sanction",
    "gaza","west bank","lebanon","syria","iraq","yemen",
    "trump","war","conflict","escalat","deploy",
]

def is_fresh(entry: dict) -> bool:
    cutoff = get_cutoff()
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t: return False
        dt = datetime(*t[:6], tzinfo=timezone.utc)
        return dt >= cutoff
    except:
        return False

def is_relevant(entry: dict, is_journalist: bool = False) -> bool:
    text = " ".join([
        str(entry.get("title", "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()
    if is_journalist:
        kw = ["iran","israel","idf","irgc","strike","war","attack","missile",
              "drone","military","nuclear","hezbollah","hamas","houthi",
              "centcom","pentagon","gaza","lebanon","tehran","sanction"]
        return any(k in text for k in kw)
    return any(k in text for k in KEYWORDS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ÙÛŒØ¯Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_one(client: httpx.AsyncClient, cfg: dict) -> list:
    try:
        r = await client.get(
            cfg["url"],
            timeout=httpx.Timeout(12.0),
            headers={"User-Agent": "Mozilla/5.0 MilNewsBot/10.0"}
        )
        if r.status_code == 200:
            entries = feedparser.parse(r.text).entries
            return entries or []
    except:
        pass
    return []

async def fetch_all(client: httpx.AsyncClient) -> list:
    tasks = [fetch_one(client, cfg) for cfg in ALL_FEEDS]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    out = []
    for i, res in enumerate(results):
        if isinstance(res, list):
            cfg = ALL_FEEDS[i]
            is_j = bool(cfg.get("is_journalist"))
            for entry in res:
                out.append((entry, cfg, is_j))
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âœ… Fix2: Gemini â€” dual-model + exponential backoff
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_BASE = "https://generativelanguage.googleapis.com/v1beta/models"
# Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ Ùˆ fallback Ø¨Ø§ quota Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
GEMINI_MODELS = [
    "gemini-2.0-flash",       # Ø§ØµÙ„ÛŒ: 15 RPM Ø±Ø§ÛŒÚ¯Ø§Ù†
    "gemini-1.5-flash",       # fallback: quota Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
    "gemini-1.5-flash-8b",    # Ø¢Ø®Ø±ÛŒÙ† fallback: Ø³Ø¨Ú©â€ŒØªØ±
]

async def translate_batch(
    client: httpx.AsyncClient,
    articles: list[tuple[str, str]]
) -> list[tuple[str, str]]:
    if not GEMINI_API_KEY or not articles:
        return articles

    items_text = ""
    for i, (title, summary) in enumerate(articles):
        items_text += f"###ITEM_{i}###\nTITLE: {title[:300]}\nBODY: {summary[:450]}\n"

    prompt = f"""ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø®Ø¨Ø± Ù†Ø¸Ø§Ù…ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø®Ø¨Ø±ÛŒ.
Ù‚ÙˆØ§Ù†ÛŒÙ†: ÙÙ‚Ø· ØªØ±Ø¬Ù…Ù‡ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­. Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ Ø¯Ù‚ÛŒÙ‚. Ù„Ø­Ù† Ø±Ø³Ù…ÛŒ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ.

ÙØ±Ù…Øª Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹:
###ITEM_0###
Ø¹Ù†ÙˆØ§Ù†: [ØªØ±Ø¬Ù…Ù‡]
Ù…ØªÙ†: [ØªØ±Ø¬Ù…Ù‡]
###ITEM_1###
Ø¹Ù†ÙˆØ§Ù†: [ØªØ±Ø¬Ù…Ù‡]
Ù…ØªÙ†: [ØªØ±Ø¬Ù…Ù‡]
...

===Ø®Ø¨Ø±Ù‡Ø§===
{items_text}"""

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.05, "maxOutputTokens": 8192}
    }

    # Ø§Ù…ØªØ­Ø§Ù† Ù‡Ø± Ù…Ø¯Ù„ Ø¨Ù‡ ØªØ±ØªÛŒØ¨
    for model in GEMINI_MODELS:
        url = f"{GEMINI_BASE}/{model}:generateContent?key={GEMINI_API_KEY}"
        wait = 35  # Ø´Ø±ÙˆØ¹ Ø¨Ø§ Û³Ûµ Ø«Ø§Ù†ÛŒÙ‡

        for attempt in range(3):
            try:
                log.info(f"ğŸŒ Gemini [{model}] â€” attempt {attempt+1}")
                r = await client.post(url, json=payload, timeout=httpx.Timeout(90.0))

                if r.status_code == 200:
                    raw = r.json()["candidates"][0]["content"]["parts"][0]["text"]
                    result = _parse_batch(raw, articles)
                    ok = sum(1 for i, x in enumerate(result) if x != articles[i])
                    log.info(f"âœ… Gemini [{model}]: {ok}/{len(articles)} ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯")
                    return result

                elif r.status_code == 429:
                    retry_h = r.headers.get("Retry-After", "")
                    wait_s  = int(retry_h) if retry_h.isdigit() else wait
                    log.warning(f"â³ Gemini [{model}] 429 â€” {wait_s}s ØµØ¨Ø± (attempt {attempt+1})")
                    await asyncio.sleep(wait_s)
                    wait = min(wait * 2, 120)  # exponential backoff ØªØ§ Û² Ø¯Ù‚ÛŒÙ‚Ù‡

                elif r.status_code == 503:
                    log.warning(f"â³ Gemini [{model}] 503 â€” 20s")
                    await asyncio.sleep(20)

                else:
                    log.warning(f"Gemini [{model}] {r.status_code}")
                    break  # Ø§ÛŒÙ† Ù…Ø¯Ù„ Ú©Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¨Ø¹Ø¯ÛŒ

            except asyncio.TimeoutError:
                log.warning(f"â³ Gemini [{model}] timeout")
                await asyncio.sleep(10)
            except Exception as e:
                log.debug(f"Gemini [{model}]: {e}")
                break

        log.warning(f"âš ï¸ Gemini [{model}] Ø´Ú©Ø³Øª â€” Ù…Ø¯Ù„ Ø¨Ø¹Ø¯ÛŒ")

    # Ø§Ú¯Ù‡ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯Ù†ØŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
    log.warning("âš ï¸ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Gemini Ø´Ú©Ø³Øª â€” Ø®Ø¨Ø± Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
    return articles


def _parse_batch(raw: str, fallback: list[tuple[str, str]]) -> list[tuple[str, str]]:
    results = list(fallback)
    pattern = re.compile(
        r'###ITEM_(\d+)###\s*\n'
        r'(?:Ø¹Ù†ÙˆØ§Ù†|title)\s*:\s*(.+?)\s*\n'
        r'(?:Ù…ØªÙ†|body|text)\s*:\s*(.+?)(?=###ITEM_|\Z)',
        re.IGNORECASE | re.DOTALL
    )
    for m in pattern.finditer(raw):
        idx  = int(m.group(1))
        fa_t = m.group(2).strip().replace("**","").replace("*","")
        fa_s = m.group(3).strip().replace("**","").replace("*","")
        if 0 <= idx < len(results) and fa_t:
            results[idx] = (nfa(fa_t), nfa(fa_s))
    return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(text: str) -> str:
    if not text: return ""
    return BeautifulSoup(str(text), "html.parser").get_text(" ", strip=True)

def make_id(entry: dict) -> str:
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def make_title_id(title: str) -> str:
    t = re.sub(r'[^a-z0-9\u0600-\u06FF]', '', title.lower())
    return "t:" + hashlib.md5(t[:200].encode()).hexdigest()

def format_dt(entry: dict) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ).strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except: pass
    return ""

def esc(t: str) -> str:
    return (t or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def trim(t: str, n: int) -> str:
    t = re.sub(r'\s+', ' ', t).strip()
    return t if len(t) <= n else t[:n].rsplit(" ", 1)[0] + "â€¦"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ø§ÙØ¸Ù‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen: set):
    with open(SEEN_FILE, "w") as f:
        json.dump(list(seen)[-15000:], f)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client: httpx.AsyncClient, text: str) -> bool:
    for attempt in range(4):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID,
                "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            }, timeout=httpx.Timeout(15.0))
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                wait = data.get("parameters", {}).get("retry_after", 20)
                await asyncio.sleep(wait)
            elif data.get("error_code") in (400, 403):
                log.error(f"TG fatal: {data.get('description')}")
                return False
            else:
                await asyncio.sleep(5)
        except Exception as e:
            log.warning(f"TG #{attempt+1}: {e}")
            await asyncio.sleep(8)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡!"); return

    seen   = load_seen()
    cutoff = get_cutoff()
    log.info(f"ğŸš€ {len(ALL_FEEDS)} Ù…Ù†Ø¨Ø¹ ({len(RSS_FEEDS)} RSS + {len(TOPIC_FEEDS)} Ù…ÙˆØ¶ÙˆØ¹ + {len(JOURNALIST_FEEDS)} Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±)")
    log.info(f"ğŸ“… Cutoff: {CUTOFF_HOURS} Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ± ({cutoff.astimezone(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')} Ø¨Ù‡ Ø¨Ø¹Ø¯)")
    log.info(f"ğŸ’¾ Ø­Ø§ÙØ¸Ù‡: {len(seen)} Ø®Ø¨Ø± Ù‚Ø¨Ù„ÛŒ")

    async with httpx.AsyncClient(follow_redirects=True) as client:

        # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¯Ø±ÛŒØ§ÙØª
        log.info("â¬ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù†...")
        raw = await fetch_all(client)
        log.info(f"ğŸ“¥ {len(raw)} Ø¢ÛŒØªÙ… Ø®Ø§Ù…")

        # Ù…Ø±Ø­Ù„Ù‡ Û²: ÙÛŒÙ„ØªØ±
        collected  = []
        title_seen = set()
        old_cnt = irrel_cnt = dup_cnt = 0

        for entry, cfg, is_j in raw:
            eid = make_id(entry)
            if eid in seen: continue

            if not is_fresh(entry):
                seen.add(eid); old_cnt += 1; continue

            if not is_relevant(entry, is_journalist=is_j):
                seen.add(eid); irrel_cnt += 1; continue

            raw_title = clean_html(entry.get("title", ""))
            tid = make_title_id(raw_title)
            if tid in title_seen:
                seen.add(eid); dup_cnt += 1; continue

            title_seen.add(tid)
            collected.append((eid, entry, cfg, is_j))

        log.info(f"ğŸ“Š ÙÛŒÙ„ØªØ±: {old_cnt} Ù‚Ø¯ÛŒÙ…ÛŒ | {irrel_cnt} Ù†Ø§Ù…Ø±ØªØ¨Ø· | {dup_cnt} ØªÚ©Ø±Ø§Ø±ÛŒ | âœ… {len(collected)} Ø¬Ø¯ÛŒØ¯")

        collected = list(reversed(collected))
        if len(collected) > MAX_NEW_PER_RUN:
            log.warning(f"âš ï¸ {len(collected)} â†’ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ {MAX_NEW_PER_RUN}")
            collected = collected[-MAX_NEW_PER_RUN:]

        if not collected:
            log.info("ğŸ’¤ Ù‡ÛŒÚ† Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ù†ÛŒØ³Øª")
            save_seen(seen)
            return

        # Ù…Ø±Ø­Ù„Ù‡ Û³: ØªØ±Ø¬Ù…Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        articles_in = []
        for eid, entry, cfg, is_j in collected:
            en_t = trim(clean_html(entry.get("title", "")), 300)
            en_s = trim(clean_html(entry.get("summary") or entry.get("description") or ""), 450)
            articles_in.append((en_t, en_s))

        log.info(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ {len(articles_in)} Ø®Ø¨Ø±...")
        translations = await translate_batch(client, articles_in)

        # Ù…Ø±Ø­Ù„Ù‡ Û´: Ø§Ø±Ø³Ø§Ù„
        sent = 0
        for i, (eid, entry, cfg, is_j) in enumerate(collected):
            en_title         = articles_in[i][0]
            fa_title, fa_sum = translations[i]
            link = entry.get("link", "")
            dt   = format_dt(entry)
            icon = "ğ•" if is_j else "ğŸ“¡"

            lines = [f"ğŸ”´ <b>{esc(fa_title)}</b>", ""]
            if fa_sum and len(fa_sum) > 10 and fa_sum.lower() not in fa_title.lower():
                lines += [esc(fa_sum), ""]
            lines += ["â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€", f"ğŸ“Œ <i>{esc(en_title)}</i>"]
            if dt:   lines.append(dt)
            lines.append(f"{icon} <b>{cfg['name']}</b>")
            if link: lines.append(f'ğŸ”— <a href="{link}">Ù…Ù†Ø¨Ø¹</a>')

            if await tg_send(client, "\n".join(lines)):
                seen.add(eid); sent += 1
                log.info(f"  âœ… {fa_title[:50]}")
            else:
                log.error("  âŒ Ø§Ø±Ø³Ø§Ù„ Ù†Ø§Ù…ÙˆÙÙ‚")
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"ğŸ Ù¾Ø§ÛŒØ§Ù† | {sent}/{len(collected)} Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")

if __name__ == "__main__":
    asyncio.run(main())
