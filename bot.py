import os, json, hashlib, asyncio, logging, re, io
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_OK = True
except ImportError:
    PIL_OK = False

try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("WarBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE         = "seen.json"
STORIES_FILE      = "stories.json"
GEMINI_STATE_FILE = "gemini_state.json"
FLIGHT_ALERT_FILE = "flight_alerts.json"
RUN_STATE_FILE    = "run_state.json"
NITTER_CACHE_FILE = "nitter_cache.json"

# â”€â”€ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ø­Ù„Ù‚Ù‡ Ø¯Ø§Ø¦Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CUTOFF_BUFFER_MIN  = 4    # overlap â€” Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ø§Ø¬Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ù†
MAX_LOOKBACK_MIN   = 90   # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ø±Ú¯Ø´Øª (Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§ / crash)
SEEN_TTL_HOURS     = 12
NITTER_CACHE_TTL   = 900

LOOP_INTERVAL_SEC  = 45   # Ù‡Ø± Û´Ûµ Ø«Ø§Ù†ÛŒÙ‡ ÛŒÚ© Ú†Ø±Ø®Ù‡ â€” Ø§Ø±Ø³Ø§Ù„ ÙÙˆØ±ÛŒ Ù‡Ø± Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯
# Ø¯Ø± GitHub Actions: bot Ø±Ø§ Û³ÛµÛ° Ø¯Ù‚ÛŒÙ‚Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†ØŒ Actions Ù‡Ø± Û¶ Ø³Ø§Ø¹Øª restart Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø­Ù„ÛŒ (CI=False): Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
_CI = bool(os.environ.get("CI") or os.environ.get("GITHUB_ACTIONS"))
BOT_MAX_RUNTIME_MIN = 350 if _CI else 99999

MAX_NEW_PER_RUN    = 50   # Ù‡Ø± Ú†Ø±Ø®Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± ÛµÛ° Ø®Ø¨Ø±
MAX_MSG_LEN        = 4096
SEND_DELAY         = 0.3
JACCARD_THRESHOLD  = 0.78  # dedup Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
MAX_STORIES        = 300   # Ø­Ø§ÙØ¸Ù‡ Ø¨ÛŒØ´ØªØ± = ØªÚ©Ø±Ø§Ø±ÛŒ Ú©Ù…ØªØ±
RSS_TIMEOUT        = 8.0
TG_TIMEOUT         = 10.0
TW_TIMEOUT         = 6.0
RICH_CARD_THRESHOLD = 5

TEHRAN_TZ = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ù†Ø§Ø¨Ø¹ RSS â€” Feb 27 2026 â€” Ù…Ø°Ø§Ú©Ø±Ø§Øª Ú˜Ù†Ùˆ Ø¯ÙˆØ± Ø³ÙˆÙ… / Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IRAN_FEEDS = [
    # â”€â”€â”€ ÙØ§Ø±Ø³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡®ğŸ‡· Ø§ÛŒØ±Ù†Ø§",          "u":"https://www.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· ØªØ³Ù†ÛŒÙ…",         "u":"https://www.tasnimnews.com/fa/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Ù…Ù‡Ø±",           "u":"https://www.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· ÙØ§Ø±Ø³",          "u":"https://www.farsnews.ir/rss/fa"},
    {"n":"ğŸ‡®ğŸ‡· Ù…Ø´Ø±Ù‚",          "u":"https://www.mashreghnews.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø¯ÙØ§Ø¹ Ù¾Ø±Ø³",      "u":"https://www.defapress.ir/fa/rss"},
    {"n":"ğŸ‡®ğŸ‡· YJC",           "u":"https://www.yjc.ir/fa/rss/allnews"},
    # â”€â”€â”€ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡®ğŸ‡· IRNA EN",       "u":"https://en.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Mehr EN",       "u":"https://en.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tasnim EN",     "u":"https://www.tasnimnews.com/en/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Press TV",      "u":"https://www.presstv.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tehran Times",  "u":"https://www.tehrantimes.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran Intl EN",  "u":"https://www.iranintl.com/en/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran Wire",     "u":"https://iranwire.com/en/feed/"},
    {"n":"ğŸ‡®ğŸ‡· Radio Farda",   "u":"https://en.radiofarda.com/api/zqpqetrruqo"},
    # â”€â”€â”€ Google News ÙØ§Ø±Ø³ÛŒ â€” Ø§Ù…Ø±ÙˆØ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ“° GN Ú˜Ù†Ùˆ Ø§Ù…Ø±ÙˆØ²",   "u":"https://news.google.com/rss/search?q=Ø§ÛŒØ±Ø§Ù†+Ù…Ø°Ø§Ú©Ø±Ø§Øª+Ú˜Ù†Ùˆ+Ø¹Ø±Ø§Ù‚Ú†ÛŒ+ÙˆÛŒØªÚ©ÙˆÙ&hl=fa&gl=IR&ceid=IR:fa&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN Ø³Ù¾Ø§Ù‡ Ø§Ù…Ø±ÙˆØ²",  "u":"https://news.google.com/rss/search?q=Ø³Ù¾Ø§Ù‡+Ù¾Ø§Ø³Ø¯Ø§Ø±Ø§Ù†+Ø­Ù…Ù„Ù‡+Ù…ÙˆØ´Ú©+Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ&hl=fa&gl=IR&ceid=IR:fa&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GN Ø§Ø¹ØªØ±Ø§Ø¶ Ø§ÛŒØ±Ø§Ù†","u":"https://news.google.com/rss/search?q=Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª+Ø§ÛŒØ±Ø§Ù†+Ø³Ø±Ú©ÙˆØ¨+Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ+Û±Û´Û°Û´&hl=fa&gl=IR&ceid=IR:fa&num=10&tbs=qdr:d"},
]

ISRAEL_FEEDS = [
    {"n":"ğŸ‡®ğŸ‡± Times of Israel","u":"https://www.timesofisrael.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± Jerusalem Post", "u":"https://rss.jpost.com/rss/rssfeedsheadlines"},
    {"n":"ğŸ‡®ğŸ‡± Haaretz EN",     "u":"https://www.haaretz.com/srv/haaretz-latest-articles.rss"},
    {"n":"ğŸ‡®ğŸ‡± Israel Hayom",   "u":"https://www.israelhayom.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± i24 News",       "u":"https://www.i24news.tv/en/rss"},
    # â”€â”€â”€ Google News Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ“° GN Netanyahu",   "u":"https://news.google.com/rss/search?q=Netanyahu+Iran+nuclear+deal+war+2026&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN IDF Iran",    "u":"https://news.google.com/rss/search?q=IDF+Israel+Iran+strike+military&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]

USA_FEEDS = [
    # â”€â”€â”€ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡ºğŸ‡¸ AP World",        "u":"https://apnews.com/hub/world-news.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ AP Middle East",  "u":"https://apnews.com/hub/middle-east.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ AP Nuclear",      "u":"https://apnews.com/hub/nuclear-weapons.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ NBC World",       "u":"https://feeds.nbcnews.com/feeds/worldnews"},
    {"n":"ğŸ‡ºğŸ‡¸ PBS NewsHour",    "u":"https://www.pbs.org/newshour/feed"},
    # â”€â”€â”€ Ù†Ø¸Ø§Ù…ÛŒ/Ø¯ÙØ§Ø¹ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡ºğŸ‡¸ USNI News",       "u":"https://news.usni.org/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Breaking Defense","u":"https://breakingdefense.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ The War Zone",    "u":"https://www.twz.com/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Defense News",    "u":"https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"n":"ğŸ‡ºğŸ‡¸ Stars & Stripes", "u":"https://www.stripes.com/rss/arc/outboundfeeds/news/"},
    {"n":"ğŸ‡ºğŸ‡¸ CTP-ISW Iran",    "u":"https://www.criticalthreats.org/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Long War Journal","u":"https://www.longwarjournal.org/feed"},
    # â”€â”€â”€ ØªØ­Ù„ÛŒÙ„/Ø³ÛŒØ§Ø³Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡ºğŸ‡¸ Foreign Policy",  "u":"https://foreignpolicy.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ CFR",             "u":"https://www.cfr.org/rss/feeds/news.xml"},
    {"n":"ğŸ‡ºğŸ‡¸ Axios World",     "u":"https://api.axios.com/feed/"},
    # â”€â”€â”€ Google News â€” Ø¨Ø­Ø±Ø§Ù† Ø§Ù…Ø±ÙˆØ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ“° GN Witkoff Geneva","u":"https://news.google.com/rss/search?q=Witkoff+Kushner+Iran+nuclear+Geneva+talks&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN Trump Iran war","u":"https://news.google.com/rss/search?q=Trump+Iran+military+strike+war+2026&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN USS Lincoln",   "u":"https://news.google.com/rss/search?q=USS+Abraham+Lincoln+carrier+Iran+Persian+Gulf&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GN Vance Iran",    "u":"https://news.google.com/rss/search?q=Vance+Rubio+Hegseth+Iran+military+nuclear&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GN Hormuz",        "u":"https://news.google.com/rss/search?q=Strait+Hormuz+Iran+US+navy+oil&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]

EMBASSY_FEEDS = [
    # ØªØ®Ù„ÛŒÙ‡ Ø¯ÛŒÙ¾Ù„Ù…Ø§Øªâ€ŒÙ‡Ø§ â€” ÙˆØ¶Ø¹ÛŒØª Ø§Ù…Ø±ÙˆØ² Ø­Ø§Ø¯ Ø§Ø³Øª
    {"n":"ğŸ›ï¸ US State Dept",   "u":"https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html.rss"},
    {"n":"ğŸ›ï¸ UK FCDO",         "u":"https://www.gov.uk/foreign-travel-advice/iran.atom"},
    {"n":"ğŸ“° GN Evacuation",   "u":"https://news.google.com/rss/search?q=embassy+evacuation+diplomats+Iran+Lebanon+2026&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]

INTL_FEEDS = [
    {"n":"ğŸŒ BBC Middle East", "u":"https://feeds.bbci.co.uk/news/world/middle_east/rss.xml"},
    {"n":"ğŸŒ Al Jazeera",      "u":"https://www.aljazeera.com/xml/rss/all.xml"},
    {"n":"ğŸŒ Middle East Eye", "u":"https://www.middleeasteye.net/rss"},
    {"n":"ğŸŒ The Guardian ME", "u":"https://www.theguardian.com/world/middleeast/rss"},
    {"n":"ğŸŒ MEI",             "u":"https://www.mei.edu/rss.xml"},
]


ALL_RSS_FEEDS = IRAN_FEEDS + ISRAEL_FEEDS + USA_FEEDS + EMBASSY_FEEDS + INTL_FEEDS
EMBASSY_SET   = {id(f) for f in EMBASSY_FEEDS}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X handles
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_HANDLES = [
    # â”€â”€â”€ OSINT / Breaking â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # âŒ "OSINTdefender" Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨ÙˆØ¯ â€” handle ÙˆØ§Ù‚Ø¹ÛŒ @sentdefender Ø§Ø³Øª
    ("ğŸ” OSINTdefender",        "sentdefender"),
    ("ğŸ” OSINTtechnical",       "Osinttechnical"),
    ("ğŸ” IntelCrab",            "IntelCrab"),
    ("ğŸ” GeoConfirmed",         "GeoConfirmed"),
    ("ğŸ” WarMonitor",           "WarMonitor3"),
    ("ğŸ” AuroraIntel",          "AuroraIntel"),
    ("ğŸ” Faytuks",              "Faytuks"),
    ("ğŸ” Clash Report",         "clashreport"),
    ("ğŸ” Megatron",             "Megatron_Ron"),
    ("ğŸ” ELINT News",           "ELINTNews"),
    ("ğŸ” War Zone TW",          "TheWarZoneTW"),
    # â”€â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¯ÙˆÙ„ØªÛŒ / Ù†Ø¸Ø§Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡ºğŸ‡¸ CENTCOM",              "CENTCOM"),
    ("ğŸ‡ºğŸ‡¸ DoD",                  "DeptofDefense"),
    ("ğŸ‡ºğŸ‡¸ Natasha Bertrand",     "NatashaBertrand"),
    ("ğŸ‡ºğŸ‡¸ Barak Ravid",          "BarakRavid"),
    ("ğŸ‡ºğŸ‡¸ Idrees Ali",           "idreesali114"),
    ("ğŸ‡ºğŸ‡¸ Jack Detsch",          "JackDetsch"),
    ("ğŸ‡ºğŸ‡¸ Lara Seligman",        "laraseligman"),
    ("ğŸ‡ºğŸ‡¸ Jim Sciutto",          "jimsciutto"),
    # â”€â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡± IDF",                  "IDF"),
    ("ğŸ‡®ğŸ‡± Israeli PM",           "IsraeliPM"),
    ("ğŸ‡®ğŸ‡± Yossi Melman",         "yossi_melman"),
    ("ğŸ‡®ğŸ‡± Seth Frantzman",       "sfrantzman"),
    ("ğŸ‡®ğŸ‡± Emanuel Fabian",       "manniefabian"),
    ("ğŸ‡®ğŸ‡± Anna Ahronheim",       "AAhronheim"),
    # â”€â”€â”€ Ø§ÛŒØ±Ø§Ù† / Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡· IranIntl EN",          "IranIntl_En"),
    ("ğŸ‡®ğŸ‡· IRNA EN",              "IRNA_English"),
    ("ğŸ‡®ğŸ‡· Press TV",             "PressTV"),
    ("ğŸ‡®ğŸ‡· Farnaz Fassihi",       "farnazfassihi"),
    ("ğŸ‡®ğŸ‡· Kasra Aarabi",         "KasraAarabi"),
    # â”€â”€â”€ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Brk",       "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ AFP News",              "AFPnews"),
    # â”€â”€â”€ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ” Ian Bremmer",           "ianbremmer"),
    ("ğŸ” Ellie Geranmayeh",      "EllieGeranmayeh"),
    ("ğŸ” Michael Knights",       "Mikeknightsiraq"),
    ("ğŸ” Aric Toler",            "AricToler"),
    ("âš ï¸ DEFCONLevel",           "DEFCONLevel"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TELEGRAM_CHANNELS = [
    # OSINT â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
    ("ğŸ”´ Middle East Spectator", "Middle_East_Spectator"),
    ("ğŸ”´ Intel Slava Z",         "intelslava"),
    ("ğŸ”´ ELINT News",            "ELINTNews"),
    ("ğŸ”´ Clash Report",          "ClashReport"),
    ("ğŸ”´ Megatron OSINT",        "Megatron_Ron"),
    ("ğŸ”´ Disclose TV",           "disclosetv"),
    ("ğŸ” OSINTtechnical",        "Osinttechnical"),
    ("ğŸ” Aurora Intel",          "Aurora_Intel"),
    ("ğŸ” War Monitor",           "WarMonitor3"),
    # Ø§ÛŒØ±Ø§Ù† ÙØ§Ø±Ø³ÛŒ
    # âŒ "IranIntlPersian" Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨ÙˆØ¯ â€” handle ÙˆØ§Ù‚Ø¹ÛŒ @IranintlTV Ø§Ø³Øª (Û± Ù…ÛŒÙ„ÛŒÙˆÙ† Ø¹Ø¶Ùˆ)
    ("ğŸ‡®ğŸ‡· Iran Intl Persian",   "IranintlTV"),
    ("ğŸ‡®ğŸ‡· ØªØ³Ù†ÛŒÙ… ÙØ§Ø±Ø³ÛŒ",          "tasnimnewsfa"),
    ("ğŸ‡®ğŸ‡· Ù…Ù‡Ø± ÙØ§Ø±Ø³ÛŒ",             "mehrnews_fa"),
    ("ğŸ‡®ğŸ‡· Ø§ÛŒØ±Ù†Ø§ ÙØ§Ø±Ø³ÛŒ",           "irnafarsi"),
    ("ğŸ‡®ğŸ‡· Press TV",              "PressTVnews"),
    # Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    ("ğŸ‡®ğŸ‡± Kann News",            "kann_news"),
    ("ğŸ‡®ğŸ‡± Times of Israel",      "timesofisrael"),
    # Ù…Ù†Ø·Ù‚Ù‡
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Breaking",  "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸ‡¾ğŸ‡² Masirah TV",           "AlMasirahNet"),
    ("ğŸ‡±ğŸ‡§ Naharnet",             "Naharnet"),
    # Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ GeoConfirmed",          "GeoConfirmed"),
    ("ğŸŒ IntelCrab",             "IntelCrab"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Û²Û· ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶ â€” Ø´Ø®ØµÛŒØªâ€ŒÙ‡Ø§ Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø¬Ø§Ø±ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€ Ø§ÛŒØ±Ø§Ù† â€” Ø±Ù‡Ø¨Ø±ÛŒ + Ù†Ø¸Ø§Ù…ÛŒ + Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
IRAN_KW = [
    # Ø§Ø³Ø§Ù…ÛŒ â€” Ù…Ù‚Ø§Ù…Ø§Øª ÙØ¹Ù„ÛŒ Û²Û°Û²Û¶
    "khamenei","pezeshkian","araghchi","abbas araghchi",
    "ali shamkhani","shamkhani",          # Ø¯Ø¨ÛŒØ± Ø´ÙˆØ±Ø§ÛŒ Ø¹Ø§Ù„ÛŒ Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ
    "ali larijani","larijani",             # Ø±Ø¦ÛŒØ³ SNSC
    "esmail baghaei","baghaei",            # Ø³Ø®Ù†Ú¯ÙˆÛŒ ÙˆØ²Ø§Ø±Øª Ø®Ø§Ø±Ø¬Ù‡
    "hossein salami","salami",             # ÙØ±Ù…Ø§Ù†Ø¯Ù‡ Ø³Ù¾Ø§Ù‡
    "mohammad bagheri","bagheri",          # Ø±Ø¦ÛŒØ³ Ø³ØªØ§Ø¯ Ú©Ù„
    "ali fadavi","fadavi",                 # ÙØ±Ù…Ø§Ù†Ø¯Ù‡ Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ Ø³Ù¾Ø§Ù‡
    # Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§
    "irgc","sepah","basij","quds force","islamic republic",
    "iran","iranian","tehran",
    # Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Û²Û°Û²Û¶ (Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ù…Ù„Ø§Øª Ú˜ÙˆØ¦Ù† Û²Û°Û²Ûµ)
    "natanz","fordow","isfahan","arak",    # ØªØ£Ø³ÛŒØ³Ø§Øª Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ
    "iran nuclear","uranium enrichment","centrifuge",
    "60 percent","90 percent","weapons grade",
    "reconstitute","rebuild nuclear",      # Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ
    "planetary mixer",                     # ØªØ¬Ù‡ÛŒØ²Ø§Øª Ù…ÙˆØ´Ú©ÛŒ Ú©Ø´Ùâ€ŒØ´Ø¯Ù‡
    # Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ Û²Û°Û²Û¶
    "geneva talks","oman talks","vienna talks","nuclear deal",
    "witkoff iran","kushner iran","araghchi witkoff",
    "iran sanctions relief","iran deal",
    # Ø¬ØºØ±Ø§ÙÛŒØ§
    "persian gulf","strait of hormuz","hormuz closure",
    "iran naval","iris","bandar abbas",    # Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ Ø§ÛŒØ±Ø§Ù†
    # Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Û²Û°Û²Ûµ-Û²Û°Û²Û¶
    "iran protests","iranian protests","iran crackdown",
    "iran unrest","iran uprising","iran demonstrations",
    "twelve-day war","iran-israel war",    # Ø¬Ù†Ú¯ Û±Û² Ø±ÙˆØ²Ù‡ Ú˜ÙˆØ¦Ù† Û²Û°Û²Ûµ
    # ÙØ§Ø±Ø³ÛŒ
    "Ø§ÛŒØ±Ø§Ù†","Ø³Ù¾Ø§Ù‡","Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ","ØªÙ‡Ø±Ø§Ù†","Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ",
    "Ù¾Ø²Ø´Ú©ÛŒØ§Ù†","Ø¹Ø±Ø§Ù‚Ú†ÛŒ","Ø´Ù…Ø®Ø§Ù†ÛŒ","Ù„Ø§Ø±ÛŒØ¬Ø§Ù†ÛŒ","Ø¨Ø§Ù‚Ø±ÛŒ",
    "Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ø§ØµÙÙ‡Ø§Ù†","ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²","Ø®Ù„ÛŒØ¬ ÙØ§Ø±Ø³",
    "ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ","Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…","ØªÙˆØ§ÙÙ‚ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ù…Ø°Ø§Ú©Ø±Ø§Øª Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ",
    "Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø§ÛŒØ±Ø§Ù†","Ø³Ø±Ú©ÙˆØ¨","Ø¬Ù†Ú¯ Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ø±ÙˆØ²Ù‡",
    "Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ÙˆØ´Ú©ÛŒ Ø§ÛŒØ±Ø§Ù†","Ù…ÙˆØ´Ú© Ø¨Ø§Ù„Ø³ØªÛŒÚ© Ø§ÛŒØ±Ø§Ù†",
]

# â”€â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ â€” ØªÛŒÙ… ØªØ±Ø§Ù…Ù¾ Û²Û°Û²Û¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USA_KW = [
    # Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± + ØªÛŒÙ… Ø§ØµÙ„ÛŒ
    "trump","donald trump","white house",
    "jd vance","vance",                    # Ù…Ø¹Ø§ÙˆÙ† Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ±
    "marco rubio","rubio",                 # ÙˆØ²ÛŒØ± Ø®Ø§Ø±Ø¬Ù‡
    "pete hegseth","hegseth",              # ÙˆØ²ÛŒØ± Ø¯ÙØ§Ø¹
    "scott bessent","bessent",             # ÙˆØ²ÛŒØ± Ø®Ø²Ø§Ù†Ù‡â€ŒØ¯Ø§Ø±ÛŒ
    "tulsi gabbard","gabbard",             # Ø±Ø¦ÛŒØ³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù„ÛŒ
    # Ù…Ø°Ø§Ú©Ø±Ù‡â€ŒÚ©Ù†Ù†Ø¯Ú¯Ø§Ù† Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Û²Û°Û²Û¶
    "steve witkoff","witkoff",             # Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ ÙˆÛŒÚ˜Ù‡ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡
    "jared kushner","kushner",             # Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ ÙˆÛŒÚ˜Ù‡
    "brad cooper","cooper",                # ÙØ±Ù…Ø§Ù†Ø¯Ù‡ CENTCOM (Ø¯Ø± Ù…Ø°Ø§Ú©Ø±Ø§Øª Ø¹Ù…Ø§Ù†)
    "mike huckabee","huckabee",            # Ø³ÙÛŒØ± Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¯Ø± Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    # Ù†Ø¸Ø§Ù…ÛŒ
    "pentagon","centcom","us military","us navy",
    "us air force","us army","us forces","us troops",
    "carrier strike group","aircraft carrier",
    "uss abraham lincoln","lincoln carrier",
    "uss gerald r ford","ford carrier",    # Ù†Ø§Ùˆ Ø¯ÙˆÙ… Ú©Ù‡ ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶ Ø§Ø¹Ø²Ø§Ù… Ø´Ø¯
    "b-52","b-2","f-35","bunker buster",   # Ø³Ù„Ø§Ø­â€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø­Ù…Ù„Ù‡ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†
    "gbu-57","massive ordnance penetrator","mop",
    "al udeid","al-udeid",                 # Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ù‚Ø·Ø± Ú©Ù‡ Ù…ÙˆØ´Ú©â€ŒÙ‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯
    # Ø³ÛŒØ§Ø³ÛŒ
    "united states","u.s.","state department","cia",
    "iran sanctions","maximum pressure","us tariff iran",
    "war authorization","aumf","congress iran",
    "state of the union","sotu iran",      # Ø³Ø®Ù†Ø±Ø§Ù†ÛŒ ØªØ±Ø§Ù…Ù¾ Û²Ûµ ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶
    # ÙØ§Ø±Ø³ÛŒ
    "Ø¢Ù…Ø±ÛŒÚ©Ø§","ØªØ±Ø§Ù…Ù¾","Ù¾Ù†ØªØ§Ú¯ÙˆÙ†","Ú©Ø§Ø® Ø³ÙÛŒØ¯",
    "Ù†Ø§Ùˆ Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ø¨Ø±","Ù†Ø§Ùˆ Ø¢Ø¨Ø±Ø§Ù‡Ø§Ù… Ù„ÛŒÙ†Ú©Ù„Ù†","Ù†Ø§Ùˆ Ø¬Ø±Ø§Ù„Ø¯ ÙÙˆØ±Ø¯",
    "ÙˆÛŒØªÚ©ÙˆÙ","Ú©ÙˆØ´Ù†Ø±","Ø±ÙˆØ¨ÛŒÙˆ","Ù‡Ú¯Ø³Øª","Ø¨Ø³Ù†Øª","Ú¯Ø¨Ø§Ø±Ø¯","ÙˆÙ†Ø³",
    "ØªØ­Ø±ÛŒÙ…","ÙØ´Ø§Ø± Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ","Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø§Ù„Ø¹Ø¯ÛŒØ¯",
]

# â”€â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â€” Ø±Ù‡Ø¨Ø±ÛŒ + Ù†Ø¸Ø§Ù…ÛŒ Û²Û°Û²Û¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ISRAEL_KW = [
    # Ø±Ù‡Ø¨Ø±ÛŒ Û²Û°Û²Û¶
    "netanyahu","benjamin netanyahu",
    "eyal zamir","yoav gallant",           # ÙˆØ²Ø±Ø§ÛŒ Ø¯ÙØ§Ø¹
    "bezalel smotrich","smotrich",         # ÙˆØ²ÛŒØ± Ù…Ø§Ù„ÛŒ Ø§Ø¦ØªÙ„Ø§Ù Ø±Ø§Ø³Øª Ø§ÙØ±Ø§Ø·ÛŒ
    "itamar ben gvir","ben gvir",          # ÙˆØ²ÛŒØ± Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ
    "israel katz","katz",                  # ÙˆØ²ÛŒØ± Ø®Ø§Ø±Ø¬Ù‡
    # Ù†Ø¸Ø§Ù…ÛŒ
    "idf","mossad","shin bet","aman",
    "israel","israeli","iaf","israeli air force",
    "iron dome","arrow missile","david's sling",
    "tel aviv","jerusalem",
    "israel iran war","june 2025 strikes",  # Ø¬Ù†Ú¯ Ú˜ÙˆØ¦Ù† Û²Û°Û²Ûµ
    "israeli strike iran","iran strike israel",
    # ÙØ§Ø±Ø³ÛŒ
    "Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ","Ù…ÙˆØ³Ø§Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†","Ù…ÙˆØ´Ú© Ø§ÛŒØ±Ø§Ù†",
    "ØªÙ„â€ŒØ¢ÙˆÛŒÙˆ","Ø§ÙˆØ±Ø´Ù„ÛŒÙ…","Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ù†ÛŒØ±ÙˆÛŒ Ù‡ÙˆØ§ÛŒÛŒ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„",
    "Ø§Ø³Ù…ÙˆØªØ±ÛŒÚ†","Ø¨Ù†â€ŒÚ¯ÙˆÛŒØ±",
]

# â”€â”€â”€ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ / Ù¾Ø±ÙˆÚ©Ø³ÛŒ / Ù…ÛŒØ§Ù†Ø¬ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROXY_KW = [
    # Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† (Ù…Ø­ÙˆØ± Ù…Ù‚Ø§ÙˆÙ…Øª â€” ØªØ¶Ø¹ÛŒÙâ€ŒØ´Ø¯Ù‡ Ø§Ù…Ø§ ÙØ¹Ø§Ù„)
    "hamas","hezbollah","houthi","ansar allah",
    "pij","islamic jihad","kataib hezbollah",
    # Ù…ÛŒØ§Ù†Ø¬ÛŒØ§Ù† Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Û²Û°Û²Û¶
    "oman","badr al-busaidi","al-busaidi",  # ÙˆØ²ÛŒØ± Ø®Ø§Ø±Ø¬Ù‡ Ø¹Ù…Ø§Ù† â€” Ù…ÛŒØ§Ù†Ø¬ÛŒ
    "rafael grossi","grossi","iaea",        # Ù…Ø¯ÛŒØ± Ø¢Ú˜Ø§Ù†Ø³ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ø§Ù†Ø±Ú˜ÛŒ Ø§ØªÙ…ÛŒ
    "turkey mediation","erdogan iran",
    "qatar mediation","qatar iran",
    # ÙØ§Ø±Ø³ÛŒ
    "Ø­Ù…Ø§Ø³","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡","Ø­ÙˆØ«ÛŒ","Ø§Ù†ØµØ§Ø±Ø§Ù„Ù„Ù‡","Ø¬Ù‡Ø§Ø¯ Ø§Ø³Ù„Ø§Ù…ÛŒ",
    "Ø¹Ù…Ø§Ù†","Ú¯Ø±ÙˆØ³ÛŒ","Ø¢Ú˜Ø§Ù†Ø³ Ø§ØªÙ…ÛŒ","Ù…ÛŒØ§Ù†Ø¬ÛŒÚ¯Ø±ÛŒ",
]

# â”€â”€â”€ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¬Ù†Ú¯/Ø¨Ø­Ø±Ø§Ù† Û²Û°Û²Û¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WAR_CONTEXT_KW = [
    # Ø¨Ø­Ø±Ø§Ù† Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ
    "nuclear weapon","nuclear strike","nuclear deal","nuclear talks",
    "uranium enrichment","weapons grade","iaea inspection",
    "nuclear breakout","nuclear threshold",
    "fordow destroy","natanz destroy","isfahan bomb",
    # Ø­Ù…Ù„Ù‡ Ù†Ø¸Ø§Ù…ÛŒ
    "military strike","airstrike","attack iran","strike iran",
    "bomb iran","regime change","decapitation strike",
    "us strike","israel strike",
    # Ù†Ø§ÙˆÚ¯Ø§Ù† Ø¢Ù…Ø±ÛŒÚ©Ø§
    "carrier strike group","persian gulf fleet","arabian sea",
    "military buildup","war preparations",
    "last chance","final warning","war clock",
    # ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø² Û²Û°Û²Û¶
    "hormuz closure","strait blocked","oil tanker iran",
    "fast attack boat","iranian drone","iranian naval",
    # ØªØ­Ø±ÛŒÙ…â€ŒÙ‡Ø§
    "iran oil sanctions","25 percent tariff china iran",
    "china iran oil","iran oil exports",
    # Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª + Ú©ÙˆØ¯ØªØ§
    "iran uprising","iran revolution","regime collapse",
    "iran protests killed","iran crackdown 2026",
    # Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶
    "geneva round","fourth round talks","vienna iaea",
    "technical teams iran","nuclear framework",
    # ÙØ§Ø±Ø³ÛŒ
    "Ø­Ù…Ù„Ù‡ Ù†Ø¸Ø§Ù…ÛŒ","Ø¶Ø±Ø¨Ù‡ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","ØªØºÛŒÛŒØ± Ø±Ú˜ÛŒÙ…",
    "Ø¬Ù†Ú¯ Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ø±ÙˆØ²Ù‡","Ù…Ø°Ø§Ú©Ø±Ø§Øª Ú˜Ù†Ùˆ","Ù…Ø°Ø§Ú©Ø±Ø§Øª ÙˆÛŒÙ†",
    "ØªÙ‡Ø¯ÛŒØ¯ Ø¨Ù‡ Ø¬Ù†Ú¯","Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ¨Ø§Ø´ Ù†Ø¸Ø§Ù…ÛŒ","Ø¨Ø³ØªÙ‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ",
    "Ú¯ÙØªÚ¯ÙˆÛŒ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","ÙØ´Ø§Ø± Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ","ØªØ­Ø±ÛŒÙ… Ù†ÙØª Ø§ÛŒØ±Ø§Ù†",
]

# â”€â”€â”€ Ø­Ø°Ù Ù‚Ø·Ø¹ÛŒ (Ú©Ø§Ù…Ù„Ø§Ù‹ ØºÛŒØ±Ù…Ø±ØªØ¨Ø·) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HARD_EXCLUDE = [
    "football","soccer","basketball","nba","nfl","world cup","championship",
    "olympic","marathon","tennis","golf","cricket","baseball","rugby",
    "celebrity","entertainment","movie","film","music award","concert",
    "box office","grammy","oscar","emmy","fashion","cooking","recipe","travel guide",
    "ÙÙˆØªØ¨Ø§Ù„","Ø³ÛŒÙ†Ù…Ø§","Ù…ÙˆØ³ÛŒÙ‚ÛŒ","Ø¢Ø´Ù¾Ø²ÛŒ","Ù…Ø¯","Ø¨Ø§Ø²ÛŒ","Ø³Ø±ÛŒØ§Ù„","ØªÙˆØ±ÛŒØ³Øª","Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ",
    "stock market","crypto","bitcoin","forex",
    "Ø¨ÙˆØ±Ø³","Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„","Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ†",
    "climate change","global warming","weather","earthquake","flood",  # Ø¨Ù„Ø§ÛŒØ§ Ø·Ø¨ÛŒØ¹ÛŒ
    "Ø¢Ø¨â€ŒÙˆÙ‡ÙˆØ§","Ø²Ù„Ø²Ù„Ù‡","Ø³ÛŒÙ„",
]

EMBASSY_OVERRIDE = [
    "evacuate","leave immediately","travel warning","security alert","emergency",
    "warden message","embassy closed","consulate closed",
    "ØªØ®Ù„ÛŒÙ‡","ÙÙˆØ±ÛŒ ØªØ±Ú©","Ù‡Ø´Ø¯Ø§Ø± Ø§Ù…Ù†ÛŒØªÛŒ","Ø§Ø¶Ø·Ø±Ø§Ø±","Ù‡Ø´Ø¯Ø§Ø± Ø³ÙØ§Ø±Øª",
]

def is_war_relevant(text: str, is_embassy=False, is_tg=False, is_tw=False) -> bool:
    """
    ÙÛŒÙ„ØªØ± Û²Û°Û²Û¶ â€” ÙÙ‚Ø· Ø¬Ù†Ú¯ Ùˆ ØªÙ†Ø´ Ø§ÛŒØ±Ø§Ù†/Ø¢Ù…Ø±ÛŒÚ©Ø§/Ø§Ø³Ø±Ø§ÛŒÛŒÙ„

    Ù…Ù†Ø·Ù‚:
    Û±. Ø­Ø°Ù Ù‚Ø·Ø¹ÛŒ (ÙˆØ±Ø²Ø´/Ø³Ø±Ú¯Ø±Ù…ÛŒ)
    Û². Ø³ÙØ§Ø±Øª + Ù‡Ø´Ø¯Ø§Ø± â†’ pass
    Û³. Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø·Ø±Ù Ø§ØµÙ„ÛŒ (Ø§ÛŒØ±Ø§Ù†/Ø¢Ù…Ø±ÛŒÚ©Ø§/Ø§Ø³Ø±Ø§ÛŒÛŒÙ„) â†’ pass
    Û´. Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù†Ú¯ Ø¨Ø¯ÙˆÙ† Ú©Ø´ÙˆØ± Ù…Ø´Ø®Øµ â†’ pass (Ù…Ø«Ù„Ø§Ù‹ "nuclear talks" Ø¨Ø¯ÙˆÙ† Ø°Ú©Ø± Ø§ÛŒØ±Ø§Ù†)
    """
    txt = text.lower()

    # Û±. Ø­Ø°Ù Ù‚Ø·Ø¹ÛŒ
    if any(k in txt for k in HARD_EXCLUDE):
        return False

    # Û². Ø³ÙØ§Ø±Øª
    if is_embassy and any(k in txt for k in EMBASSY_OVERRIDE):
        return True

    # Û³. Ø­Ø¶ÙˆØ± Ù‡Ø± Ø·Ø±Ù Ø§ØµÙ„ÛŒ
    if any(k in txt for k in IRAN_KW):   return True
    if any(k in txt for k in USA_KW):    return True
    if any(k in txt for k in ISRAEL_KW): return True
    if any(k in txt for k in PROXY_KW):  return True

    # Û´. Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¬Ù†Ú¯ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ Ø­ØªÛŒ Ø¨Ø¯ÙˆÙ† Ø°Ú©Ø± ØµØ±ÛŒØ­ Ú©Ø´ÙˆØ±
    if any(k in txt for k in WAR_CONTEXT_KW): return True

    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X â€” Feb 2026 â€” ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª Ø§Ø² Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† uptime Ø¯Ø± GitHub Actions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…: Ø§Ú©Ø«Ø± Nitter instances Ø¯Ø± GitHub Actions IPs Ø¨Ù„Ø§Ú© Ù‡Ø³ØªÙ†Ø¯
# RSSHub Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¹ØªÙ…Ø§Ø¯ØªØ± Ø§Ø³Øª â€” Ø§Ø² Ø¢Ù† Ø§Ø¨ØªØ¯Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
RSSHUB_INSTANCES = [
    "https://rsshub.app",               # âœ… Ø§ØµÙ„ÛŒ â€” Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ±ÛŒÙ†
    "https://rsshub.rss.now.sh",       # âœ… mirror
    "https://rss.shab.fun",            # backup
    "https://rsshub.moeyy.xyz",        # backup
    "https://hub.slar.ru",             # backup
]
NITTER_INSTANCES = [
    "https://rss.xcancel.com",         # âœ… subdomain Ù…Ø³ØªÙ‚ÛŒÙ…
    "https://xcancel.com",             # âœ… redirect
    "https://nitter.poast.org",        # âœ… Ø§ØºÙ„Ø¨ Ø¯Ø± CI Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    "https://nitter.privacyredirect.com",
    "https://nitter.tiekoetter.com",
    "https://lightbrd.com",
    "https://nitter.catsarch.com",
    "https://n.ramle.be",
    "https://nitter.space",
    "https://nitter.net",
    "https://nitter.it",
    "https://nitter.unixfox.eu",
]

NITTER_HDR = {
    "User-Agent": "Mozilla/5.0 (compatible; Feedfetcher-Google; +http://www.google.com/feedfetcher.html)",
    "Accept": "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Cache-Control": "no-cache",
}
COMMON_UA = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

_nitter_pool: list[str]  = []
_rsshub_pool: list[str]  = []
_TW_SEMA: asyncio.Semaphore | None = None

def _load_nitter_cache() -> tuple[list, list, float]:
    try:
        if Path(NITTER_CACHE_FILE).exists():
            d = json.load(open(NITTER_CACHE_FILE))
            return d.get("nitter", []), d.get("rsshub", []), d.get("ts", 0.0)
    except: pass
    return [], [], 0.0

def _save_nitter_cache(nitter, rsshub):
    json.dump({"nitter": nitter, "rsshub": rsshub,
               "ts": datetime.now(timezone.utc).timestamp()},
              open(NITTER_CACHE_FILE, "w"))

def _is_rss(body: str, ct: str) -> bool:
    b = body[:600].lower()
    return ("xml" in ct) or ("<rss" in b) or ("<?xml" in b) or ("<feed" in b)

async def _try_rss(client: httpx.AsyncClient, url: str, timeout: float = TW_TIMEOUT) -> list:
    """
    RSS URL Ø±Ø§ fetch Ú©Ø±Ø¯Ù‡ entries Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    follow_redirects=True Ù…Ù‡Ù… Ø§Ø³Øª (xcancel.com â†’ rss.xcancel.com)
    """
    try:
        r = await client.get(url,
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=timeout,
                                                   write=5.0, pool=5.0))
        if r.status_code not in (200, 304):
            return []
        ct = r.headers.get("content-type", "")
        body = r.text or ""
        if not _is_rss(body, ct):
            return []
        parsed = feedparser.parse(body)
        entries = getattr(parsed, "entries", []) or []
        return [e for e in entries if len((e.get("title") or "").strip()) > 3]
    except Exception:
        return []

async def _probe_instance(client: httpx.AsyncClient, url: str,
                          handle: str = "OSINTdefender") -> tuple | None:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ÛŒÚ© instance ÙˆØ§Ù‚Ø¹Ø§Ù‹ RSS Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ù…Ù‡Ù…: ÙÙ‚Ø· Ø³Ø§Ø®ØªØ§Ø± RSS Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù†Ù‡ ØªØ¹Ø¯Ø§Ø¯ entries.
    """
    t0 = asyncio.get_running_loop().time()
    try:
        r = await client.get(f"{url}/{handle}/rss",
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=7.0,
                                                   write=5.0, pool=5.0))
        if r.status_code not in (200, 304):
            return None
        ct   = r.headers.get("content-type", "")
        body = r.text or ""
        # ÙÙ‚Ø· Ú†Ú© Ø³Ø§Ø®ØªØ§Ø± â€” Ù†Ù‡ entries
        if _is_rss(body, ct):
            ms = (asyncio.get_running_loop().time() - t0) * 1000
            return url, ms
    except Exception:
        pass
    return None

async def _probe_rsshub(client: httpx.AsyncClient, inst: str) -> tuple | None:
    t0 = asyncio.get_running_loop().time()
    try:
        r = await client.get(f"{inst}/twitter/user/OSINTdefender",
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=8.0,
                                                   write=5.0, pool=5.0))
        if r.status_code in (200, 304) and _is_rss(r.text or "", r.headers.get("content-type","")):
            ms = (asyncio.get_running_loop().time() - t0) * 1000
            return inst, ms
    except Exception:
        pass
    return None

async def build_twitter_pools(client: httpx.AsyncClient):
    """
    Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡: probe Ø­Ø°Ù Ø´Ø¯.
    Ù‡Ù…Ù‡ fetch_twitter Ù…Ø³ØªÙ‚ÛŒÙ… RSSHub â†’ Nitter Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.
    ÙÙ‚Ø· cache Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… Ú©Ù‡ Ø¢Ø®Ø±ÛŒÙ† instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø¨ÛŒØ§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
    """
    global _nitter_pool, _rsshub_pool
    cached_n, cached_r, ts = _load_nitter_cache()
    age = datetime.now(timezone.utc).timestamp() - ts
    # Ø§Ú¯Ù‡ cache Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª: Ø¢Ø®Ø±ÛŒÙ† instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø§ÙˆÙ„ Ø¨Ú¯Ø°Ø§Ø±
    if age < NITTER_CACHE_TTL:
        if cached_r: _rsshub_pool = cached_r + [i for i in RSSHUB_INSTANCES if i not in cached_r]
        if cached_n: _nitter_pool = cached_n + [i for i in NITTER_INSTANCES if i not in cached_n]
    if not _rsshub_pool: _rsshub_pool = list(RSSHUB_INSTANCES)
    if not _nitter_pool: _nitter_pool = list(NITTER_INSTANCES)
    log.info(f"ğ• pools: RSSHub={len(_rsshub_pool)} Nitter={len(_nitter_pool)}")

async def fetch_twitter(client: httpx.AsyncClient, label: str, handle: str) -> list:
    """
    Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§:
    1. RSSHub (Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ø¯Ø± GitHub Actions CI)
    2. Nitter instances
    Ø§ÙˆÙ„ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆÙÙ‚ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¯ÙØ¹Ù‡ Ø¨Ø¹Ø¯ Ø§ÙˆÙ„ Ø§Ù…ØªØ­Ø§Ù† Ø´ÙˆØ¯.
    """
    sema = _TW_SEMA or asyncio.Semaphore(15)
    async with sema:
        # â”€â”€ RSSHub Ø§ÙˆÙ„ (Ø¯Ø± CI Ø¨Ù‡ØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for inst in (_rsshub_pool or RSSHUB_INSTANCES):
            for path in (f"/twitter/user/{handle}", f"/x/user/{handle}"):
                e = await _try_rss(client, f"{inst}{path}", timeout=8.0)
                if e:
                    log.debug(f"ğ• {handle} â† RSSHub {inst.split('//')[-1]} ({len(e)})")
                    # Ø§ÛŒÙ† instance Ø±Ø§ Ø¨Ù‡ Ø§ÙˆÙ„ cache Ø¨ÙØ±Ø³Øª
                    _update_pool_cache(inst, is_rsshub=True)
                    return [(x, f"ğ• {label}", "tw", False) for x in e]

        # â”€â”€ Nitter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for inst in (_nitter_pool or NITTER_INSTANCES):
            e = await _try_rss(client, f"{inst}/{handle}/rss", timeout=6.0)
            if e:
                log.debug(f"ğ• {handle} â† Nitter {inst.split('//')[-1]} ({len(e)})")
                _update_pool_cache(inst, is_rsshub=False)
                return [(x, f"ğ• {label}", "tw", False) for x in e]

    log.debug(f"ğ• {handle}: Ù‡Ù…Ù‡ fail")
    return []

def _update_pool_cache(working_inst: str, is_rsshub: bool):
    """instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø¨Ù‡ Ø§ÙˆÙ„ Ù„ÛŒØ³Øª cache Ù…ÛŒâ€ŒØ¨Ø±Ø¯"""
    global _nitter_pool, _rsshub_pool
    if is_rsshub:
        pool = [working_inst] + [i for i in _rsshub_pool if i != working_inst]
        _rsshub_pool = pool
        json.dump({"nitter": _nitter_pool, "rsshub": pool,
                   "ts": datetime.now(timezone.utc).timestamp()},
                  open(NITTER_CACHE_FILE, "w"))
    else:
        pool = [working_inst] + [i for i in _nitter_pool if i != working_inst]
        _nitter_pool = pool
        json.dump({"nitter": pool, "rsshub": _rsshub_pool,
                   "ts": datetime.now(timezone.utc).timestamp()},
                  open(NITTER_CACHE_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADS-B
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ADSB_API     = "https://api.adsb.one/v2"
ADSB_REGIONS = [
    ("Ø§ÛŒØ±Ø§Ù†",          32.4, 53.7, 250),
    ("Ø®Ù„ÛŒØ¬â€ŒÙØ§Ø±Ø³",     26.5, 52.0, 250),
    ("Ø§Ø³Ø±Ø§ÛŒÛŒÙ„/Ù„Ø¨Ù†Ø§Ù†", 32.1, 35.2, 200),
    ("Ø¹Ø±Ø§Ù‚",           33.3, 44.4, 250),
]
# ÙÙ‚Ø· Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ø¬Ù†Ú¯ÛŒ Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ â€” Ø¨Ø¯ÙˆÙ† ØªØ±Ø§Ø¨Ø±ÛŒ (C17, KC135, C130, ...)
_COMBAT_TYPES   = {"F15","F16","F22","F35","F18","F14","SU35","SU30","MIG29",
                   "B52","B2","B1",        # Ø¨Ù…Ø¨â€ŒØ§ÙÚ©Ù†â€ŒÙ‡Ø§
                   "E3","E8","E767","E737", # Ù‡Ø´Ø¯Ø§Ø± Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… (AWACS)
                   "RC135","EP3","P8",      # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©
                   "U2","SR71","RQ4",       # Ù¾Ù‡Ù¾Ø§Ø¯/Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ø±ØªÙØ§Ø¹ Ø¨Ø§Ù„Ø§
                   "MQ9","MQ1","TB2","HESA",# Ù¾Ù‡Ù¾Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø³Ù„Ø­
                   "A10","AV8","AC130",     # Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ø²Ø¯ÛŒÚ©
                   "EA18","EA6",            # Ø¬Ù†Ú¯ Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©
                   }
_COMBAT_CALLSIGN = ["DOOM","BONE","BUCK","CIAO","JAKE","TORC","GRIM","HAVOC",
                    "GHOST","VIPER","EAGLE","RAPTOR","DEMON","REAPER","PREDATOR"]
_ADSB_SEEN    = set()

async def fetch_military_flights(client: httpx.AsyncClient) -> tuple[list, list]:
    """
    Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯: (msgs, aircraft_list)
    aircraft_list: [{"callsign","type","lat","lon","alt","gs","region"}, ...]
    """
    global _ADSB_SEEN
    msgs     = []
    aircraft = []
    try:
        try:
            if Path(FLIGHT_ALERT_FILE).exists():
                _ADSB_SEEN = set(json.load(open(FLIGHT_ALERT_FILE)).get("seen", []))
        except: pass

        for region, r_lat, r_lon, radius in ADSB_REGIONS:
            try:
                r = await client.get(f"{ADSB_API}/point/{r_lat}/{r_lon}/{radius}",
                                     timeout=httpx.Timeout(7.0),
                                     headers={"Accept": "application/json"})
                if r.status_code != 200: continue
                for ac in (r.json().get("ac") or []):
                    hex_id   = (ac.get("hex") or ac.get("icao","")).upper()
                    callsign = (ac.get("flight") or ac.get("callsign","")).strip()
                    cat      = (ac.get("category") or "").upper()
                    atype    = (ac.get("t") or ac.get("type","")).upper()
                    ac_lat   = ac.get("lat") or ac.get("latitude")
                    ac_lon   = ac.get("lon") or ac.get("longitude")
                    is_combat = (
                        any(atype.startswith(m) for m in _COMBAT_TYPES)
                        or any(callsign.startswith(p) for p in _COMBAT_CALLSIGN)
                        or cat in ("A5", "A6", "A7")  # ICAO military/UAV categories
                    )
                    if not is_combat: continue
                    uid = f"{hex_id}_{callsign}"
                    if uid in _ADSB_SEEN: continue
                    _ADSB_SEEN.add(uid)
                    alt = ac.get("alt_baro") or ac.get("alt", 0)
                    gs  = ac.get("gs") or ac.get("speed", 0)
                    msgs.append(
                        f"âœˆï¸ <b>ØªØ­Ø±Ú© Ù†Ø¸Ø§Ù…ÛŒ â€” {region}</b>\n"
                        f"Ù†ÙˆØ¹: <code>{atype or '?'}</code>  Ú©Ø§Ù„â€ŒØ³Ø§ÛŒÙ†: <code>{callsign or hex_id}</code>\n"
                        f"Ø§Ø±ØªÙØ§Ø¹: {alt:,} ft  Ø³Ø±Ø¹Øª: {gs} kt"
                    )
                    if ac_lat and ac_lon:
                        aircraft.append({
                            "callsign": callsign or hex_id,
                            "type":     atype or "?",
                            "lat":      float(ac_lat),
                            "lon":      float(ac_lon),
                            "alt":      alt,
                            "gs":       gs,
                            "region":   region,
                        })
            except Exception as e:
                log.debug(f"ADS-B {region}: {e}")

        json.dump({"seen": list(_ADSB_SEEN)[-300:]}, open(FLIGHT_ALERT_FILE, "w"))
    except Exception as e:
        log.warning(f"ADS-B: {e}")
    return msgs, aircraft


def make_flight_map(aircraft: list) -> "io.BytesIO | None":
    """
    Ù†Ù‚Ø´Ù‡ Ø¯Ù‚ÛŒÙ‚ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ Ø¨Ø§ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ
    Ù…Ø±Ø²Ù‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ + Ø´Ø¨Ú©Ù‡ Ù…Ø®ØªØµØ§Øª + Ø¨Ø±Ú†Ø³Ø¨
    """
    if not PIL_OK or not aircraft:
        return None
    try:
        W, H    = 1200, 800
        PAD_L   = 50    # ÙØ¶Ø§ÛŒ Ø³Ù…Øª Ú†Ù¾ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø¬Ø§Øª
        PAD_B   = 30    # ÙØ¶Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ†
        PAD_T   = 50    # Ù‡Ø¯Ø±
        MAP_W   = W - PAD_L
        MAP_H   = H - PAD_T - PAD_B

        # Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ â€” Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ Ú©Ø§Ù…Ù„
        LAT_MIN, LAT_MAX =  16.0, 43.0
        LON_MIN, LON_MAX =  26.0, 65.0

        def gp(lat, lon):
            """geo to pixel"""
            x = PAD_L + int((lon - LON_MIN) / (LON_MAX - LON_MIN) * MAP_W)
            y = PAD_T + int((LAT_MAX - lat) / (LAT_MAX - LAT_MIN) * MAP_H)
            return max(0, min(W-1, x)), max(0, min(H-1, y))

        # â”€â”€ Ø±Ù†Ú¯â€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        C_OCEAN  = (8,  28,  52)
        C_LAND   = (32, 45,  55)
        C_LAND2  = (38, 52,  62)   # Ø±Ù†Ú¯ Ù…ØªÙØ§ÙˆØª Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§ÛŒØ²
        C_BORDER = (80, 110, 140)
        C_GRID   = (22, 35,  48)
        C_GRID_L = (40, 58,  72)
        C_PLANE  = (255, 70,  50)
        C_PLANE2 = (255, 180, 50)   # Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒ Ø¯ÙˆÙ…
        C_LABEL  = (210, 230, 250)
        C_DIM    = (100, 130, 155)
        C_ACCENT = (255, 160, 30)
        C_HEAD   = (12,  18,  28)

        img = Image.new("RGB", (W, H), C_OCEAN)
        drw = ImageDraw.Draw(img)

        # â”€â”€ ÙÙˆÙ†Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            F14 = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 14)
            F12 = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
            F11 = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 11)
            FB  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 15)
            FBL = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 18)
        except:
            F14 = F12 = F11 = FB = FBL = ImageFont.load_default()

        # â”€â”€ Ø´Ø¨Ú©Ù‡ Ù…Ø®ØªØµØ§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for lat in range(17, 44, 2):
            y = gp(lat, LON_MIN)[1]
            drw.line([(PAD_L, y), (W, y)], fill=C_GRID, width=1)
            drw.text((2, y - 7), f"{lat}Â°", fill=C_DIM, font=F11)
        for lat in range(20, 44, 5):
            y = gp(lat, LON_MIN)[1]
            drw.line([(PAD_L, y), (W, y)], fill=C_GRID_L, width=1)

        for lon in range(28, 65, 2):
            x = gp(LAT_MIN, lon)[0]
            drw.line([(x, PAD_T), (x, H - PAD_B)], fill=C_GRID, width=1)
        for lon in range(30, 65, 5):
            x = gp(LAT_MIN, lon)[0]
            drw.line([(x, PAD_T), (x, H - PAD_B)], fill=C_GRID_L, width=1)
            drw.text((x - 8, H - PAD_B + 5), f"{lon}Â°", fill=C_DIM, font=F11)

        # â”€â”€ Ù…Ø±Ø²Ù‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ (Ù¾Ù„ÛŒÚ¯ÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨ÛŒ polygon) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ÙØ±Ù…Øª: [(lon, lat), ...] â€” Ù…Ø®ØªØµØ§Øª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ
        COUNTRIES = {
            "IRAN": {
                "color": (38, 52, 62),
                "pts": [
                    (44.0,37.0),(44.8,39.2),(45.5,39.6),(46.0,39.0),(47.0,39.5),
                    (48.0,40.0),(49.0,40.2),(50.0,40.0),(51.0,40.8),(52.0,41.0),
                    (53.0,41.5),(54.0,41.2),(55.0,41.0),(56.0,40.5),(57.0,40.0),
                    (58.0,39.5),(59.0,38.0),(60.0,37.0),(61.0,36.5),(61.5,35.0),
                    (61.0,34.0),(60.5,33.0),(60.0,31.5),(59.5,30.5),(58.5,29.5),
                    (57.5,28.0),(57.0,27.0),(56.5,27.0),(56.0,27.0),(55.0,26.5),
                    (54.0,26.5),(53.5,27.0),(53.0,26.5),(52.5,27.0),(52.0,27.0),
                    (51.5,27.5),(51.0,28.0),(50.5,28.5),(50.0,29.0),(49.5,29.5),
                    (49.0,30.0),(48.5,30.5),(48.0,31.5),(47.5,32.0),(47.0,33.0),
                    (46.5,33.5),(46.0,34.0),(45.5,35.0),(45.0,36.0),(44.5,36.5),
                    (44.0,37.0)
                ]
            },
            "IRAQ": {
                "color": (36, 50, 60),
                "pts": [
                    (38.8,33.4),(39.5,33.8),(40.0,34.2),(41.0,34.7),(42.0,35.2),
                    (43.0,36.0),(44.0,37.0),(44.5,36.5),(45.0,36.0),(45.5,35.0),
                    (46.0,34.0),(46.5,33.5),(47.0,33.0),(47.5,32.0),(48.0,31.5),
                    (48.5,30.5),(47.5,30.0),(47.0,29.5),(46.5,29.2),(46.0,29.0),
                    (44.7,29.2),(43.5,29.5),(42.0,30.5),(41.0,31.5),(40.0,32.0),
                    (39.0,32.5),(38.8,33.4)
                ]
            },
            "SYRIA": {
                "color": (34, 48, 58),
                "pts": [
                    (35.7,36.8),(36.0,36.5),(36.5,36.8),(37.0,36.5),(38.0,36.8),
                    (39.0,36.5),(40.0,36.8),(41.0,37.5),(42.0,37.2),(42.5,37.0),
                    (43.0,36.0),(42.0,35.2),(41.0,34.7),(40.0,34.2),(39.5,33.8),
                    (38.8,33.4),(38.0,33.5),(37.5,33.3),(37.0,33.5),(36.5,33.5),
                    (36.0,33.0),(35.8,33.5),(35.5,34.0),(35.7,35.0),(35.7,36.8)
                ]
            },
            "TURKEY": {
                "color": (36, 50, 60),
                "pts": [
                    (26.0,41.0),(27.0,41.5),(28.0,41.8),(29.0,41.5),(30.0,41.5),
                    (31.0,41.5),(32.0,42.0),(33.0,42.0),(34.0,42.0),(35.0,42.0),
                    (36.0,41.5),(37.0,41.5),(38.0,40.5),(39.0,40.5),(40.0,40.5),
                    (41.0,40.0),(42.0,40.5),(43.0,40.5),(44.0,40.0),(44.5,39.8),
                    (44.0,39.2),(43.0,38.5),(42.0,38.5),(41.0,38.5),(40.0,38.0),
                    (39.0,37.5),(38.0,37.0),(37.0,37.0),(36.5,36.8),(36.0,36.5),
                    (35.7,36.8),(35.5,36.5),(35.0,36.5),(34.5,37.0),(34.0,37.0),
                    (32.0,37.0),(30.0,36.5),(28.0,37.0),(26.5,38.0),(26.0,39.0),
                    (26.0,41.0)
                ]
            },
            "SAUDI": {
                "color": (34, 46, 56),
                "pts": [
                    (36.5,29.5),(37.0,29.0),(38.0,28.0),(39.0,27.0),(40.0,26.0),
                    (41.0,25.0),(42.0,24.5),(43.0,24.0),(44.0,23.5),(45.0,23.0),
                    (46.0,22.5),(47.0,22.0),(48.0,21.5),(49.0,21.0),(50.0,20.5),
                    (51.0,20.0),(52.0,19.5),(53.0,19.0),(54.0,18.5),(55.0,18.0),
                    (56.0,18.5),(56.0,20.0),(55.0,22.0),(54.0,24.0),(53.0,25.0),
                    (52.0,26.0),(51.0,27.0),(50.5,28.5),(50.0,29.0),(49.5,29.5),
                    (49.0,30.0),(48.5,30.5),(48.0,31.5),(47.5,32.0),(47.0,31.5),
                    (46.5,31.0),(46.0,29.0),(44.7,29.2),(43.5,29.5),(42.0,30.5),
                    (41.0,31.5),(40.0,32.0),(39.0,32.5),(38.8,33.4),(38.0,33.5),
                    (37.5,32.0),(37.0,31.0),(36.8,30.0),(36.5,29.5)
                ]
            },
            "ISRAEL_PAL": {
                "color": (40, 55, 68),
                "pts": [
                    (34.3,31.3),(34.5,31.0),(34.9,30.0),(35.1,29.5),(35.0,29.0),
                    (34.8,28.5),(34.5,29.5),(34.0,30.5),(33.8,31.0),(34.0,31.5),
                    (34.3,31.3)
                ]
            },
            "LEBANON": {
                "color": (36, 52, 64),
                "pts": [
                    (35.1,33.0),(35.7,34.0),(36.5,34.0),(36.6,33.5),(36.0,33.3),
                    (35.5,33.0),(35.1,33.0)
                ]
            },
            "JORDAN": {
                "color": (34, 48, 58),
                "pts": [
                    (34.9,30.0),(35.0,32.0),(35.5,33.0),(36.0,33.3),(36.5,33.5),
                    (36.6,33.5),(37.0,33.5),(38.0,33.5),(38.8,33.4),(39.0,32.5),
                    (39.0,31.5),(38.5,30.5),(37.5,30.0),(36.8,30.0),(36.5,29.5),
                    (36.0,29.5),(35.5,29.5),(35.2,29.6),(35.1,29.5),(34.9,30.0)
                ]
            },
            "YEMEN": {
                "color": (32, 45, 54),
                "pts": [
                    (42.5,16.5),(43.5,16.0),(44.5,15.5),(45.0,15.0),(45.5,14.5),
                    (46.0,14.0),(47.0,14.5),(48.0,14.0),(49.0,14.5),(50.0,15.0),
                    (51.0,16.0),(52.0,17.0),(53.0,17.5),(54.0,17.8),(55.0,17.5),
                    (55.5,16.5),(55.0,16.0),(54.5,15.5),(53.5,16.0),(52.5,17.0),
                    (51.5,17.0),(50.5,16.5),(49.5,16.0),(48.5,16.0),(47.5,16.5),
                    (46.5,17.0),(45.5,17.5),(44.5,17.5),(43.5,17.0),(42.5,16.5)
                ]
            },
            "UAE_OMAN": {
                "color": (34, 48, 58),
                "pts": [
                    (51.5,24.0),(52.5,24.5),(53.0,25.0),(54.0,25.5),(55.0,26.0),
                    (56.0,26.5),(57.0,27.0),(57.5,22.5),(56.5,22.0),(55.5,22.0),
                    (55.0,23.0),(54.0,24.0),(53.0,23.5),(52.5,23.5),(51.5,24.0)
                ]
            },
        }

        # Ø±Ø³Ù… Ú©Ø´ÙˆØ±Ù‡Ø§
        for country, info in COUNTRIES.items():
            pts_geo = info["pts"]
            if not pts_geo: continue
            pts_px = [gp(lat, lon) for lon, lat in pts_geo]
            drw.polygon(pts_px, fill=info["color"], outline=C_BORDER)

        # â”€â”€ Ù†Ø§Ù… Ú©Ø´ÙˆØ±Ù‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        LABELS = [
            (32.5, 53.0, "IRAN",    C_LABEL),
            (33.3, 44.4, "IRAQ",    C_DIM),
            (35.0, 38.5, "SYRIA",   C_DIM),
            (31.5, 35.0, "ISRAEL",  C_DIM),
            (25.0, 45.0, "SAUDI",   C_DIM),
            (24.5, 54.5, "UAE",     C_DIM),
            (15.5, 48.0, "YEMEN",   C_DIM),
            (32.0, 36.0, "JORDAN",  C_DIM),
            (33.5, 36.2, "LEBANON", C_DIM),
            (39.0, 35.0, "TURKEY",  C_DIM),
            (26.5, 51.5, "GULF",    (60, 100, 140)),
        ]
        for r_lat, r_lon, name, color in LABELS:
            if LAT_MIN <= r_lat <= LAT_MAX and LON_MIN <= r_lon <= LON_MAX:
                px, py = gp(r_lat, r_lon)
                drw.text((px, py), name, fill=color, font=F12)

        # â”€â”€ Ø®Ù„ÛŒØ¬ ÙØ§Ø±Ø³ (Ø¢Ø¨ÛŒâ€ŒØªØ±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        gulf_pts = [gp(lat, lon) for lon, lat in [
            (48.0,30.0),(50.0,29.5),(52.0,28.5),(54.0,27.5),(56.0,27.0),
            (57.0,26.0),(57.0,25.0),(55.0,24.5),(53.0,24.0),(51.0,24.0),
            (50.0,24.5),(49.0,25.5),(48.0,27.0),(48.0,30.0)
        ]]
        drw.polygon(gulf_pts, fill=(12, 40, 72), outline=None)

        # â”€â”€ Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        red_sea_pts = [gp(lat, lon) for lon, lat in [
            (32.5,30.0),(33.0,28.0),(34.0,26.0),(35.0,24.0),(36.0,22.0),
            (37.0,20.0),(38.0,18.0),(39.0,17.5),(40.0,17.0),(43.0,16.0),
            (43.0,17.0),(41.0,18.5),(40.0,20.0),(39.0,22.0),(38.0,24.0),
            (37.5,26.0),(37.0,28.0),(36.5,30.0),(32.5,30.0)
        ]]
        drw.polygon(red_sea_pts, fill=(10, 36, 65), outline=None)

        # â”€â”€ Ø¯Ø±ÛŒØ§ÛŒ Ù…Ø¯ÛŒØªØ±Ø§Ù†Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        med_pts = [gp(lat, lon) for lon, lat in [
            (26.0,36.5),(30.0,36.0),(32.0,34.5),(34.0,33.0),(35.7,36.8),
            (34.5,37.0),(32.0,37.0),(30.0,36.5),(28.0,37.0),(26.5,38.0),
            (26.0,36.5)
        ]]
        drw.polygon(med_pts, fill=(10, 36, 65), outline=None)

        # â”€â”€ Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plane_colors = [C_PLANE, C_PLANE2, (80, 200, 120), (180, 80, 255)]
        placed = []

        for idx, ac in enumerate(aircraft):
            lat, lon = ac["lat"], ac["lon"]
            if not (LAT_MIN <= lat <= LAT_MAX and LON_MIN <= lon <= LON_MAX):
                continue
            px, py = gp(lat, lon)

            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„
            shift = 0
            for ppx, ppy in placed:
                if abs(px - ppx) < 20 and abs(py - ppy) < 20:
                    py -= 25
                    break
            placed.append((px, py))

            pc = plane_colors[idx % len(plane_colors)]

            # Ø¯Ø§ÛŒØ±Ù‡ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø¯Ø±Ø®Ø´Ø§Ù†
            drw.ellipse([(px-18, py-18), (px+18, py+18)],
                        fill=(pc[0]//4, pc[1]//4, pc[2]//4), outline=pc, width=2)
            # Ù…Ø«Ù„Ø« Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§
            tri = [(px, py-12), (px-8, py+8), (px+8, py+8)]
            drw.polygon(tri, fill=pc, outline=(255,255,255))
            # Ù†Ù‚Ø·Ù‡ Ù…Ø±Ú©Ø²ÛŒ
            drw.ellipse([(px-3, py-3), (px+3, py+3)], fill=(255,255,255))

            # Ø®Ø· Ø±Ø§Ù‡Ù†Ù…Ø§ Ø¨Ù‡ Ø¨Ø±Ú†Ø³Ø¨
            lx = px + 22
            drw.line([(px+12, py), (lx-2, py)], fill=pc, width=1)

            # Ø¨Ø±Ú†Ø³Ø¨ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡
            label   = f"{ac['callsign']} / {ac['type']}"
            alt_txt = f"alt:{int(ac['alt'])//1000 if ac['alt'] else '?'}k  {ac['gs']}kt"
            drw.rectangle([(lx-2, py-14), (lx+170, py+18)],
                          fill=(12, 18, 28), outline=pc)
            drw.text((lx+2, py-13), label,   fill=pc,    font=FB)
            drw.text((lx+2, py+2),  alt_txt, fill=C_DIM, font=F11)

        # â”€â”€ Ù‡Ø¯Ø± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        drw.rectangle([(0, 0), (W, PAD_T - 2)], fill=C_HEAD)
        drw.rectangle([(0, PAD_T - 2), (W, PAD_T)], fill=C_ACCENT)
        now_str = datetime.now(TEHRAN_TZ).strftime("%H:%M  %Y/%m/%d")
        drw.text((10, 8),
                 f"âœˆ  Military Flights â€” Middle East  |  {now_str}  |  {len(aircraft)} aircraft tracked",
                 fill=C_ACCENT, font=FB)

        # â”€â”€ ÙÙˆØªØ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        drw.rectangle([(0, H - PAD_B), (W, H)], fill=C_HEAD)
        drw.text((10, H - PAD_B + 6), "Source: ADS-B Exchange  |  WarBot v17",
                 fill=C_DIM, font=F11)

        # â”€â”€ legend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        lx, ly = W - 200, PAD_T + 10
        drw.rectangle([(lx-5, ly-5), (W-5, ly + len(aircraft)*22 + 10)],
                      fill=(10, 15, 25), outline=C_BORDER)
        for i, ac in enumerate(aircraft):
            pc = plane_colors[i % len(plane_colors)]
            drw.rectangle([(lx, ly + i*22), (lx+12, ly + i*22 + 12)], fill=pc)
            drw.text((lx+16, ly + i*22 - 2),
                     f"{ac['callsign']} â€“ {ac['region']}", fill=C_LABEL, font=F11)

        buf = io.BytesIO()
        img.save(buf, "JPEG", quality=90)
        buf.seek(0)
        return buf

    except Exception as e:
        log.warning(f"flight_map error: {e}")
        import traceback; log.debug(traceback.format_exc())
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS + Telegram fetch
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_rss(client: httpx.AsyncClient, feed: dict) -> list:
    """RSS Ø¨Ø§ conditional GET (ETag/If-Modified-Since)"""
    try:
        hdrs = dict(COMMON_UA)
        hdrs["Accept"] = "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8"
        if feed.get("_etag"):      hdrs["If-None-Match"]     = feed["_etag"]
        if feed.get("_last_mod"):  hdrs["If-Modified-Since"] = feed["_last_mod"]
        r = await client.get(feed["u"], timeout=httpx.Timeout(RSS_TIMEOUT), headers=hdrs)
        if r.status_code == 304: return []
        if r.status_code != 200: return []
        if r.headers.get("ETag"):          feed["_etag"]     = r.headers["ETag"]
        if r.headers.get("Last-Modified"): feed["_last_mod"] = r.headers["Last-Modified"]
        entries = feedparser.parse(r.text).entries or []
        is_emb  = id(feed) in EMBASSY_SET
        return [(e, feed["n"], "rss", is_emb) for e in entries]
    except: return []

async def fetch_telegram_channel(client: httpx.AsyncClient, label: str,
                                  handle: str, cutoff: datetime) -> list:
    """
    scrape t.me/s/{handle} â€” ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² cutoff
    """
    url  = f"https://t.me/s/{handle}"
    hdrs = {
        "User-Agent": "TelegramBot (like TwitterBot)",
        "Accept": "text/html,application/xhtml+xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Cache-Control": "no-cache",
    }
    try:
        r = await client.get(url, timeout=httpx.Timeout(TG_TIMEOUT), headers=hdrs)
        if r.status_code not in (200, 301, 302): return []
        soup = BeautifulSoup(r.text, "html.parser")
        msgs = soup.select(".tgme_widget_message_wrap")
        if not msgs: return []
        results = []
        for msg in msgs[-30:]:
            txt_el = msg.select_one(".tgme_widget_message_text")
            text   = txt_el.get_text(" ", strip=True) if txt_el else ""
            if not text or len(text) < 15: continue
            time_el  = msg.select_one("time")
            dt_str   = time_el.get("datetime", "") if time_el else ""
            entry_dt = None
            if dt_str:
                try: entry_dt = datetime.fromisoformat(dt_str.replace("Z","+00:00"))
                except: pass
            # ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØªØ§Ø²Ù‡â€ŒØªØ± Ø§Ø² cutoff
            if entry_dt and entry_dt < cutoff: continue
            link_el = msg.select_one("a.tgme_widget_message_date")
            link    = link_el.get("href","") if link_el else f"https://t.me/{handle}"
            results.append(({
                "title":   text[:300],
                "summary": text[:800],
                "link":    link,
                "_tg_dt":  entry_dt,
            }, label, "tg", False))
        return results
    except Exception as e:
        log.debug(f"TG {handle}: {e}"); return []

async def fetch_all(client: httpx.AsyncClient, cutoff: datetime) -> list:
    """
    ÙˆØ§Ú©Ø´ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹
    cutoff Ø¨Ø±Ø§ÛŒ Telegram Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡ (RSS Ø§Ø² is_fresh Ø¯Ø± main ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´Ù‡)
    """
    await build_twitter_pools(client)

    rss_t = [fetch_rss(client, f) for f in ALL_RSS_FEEDS]
    tg_t  = [fetch_telegram_channel(client, l, h, cutoff) for l, h in TELEGRAM_CHANNELS]
    tw_t  = [fetch_twitter(client, l, h) for l, h in TWITTER_HANDLES]

    all_res = await asyncio.gather(*rss_t, *tg_t, *tw_t, return_exceptions=True)

    out = []; rss_ok = tg_ok = tw_ok = 0
    n_rss = len(ALL_RSS_FEEDS); n_tg = len(TELEGRAM_CHANNELS)
    for i, res in enumerate(all_res):
        if not isinstance(res, list): continue
        out.extend(res)
        if   i < n_rss:          rss_ok += bool(res)
        elif i < n_rss + n_tg:   tg_ok  += bool(res)
        else:                     tw_ok  += bool(res)

    log.info(f"  ğŸ“¡ RSS:{rss_ok}/{len(ALL_RSS_FEEDS)} "
             f" ğŸ“¢ TG:{tg_ok}/{len(TELEGRAM_CHANNELS)} "
             f" ğ•:{tw_ok}/{len(TWITTER_HANDLES)}")
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø± Ù…ØªÙ†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(t): return re.sub(r"<[^>]+>", " ", t or "").strip()
def trim(t, n):
    t = t.strip()
    return t if len(t) <= n else t[:n-1] + "â€¦"
def make_id(entry):
    k = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(k.encode()).hexdigest()
def esc(t):
    return re.sub(r"([<>&])", lambda m: {"<":"&lt;",">":"&gt;","&":"&amp;"}[m.group()], t)

def format_dt(entry) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
        tg_dt = entry.get("_tg_dt")
        if tg_dt:
            return tg_dt.astimezone(TEHRAN_TZ).strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
    except: pass
    return ""

def is_fresh(entry, cutoff: datetime) -> bool:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t: return datetime(*t[:6], tzinfo=timezone.utc) >= cutoff
        tg_dt = entry.get("_tg_dt")
        if tg_dt: return tg_dt >= cutoff
        return True  # Ø¨Ø¯ÙˆÙ† timestamp â†’ Ù¾Ø§Ø³ Ø¨Ø¯Ù‡ (seen.json ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ù‡)
    except: return True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dedup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_VIOLENCE_CODES  = {"MSL","AIR","ATK","KIA","DEF","EXP"}
_POLITICAL_CODES = {"THR","DIP","SAN","NUC","SPY","STM"}

def _stem(word):
    w = word.lower()
    for suf in ("ing","ed","tion","ment","er","Ù‡Ø§","Ù‡Ø§ÛŒ","\u200cÙ‡Ø§"):
        if w.endswith(suf) and len(w) > len(suf)+3: return w[:-len(suf)]
    return w

def _bag(text):
    return {_stem(w) for w in re.findall(r"[\w\u0600-\u06FF]{3,}", text.lower())}

def _entity_triple(title):
    txt = title.lower()
    actors = (
        ["iran","irgc","khamenei","Ø³Ù¾Ø§Ù‡","Ø§ÛŒØ±Ø§Ù†"],
        ["israel","idf","netanyahu","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„"],
        ["us ","usa","centcom","pentagon","Ø¢Ù…Ø±ÛŒÚ©Ø§"],
        ["hamas","Ø­Ù…Ø§Ø³"], ["hezbollah","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡"], ["houthi","Ø­ÙˆØ«ÛŒ"],
    )
    action_cats = {
        "MSL": ["missile","rocket","ballistic","Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯"],
        "AIR": ["airstrike","bombing","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†"],
        "ATK": ["attack","strike","Ø­Ù…Ù„Ù‡"],
        "KIA": ["killed","dead","casualties","Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯"],
        "DEF": ["intercept","iron dome","Ø±Ù‡Ú¯ÛŒØ±ÛŒ"],
        "EXP": ["explosion","blast","Ø§Ù†ÙØ¬Ø§Ø±"],
        "THR": ["threat","warn","ØªÙ‡Ø¯ÛŒØ¯"],
        "SAN": ["sanction","ØªØ­Ø±ÛŒÙ…"],
        "NUC": ["nuclear","uranium","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ"],
    }
    actor1, actor2, act = "", "", ""
    for i, grp in enumerate(actors):
        if any(a in txt for a in grp):
            if not actor1: actor1 = str(i)
            elif not actor2: actor2 = str(i)
    for code, kws in action_cats.items():
        if any(k in txt for k in kws): act = code; break
    return actor1, actor2, act

def is_story_dup(title: str, stories: list) -> bool:
    bag1 = _bag(title)
    if not bag1: return False
    a1, a2, act1 = _entity_triple(title)
    for item in stories:
        if not (isinstance(item, (list, tuple)) and len(item) == 3):
            continue
        _, prev_bag_raw, prev_triple = item
        prev_bag = set(prev_bag_raw) if isinstance(prev_bag_raw, list) else prev_bag_raw
        pa, pb, pact = prev_triple
        if act1 and pact and act1 in _VIOLENCE_CODES and pact in _VIOLENCE_CODES:
            if a1 == pa and a2 == pb: return True
        if act1 and pact and act1 in _POLITICAL_CODES and pact in _POLITICAL_CODES:
            if a1 == pa: return True
        union = bag1 | prev_bag
        if union and len(bag1 & prev_bag) / len(union) >= JACCARD_THRESHOLD:
            return True
    return False

def register_story(title: str, stories: list) -> list:
    stories.append([title, list(_bag(title)), list(_entity_triple(title))])
    return stories[-MAX_STORIES:]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# seen.json â€” Ø¨Ø§ TTL â€” ÙÙ‚Ø· Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    cutoff_ts = datetime.now(timezone.utc).timestamp() - SEEN_TTL_HOURS * 3600
    try:
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                return {k for k, v in raw.items() if v > cutoff_ts}
            elif isinstance(raw, list):
                # migrate Ø§Ø² ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… â€” ÙÙ‚Ø· ÛµÛ°Û° ØªØ§ Ø¢Ø®Ø±
                return set(raw[-500:])
    except: pass
    return set()

def save_seen(seen: set):
    now_ts    = datetime.now(timezone.utc).timestamp()
    cutoff_ts = now_ts - SEEN_TTL_HOURS * 3600
    try:
        existing = {}
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                existing = {k: v for k, v in raw.items() if v > cutoff_ts}
    except: existing = {}
    for eid in seen:
        if eid not in existing: existing[eid] = now_ts
    if len(existing) > 5000:
        existing = dict(sorted(existing.items(), key=lambda x: x[1], reverse=True)[:5000])
    json.dump(existing, open(SEEN_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# run_state â€” last_run Ø¨Ø±Ø§ÛŒ cutoff Ù‡ÙˆØ´Ù…Ù†Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_run_state() -> datetime:
    """Ø¢Ø®Ø±ÛŒÙ† Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ â€” Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ cutoff"""
    try:
        if Path(RUN_STATE_FILE).exists():
            d   = json.load(open(RUN_STATE_FILE))
            ts  = d.get("last_run", 0)
            if ts:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
    except: pass
    # Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§: MAX_LOOKBACK_MIN Ø¨Ù‡ Ø¹Ù‚Ø¨
    return datetime.now(timezone.utc) - timedelta(minutes=MAX_LOOKBACK_MIN)

def save_run_state():
    existing = {}
    try:
        if Path(RUN_STATE_FILE).exists():
            existing = json.load(open(RUN_STATE_FILE))
    except: pass
    existing["last_run"] = datetime.now(timezone.utc).timestamp()
    json.dump(existing, open(RUN_STATE_FILE, "w"))

def load_stories() -> list:
    try:
        if Path(STORIES_FILE).exists():
            raw = json.load(open(STORIES_FILE))
            # migrate ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… (2-tuple) Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ (3-tuple)
            result = []
            for item in raw:
                if isinstance(item, (list, tuple)) and len(item) == 2:
                    title = item[0]
                    result.append([title, list(_bag(title)), list(_entity_triple(title))])
                elif isinstance(item, (list, tuple)) and len(item) == 3:
                    result.append(item)
            return result
    except: pass
    return []

def save_stories(stories):
    json.dump(stories[-MAX_STORIES:], open(STORIES_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªØ±Ø¬Ù…Ù‡ â€” Gemini Ø§ÙˆÙ„ØŒ MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† fallback
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_MODELS = [
    "gemini-2.0-flash",
    "gemini-1.5-flash",
    "gemini-1.5-flash-8b",
]

# ØªØ´Ø®ÛŒØµ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
def _is_farsi(text: str) -> bool:
    fa_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
    return fa_chars / max(len(text), 1) > 0.3

# ØªØ±Ø¬Ù…Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù† ÛŒÚ© Ù…ØªÙ† Ø§Ø² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ MyMemory
async def _translate_mymemory(client: httpx.AsyncClient, text: str) -> str:
    """MyMemory API â€” Ø±Ø§ÛŒÚ¯Ø§Ù†ØŒ Ø¨Ø¯ÙˆÙ† Ú©Ù„ÛŒØ¯ØŒ ØªØ§ ÛµÛ°Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¯Ø± Ø±ÙˆØ²"""
    if not text or _is_farsi(text):
        return text
    try:
        url = "https://api.mymemory.translated.net/get"
        r = await client.get(url,
            params={"q": text[:500], "langpair": "en|fa", "de": "warbot@github.com"},
            timeout=httpx.Timeout(8.0))
        if r.status_code == 200:
            data = r.json()
            tr = data.get("responseData", {}).get("translatedText", "")
            # MyMemory Ú¯Ø§Ù‡ÛŒ MYMEMORY WARNING Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
            if tr and "MYMEMORY WARNING" not in tr and len(tr) > 5:
                return tr
    except Exception as e:
        log.debug(f"MyMemory: {e}")
    return text

GEMINI_PROMPT = """ØªÙˆ ÛŒÚ© Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ø¬Ù†Ú¯ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒ. Ø§ÛŒÙ† Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ú©Ù†.

Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø±Ø§ Ø±Ø¹Ø§ÛŒØª Ú©Ù†:
###ITEM_0###
T: [Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ Ø¯Ø± ÛŒÚ© Ø®Ø·]
B: [Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„]
###ITEM_1###
T: [Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ]
B: [Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ]

Ù‚ÙˆØ§Ù†ÛŒÙ†:
- Ø§Ø³Ø§Ù…ÛŒ: Netanyahu=Ù†ØªØ§Ù†ÛŒØ§Ù‡ÙˆØŒ Khamenei=Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒØŒ IRGC=Ø³Ù¾Ø§Ù‡ØŒ IDF=Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ØŒ CENTCOM=Ø³ØªØ§Ø¯ Ù…Ø±Ú©Ø²ÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§
- Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¢Ù…Ø§Ø±ØŒ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±
- Ø§Ú¯Ù‡ Ø®Ø¨Ø± ÙØ§Ø±Ø³ÛŒÙ‡: ÙÙ‚Ø· Ù¾Ø§Ú©ÛŒØ²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†

===Ø®Ø¨Ø±Ù‡Ø§===
{items}"""

async def _translate_gemini(client: httpx.AsyncClient, articles: list) -> list | None:
    """ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Gemini â€” None Ø§Ú¯Ù‡ fail Ø´Ø¯"""
    if not GEMINI_API_KEY:
        return None
    items_txt = "".join(
        f"###ITEM_{i}###\nEN_TITLE: {t[:300]}\nEN_BODY: {s[:400]}\n\n"
        for i, (t, s) in enumerate(articles)
    )
    state = {}
    try:
        if Path(GEMINI_STATE_FILE).exists():
            state = json.load(open(GEMINI_STATE_FILE))
    except: pass
    models = state.get("models_order", GEMINI_MODELS)
    base   = "https://generativelanguage.googleapis.com/v1beta/models"

    for model in models:
        try:
            r = await client.post(
                f"{base}/{model}:generateContent?key={GEMINI_API_KEY}",
                json={
                    "contents": [{"parts": [{"text": GEMINI_PROMPT.format(items=items_txt)}]}],
                    "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
                },
                timeout=httpx.Timeout(40.0)
            )
            if r.status_code == 429:
                log.warning(f"Gemini {model}: rate-limit"); continue
            if r.status_code != 200:
                log.warning(f"Gemini {model}: HTTP {r.status_code} â€” {r.text[:200]}"); continue

            text_out = r.json()["candidates"][0]["content"]["parts"][0]["text"]
            log.info(f"ğŸŒ Gemini {model} OK")

            results = list(articles)
            ok_count = 0
            for i, (orig_t, orig_s) in enumerate(articles):
                blk = re.search(rf"###ITEM_{i}###\s*(.*?)(?=###ITEM_\d+###|\Z)", text_out, re.DOTALL)
                if not blk: continue
                block   = blk.group(1)
                t_match = re.search(r"^T:\s*(.+)$", block, re.MULTILINE)
                b_match = re.search(r"^B:\s*([\s\S]+?)$", block, re.MULTILINE)
                fa_t = t_match.group(1).strip() if t_match else ""
                fa_b = b_match.group(1).strip() if b_match else ""
                # fallback: Ù‡Ù…Ù‡ block Ø±Ø§ Ø¹Ù†ÙˆØ§Ù† Ø¨Ú¯ÛŒØ±
                if not fa_t:
                    fa_t = block.strip().split('\n')[0]
                if len(fa_t) > 5:
                    results[i] = (fa_t, fa_b or orig_s)
                    ok_count += 1
            log.info(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡: {ok_count}/{len(articles)} Ø®Ø¨Ø±")
            # Ù…Ø¯Ù„ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø±Ø§ Ø§ÙˆÙ„ Ø¨Ú¯Ø°Ø§Ø±
            state["models_order"] = [model] + [m for m in models if m != model]
            json.dump(state, open(GEMINI_STATE_FILE, "w"))
            return results
        except Exception as e:
            log.warning(f"Gemini {model}: {e}"); continue
    return None

async def translate_batch(client: httpx.AsyncClient, articles: list) -> list:
    """
    ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª:
    1. Gemini (Ø§Ú¯Ù‡ API key Ø¯Ø§Ø±ÛŒÙ…)
    2. MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† (ÙÙ‚Ø· Ø¹Ù†ÙˆØ§Ù†)
    3. Ù…ØªÙ† Ø§ØµÙ„ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØ±Ø¬Ù…Ù‡)
    """
    if not articles:
        return []

    results = list(articles)

    # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û±: Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if GEMINI_API_KEY:
        log.info(f"ğŸŒ Gemini: ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø®Ø¨Ø±...")
        gemini_res = await _translate_gemini(client, articles)
        if gemini_res:
            return gemini_res
        log.warning("ğŸŒ Gemini fail â€” fallback Ø¨Ù‡ MyMemory")
    else:
        log.info("ğŸŒ GEMINI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ â€” Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù†")

    # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û²: MyMemory â€” Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info(f"ğŸŒ MyMemory: ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø¹Ù†ÙˆØ§Ù†...")
    sema = asyncio.Semaphore(5)

    async def _tr(orig_t, orig_s):
        async with sema:
            if _is_farsi(orig_t):
                return (orig_t, orig_s)
            fa_t = await _translate_mymemory(client, orig_t)
            return (fa_t, orig_s)

    translated = await asyncio.gather(*[_tr(t, s) for t, s in articles])
    ok = sum(1 for i, (fa, _) in enumerate(translated) if fa != articles[i][0])
    log.info(f"ğŸŒ MyMemory: {ok}/{len(articles)} ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯")
    return list(translated)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Sentiment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BREAKING_KEYWORDS = [
    "breaking","urgent","alert","just in","explosion","airstrike","killed","dead",
    "war","attack","strike","nuclear","bomb","missile","invasion",
    "Ø­Ù…Ù„Ù‡","Ú©Ø´ØªÙ‡","Ø§Ù†ÙØ¬Ø§Ø±","Ø´Ù‡ÛŒØ¯","Ù…ÙˆØ´Ú©","ÙÙˆØ±ÛŒ","Ø®Ø¨Ø± ÙÙˆØ±ÛŒ","Ø§Ø¹Ù„Ø§Ù… Ø¬Ù†Ú¯",
]
IMPORTANCE_BOOST = {
    "ğŸ’€":4, "ğŸ”´":3, "ğŸ’¥":3, "ğŸš€":3, "â˜¢ï¸":3,
    "âœˆï¸":2, "ğŸš¢":2, "ğŸ›¡ï¸":2, "ğŸ•µï¸":2,
    "ğŸ”¥":1, "ğŸ’°":1, "âš ï¸":1,
}

SENTIMENT_RULES = [
    ("ğŸ’€", ["killed","dead","casualties","fatalities","wounded","martyred","massacre"],
           ["Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯","ØªÙ„ÙØ§Øª","Ú©Ø´ØªØ§Ø±","Ù…Ø¬Ø±ÙˆØ­"]),
    ("ğŸ”´", ["attack","struck","assault","launched attack","opened fire","bombed","targeted"],
           ["Ø­Ù…Ù„Ù‡","Ø¶Ø±Ø¨Ù‡","Ù…ÙˆØ±Ø¯ Ù‡Ø¯Ù","Ø­Ù…Ù„Ù‡ Ú©Ø±Ø¯"]),
    ("ğŸ’¥", ["explosion","blast","detonation","explode","blew up"],
           ["Ø§Ù†ÙØ¬Ø§Ø±","Ù…Ù†ÙØ¬Ø±","ØªØ±Ú©ÛŒØ¯"]),
    ("âœˆï¸", ["airstrike","air strike","air raid","warplane","f-35","f-15","b-52","f-16"],
           ["Ø­Ù…Ù„Ù‡ Ù‡ÙˆØ§ÛŒÛŒ","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†","Ø¬Ù†Ú¯Ù†Ø¯Ù‡"]),
    ("ğŸš€", ["missile","rocket","ballistic","cruise missile","drone strike","hypersonic"],
           ["Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯","Ù…ÙˆØ´Ú© Ø¨Ø§Ù„Ø³ØªÛŒÚ©","Ø±Ø§Ú©Øª"]),
    ("â˜¢ï¸", ["nuclear","uranium","enrichment","natanz","fordow","centrifuge","iaea"],
           ["Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…","ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ","Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ø³Ø§Ù†ØªØ±ÛŒÙÛŒÙˆÚ˜"]),
    ("ğŸš¢", ["navy","naval","warship","aircraft carrier","strait of hormuz","red sea"],
           ["Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ","Ù†Ø§Ùˆ","ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²","Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø®"]),
    ("ğŸ•µï¸", ["intelligence","mossad","cia","spy","covert","assassination","sabotage","cyber"],
           ["Ø¬Ø§Ø³ÙˆØ³ÛŒ","Ù…ÙˆØ³Ø§Ø¯","Ø®Ø±Ø§Ø¨Ú©Ø§Ø±ÛŒ","ØªØ±ÙˆØ±","Ø³Ø§ÛŒØ¨Ø±ÛŒ"]),
    ("ğŸ›¡ï¸", ["intercept","shot down","iron dome","air defense","patriot"],
           ["Ø±Ù‡Ú¯ÛŒØ±ÛŒ","Ù¾Ø¯Ø§ÙÙ†Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†","Ø³Ø±Ù†Ú¯ÙˆÙ†"]),
    ("ğŸ”¥", ["escalat","tension","brink of war","retaliat","provocation"],
           ["ØªØ´Ø¯ÛŒØ¯","ØªÙ†Ø´","ØªÙ„Ø§ÙÛŒ","Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯"]),
    ("ğŸ’°", ["sanction","embargo","swift","freeze assets"],
           ["ØªØ­Ø±ÛŒÙ…","Ù…Ø­Ø§ØµØ±Ù‡ Ø§Ù‚ØªØµØ§Ø¯ÛŒ"]),
    ("âš ï¸", ["threat","warn","warning","ultimatum","red line","will respond"],
           ["ØªÙ‡Ø¯ÛŒØ¯","Ù‡Ø´Ø¯Ø§Ø±","Ø®Ø· Ù‚Ø±Ù…Ø²","Ø§ÙˆÙ„ØªÛŒÙ…Ø§ØªÙˆÙ…"]),
    ("ğŸ¤", ["negotiation","talks","deal","diplomacy","ceasefire","agreement"],
           ["Ù…Ø°Ø§Ú©Ø±Ù‡","ØªÙˆØ§ÙÙ‚","Ø¢ØªØ´â€ŒØ¨Ø³","Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ"]),
    ("ğŸ“œ", ["statement","declared","announced","press conference","spokesperson"],
           ["Ø¨ÛŒØ§Ù†ÛŒÙ‡","Ø§Ø¹Ù„Ø§Ù…","Ù†Ø´Ø³Øª Ø®Ø¨Ø±ÛŒ","Ø³Ø®Ù†Ú¯Ùˆ"]),
]

def analyze_sentiment(text: str) -> list:
    txt = text.lower()
    found = []
    for icon, en_kws, fa_kws in SENTIMENT_RULES:
        if any(kw in txt for kw in en_kws) or any(kw in txt for kw in fa_kws):
            found.append(icon)
        if len(found) >= 3: break
    return found or ["ğŸ“°"]

def calc_importance(title: str, body: str, icons: list, stype: str) -> int:
    txt = (title + " " + body).lower()
    score = sum(IMPORTANCE_BOOST.get(ic, 0) for ic in icons)
    if any(k in txt for k in BREAKING_KEYWORDS): score += 2
    if stype == "tw" and score > 0: score += 1
    return min(score, 10)

def sentiment_bar(icons): return "  ".join(icons)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Telegram Ø§Ø±Ø³Ø§Ù„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _tgapi(path: str) -> str:
    return f"https://api.telegram.org/bot{BOT_TOKEN}/{path}"

async def tg_send_text(client: httpx.AsyncClient, text: str) -> bool:
    text = text[:MAX_MSG_LEN]
    for attempt in range(3):
        try:
            r = await client.post(_tgapi("sendMessage"),
                json={"chat_id": CHANNEL_ID, "text": text,
                      "parse_mode": "HTML", "disable_web_page_preview": False},
                timeout=httpx.Timeout(15.0))
            d = r.json()
            if r.status_code == 200 and d.get("ok"): return True
            if d.get("error_code") == 429:
                wait = d.get("parameters", {}).get("retry_after", 20)
                await asyncio.sleep(wait)
            elif attempt < 2:
                await asyncio.sleep(3)
        except Exception as e:
            log.warning(f"TG send: {e}")
            if attempt < 2: await asyncio.sleep(5)
    return False

async def tg_send_photo(client: httpx.AsyncClient, buf: io.BytesIO,
                         caption: str) -> bool:
    caption = caption[:1024]
    try:
        buf.seek(0)
        r = await client.post(_tgapi("sendPhoto"),
            data={"chat_id": CHANNEL_ID, "caption": caption, "parse_mode": "HTML"},
            files={"photo": ("card.jpg", buf, "image/jpeg")},
            timeout=httpx.Timeout(20.0))
        return r.status_code == 200 and r.json().get("ok", False)
    except Exception as e:
        log.warning(f"TG photo: {e}"); return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIL Ú©Ø§Ø±Øª Ø®Ø¨Ø±ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BG_DARK  = (14, 16, 22)
BG_BAR   = (22, 26, 34)
FG_WHITE = (235, 237, 242)
FG_GREY  = (120, 132, 148)
ACCENT_MAP = {
    "ğŸ‡®ğŸ‡·":(180,40,40), "ğŸ‡®ğŸ‡±":(30,90,180), "ğŸ‡ºğŸ‡¸":(40,80,160),
    "ğŸ”":(60,130,80), "ğŸŒ":(100,60,130), "ğŸ›ï¸":(140,100,40),
}
ICON_BG = {
    "ğŸ’€":(140,20,20),"ğŸ”´":(180,30,30),"ğŸ’¥":(190,80,10),
    "âœˆï¸":(20,90,160),"ğŸš€":(100,20,160),"â˜¢ï¸":(0,130,50),
    "ğŸš¢":(10,80,140),"ğŸ•µï¸":(60,55,70),"ğŸ›¡ï¸":(20,110,80),
    "ğŸ”¥":(180,60,0),"ğŸ’°":(130,110,0),"âš ï¸":(160,110,0),
    "ğŸ¤":(20,120,100),"ğŸ“œ":(60,80,100),"ğŸ“°":(45,58,72),
}

def _get_accent(src, urgent):
    if urgent: return (210, 40, 40)
    for k, v in ACCENT_MAP.items():
        if src.startswith(k) or k in src: return v
    return (80, 110, 140)

def _wrap(text, chars):
    words, lines_out, cur = text.split(), [], ""
    for w in words:
        if len(cur) + len(w) + 1 <= chars: cur = (cur + " " + w).strip()
        else:
            if cur: lines_out.append(cur)
            cur = w
    if cur: lines_out.append(cur)
    return lines_out

def _fonts():
    try:
        bold = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
        reg  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        sm   = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 13)
        return bold, reg, sm
    except:
        d = ImageFont.load_default(); return d, d, d

def make_news_card(headline, fa_text, src, dt_str,
                   urgent=False, sentiment_icons=None):
    if not PIL_OK: return None
    try:
        W, H = 960, 310
        acc = _get_accent(src, urgent)
        img = Image.new("RGB", (W, H), BG_DARK)
        drw = ImageDraw.Draw(img)
        F_H, F_B, F_sm = _fonts()

        drw.rectangle([(0,0),(W,5)], fill=acc)
        drw.rectangle([(0,5),(W,58)], fill=BG_BAR)
        drw.rectangle([(0,58),(W,61)], fill=acc)
        drw.text((18,18), src[:55],     font=F_sm, fill=acc)
        drw.text((W-170,18), dt_str[:25], font=F_sm, fill=FG_GREY)

        display = fa_text if (fa_text and len(fa_text) > 5) else headline
        y = 72
        for line in _wrap(display, 50)[:4]:
            drw.text((W-18, y), line, font=F_H, fill=FG_WHITE, anchor="ra")
            y += 30

        drw.rectangle([(0,H-56),(W,H)], fill=BG_BAR)
        drw.rectangle([(0,H-58),(W,H-56)], fill=acc)
        x_pos = 16
        for ico in (sentiment_icons or ["ğŸ“°"])[:4]:
            bg = ICON_BG.get(ico, (50,65,75))
            drw.rounded_rectangle([(x_pos-2,H-52),(x_pos+38,H-6)], radius=7, fill=bg)
            drw.text((x_pos+2,H-50), ico, font=F_H, fill=(255,255,255))
            x_pos += 50

        if urgent: drw.rectangle([(0,61),(5,H-58)], fill=acc)

        buf = io.BytesIO()
        img.save(buf, "JPEG", quality=85)
        buf.seek(0)
        return buf
    except Exception as e:
        log.debug(f"card: {e}"); return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² Ø³Ø§ÛŒØª (og:image ØªØµÙˆÛŒØ± Ù…Ù‚Ø§Ù„Ù‡ â€” Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± (Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ â€” Ø¹Ú©Ø³ Ù…Ù‚Ø§Ù„Ù‡)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ (Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ â€” Ø¹Ú©Ø³ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# URLâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒ Ù„ÙˆÚ¯Ùˆ Ø¯Ø§Ø±Ù†Ø¯
_SKIP_IMG_PATTERNS = [
    "logo","icon","favicon","sprite","avatar","placeholder",
    "default","blank","spacer","1x1","pixel","brand","masthead",
    "no-image","no-photo","profile","author","byline","signature",
    "/ad/","/ads/","banner","promo","subscribe","newsletter",
]
# CSS selector Ù‡Ø§ÛŒ ordered Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±
_IMG_SELECTORS = [
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ article Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    "article figure img",
    "article .featured-image img",
    "article .hero-image img",
    "[class*='article-image'] img",
    "[class*='news-image'] img",
    "[class*='story-image'] img",
    "[class*='featured-img'] img",
    "[class*='lead-image'] img",
    "[class*='post-image'] img",
    "[class*='entry-image'] img",
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
    ".detail-media img",
    ".news-photo img",
    ".content-media img",
    ".article-img img",
    ".body img",
    # Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØªØ±
    "figure img",
    "picture source",
    "picture img",
    ".content img",
    "article img",
]

async def fetch_article_image(client: httpx.AsyncClient, url: str) -> "io.BytesIO | None":
    """
    ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡:
    Û±. CSS selectors Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† ØªØµÙˆÛŒØ± Ø®Ø¨Ø± Ø¯Ø± Ù…ØªÙ† Ù…Ù‚Ø§Ù„Ù‡
    Û². og:image / twitter:image ÙÙ‚Ø· Ø§Ú¯Ù‡ Ø¹Ø±Ø¶ â‰¥ Û¶Û°Û° Ø¨Ø§Ø´Ø¯
    Û³. ÙÛŒÙ„ØªØ± Ù„ÙˆÚ¯Ùˆ: Ø­Ø¬Ù… < Û±ÛµKB ÛŒØ§ Ø§Ø¨Ø¹Ø§Ø¯ < ÛµÛ°Û°Ã—Û²Û¸Û° ÛŒØ§ ratio < 1.3 â†’ Ø±Ø¯
    """
    if not url or len(url) < 10:
        return None
    skip_domains = ("t.me", "twitter.com", "x.com", "google.com/rss",
                    "feeds.reuters", "feeds.bbci", "feed.", "rss.")
    if any(d in url for d in skip_domains):
        return None

    try:
        r = await client.get(url,
            timeout=httpx.Timeout(10.0),
            headers={**COMMON_UA,
                     "Accept": "text/html,*/*;q=0.8",
                     "Sec-Fetch-Dest": "document"},
            follow_redirects=True)
        if r.status_code != 200:
            return None

        soup = BeautifulSoup(r.text, "html.parser")

        # â”€â”€ Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        candidates: list[tuple[str, int]] = []  # (url, priority)

        # Priority 1: CSS selector Ù‡Ø§ÛŒ article/news
        for sel in _IMG_SELECTORS:
            for el in soup.select(sel)[:3]:
                src = None
                if el.name == "source":
                    src = el.get("srcset", "").split(" ")[0]
                else:
                    # srcset â†’ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ†
                    ss = el.get("srcset", "")
                    if ss:
                        parts = [p.strip().split(" ") for p in ss.split(",") if p.strip()]
                        best = sorted(parts, key=lambda x: int(x[1].rstrip("w")) if len(x)>1 and x[1].rstrip("w").isdigit() else 0, reverse=True)
                        if best: src = best[0][0]
                    if not src:
                        src = el.get("src") or el.get("data-src") or el.get("data-lazy-src")
                if src and not src.startswith("data:"):
                    candidates.append((src, 10))

        # Priority 2: og:image
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            candidates.append((og["content"], 5))

        # og:image:width Ø¨Ø±Ø±Ø³ÛŒ
        og_w = soup.find("meta", property="og:image:width")
        if og_w:
            try:
                w = int(og_w.get("content", 0))
                if w < 500 and candidates:
                    # og:image Ú©ÙˆÚ†Ú© Ø§Ø³Øª â†’ Ø§ÙˆÙ„ÙˆÛŒØª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
                    candidates = [(u, p-3 if u == og.get("content") else p) for u, p in candidates]
            except: pass

        # Priority 3: twitter:image
        for name in ("twitter:image", "twitter:image:src"):
            tw = soup.find("meta", attrs={"name": name})
            if tw and tw.get("content"):
                candidates.append((tw["content"], 4)); break

        if not candidates:
            return None

        # â”€â”€ ÙÛŒÙ„ØªØ± Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        from urllib.parse import urlparse
        base_p = urlparse(r.url)  # URL Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø¹Ø¯ Ø§Ø² redirect)

        # Ù…Ø±ØªØ¨ Ø§Ø² Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
        candidates.sort(key=lambda x: -x[1])
        tried_urls = set()

        for img_url, _ in candidates[:8]:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ URL
            if img_url.startswith("//"):
                img_url = "https:" + img_url
            elif img_url.startswith("/"):
                img_url = f"{base_p.scheme}://{base_p.netloc}{img_url}"
            elif not img_url.startswith("http"):
                continue

            # Ø­Ø°Ù query string Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
            clean_url = img_url.lower().split("?")[0]

            # ÙÛŒÙ„ØªØ± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù„ÙˆÚ¯Ùˆ Ø¯Ø± URL
            if any(p in clean_url for p in _SKIP_IMG_PATTERNS):
                log.debug(f"ğŸ–¼ skip-url: {img_url[:60]}")
                continue

            if img_url in tried_urls:
                continue
            tried_urls.add(img_url)

            # Ø¯Ø§Ù†Ù„ÙˆØ¯
            try:
                ir = await client.get(img_url,
                    timeout=httpx.Timeout(12.0),
                    headers={**COMMON_UA, "Accept": "image/*,*/*;q=0.5"},
                    follow_redirects=True)
                if ir.status_code != 200:
                    continue
            except Exception as de:
                log.debug(f"ğŸ–¼ dl-err: {de}"); continue

            raw   = ir.content
            ctype = ir.headers.get("content-type", "")

            # Ø­Ø¬Ù… Ú©Ù… â†’ Ù„ÙˆÚ¯Ùˆ
            if len(raw) < 15_000:
                log.debug(f"ğŸ–¼ skip-small: {len(raw)}B")
                continue

            # Ú†Ú© Ù†ÙˆØ¹ ØªØµÙˆÛŒØ±
            is_img = (
                ctype.startswith("image/") or
                raw[:3]  == b'\xff\xd8\xff' or
                raw[:8]  == b'\x89PNG\r\n\x1a\n' or
                raw[:6]  in (b'GIF87a', b'GIF89a') or
                raw[:4]  == b'RIFF' or
                raw[:4]  == b'WEBP'
            )
            if not is_img:
                continue

            # PIL: Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ resize
            if PIL_OK:
                try:
                    tmp = Image.open(io.BytesIO(raw))
                    w, h = tmp.size
                    # Ø¹Ø±Ø¶ < ÛµÛ°Û° ÛŒØ§ Ø§Ø±ØªÙØ§Ø¹ < Û²Û¸Û° â†’ Ù„ÙˆÚ¯Ùˆ/Ø¨Ù†Ø±
                    if w < 500 or h < 280:
                        log.debug(f"ğŸ–¼ skip-dim: {w}Ã—{h}")
                        continue
                    # Ù†Ø³Ø¨Øª < 1.3 â†’ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù…Ø±Ø¨Ø¹ ÛŒØ§ Ø¹Ù…ÙˆØ¯ÛŒ = Ù„ÙˆÚ¯Ùˆ
                    ratio = w / max(h, 1)
                    if ratio < 1.3:
                        log.debug(f"ğŸ–¼ skip-ratio: {ratio:.2f} ({w}Ã—{h})")
                        continue
                    img_rgb = tmp.convert("RGB")
                    if w > 1600 or h > 1000:
                        img_rgb.thumbnail((1600, 1000), Image.LANCZOS)
                    out = io.BytesIO()
                    img_rgb.save(out, "JPEG", quality=88, optimize=True)
                    out.seek(0)
                    log.info(f"ğŸ–¼ âœ… {w}Ã—{h} r={ratio:.1f}  {img_url[:55]}")
                    return out
                except Exception as pe:
                    log.debug(f"ğŸ–¼ PIL-err: {pe}"); continue
            else:
                buf = io.BytesIO(raw); buf.seek(0)
                return buf

        return None

    except Exception as e:
        log.debug(f"fetch_img {url[:55]}: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÛŒÚ© Ú†Ø±Ø®Ù‡ fetch â†’ filter â†’ send
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def _run_cycle(client: httpx.AsyncClient,
                     seen: set, stories: list,
                     cutoff: datetime) -> tuple:
    """
    ÛŒÚ© Ú†Ø±Ø®Ù‡ Ú©Ø§Ù…Ù„.
    Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯: (seen, stories, cutoff_for_next)
    """
    cycle_start = datetime.now(timezone.utc)
    save_run_state()

    # â”€â”€ fetch Ù…ÙˆØ§Ø²ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    flight_task = asyncio.create_task(fetch_military_flights(client))
    raw_task    = asyncio.create_task(fetch_all(client, cutoff))
    (flight_msgs, flight_aircraft), raw = await asyncio.gather(flight_task, raw_task)
    log.info(f"  ğŸ“¥ {len(raw)} Ø®Ø§Ù…  âœˆï¸ {len(flight_aircraft)} Ø¬Ù†Ú¯Ù†Ø¯Ù‡")

    # â”€â”€ Ù¾Ø±Ø¯Ø§Ø²Ø´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    collected = []
    cnt_old = cnt_irrel = cnt_dup = cnt_story = 0

    for entry, src_name, src_type, is_emb in raw:
        eid = make_id(entry)
        if eid in seen:                         cnt_dup   += 1; continue
        if not is_fresh(entry, cutoff):         cnt_old   += 1; continue
        t   = clean_html(entry.get("title",""))
        s   = clean_html(entry.get("summary") or entry.get("description") or "")
        if not is_war_relevant(f"{t} {s}", is_embassy=is_emb,
                               is_tg=(src_type=="tg"), is_tw=(src_type=="tw")):
            cnt_irrel += 1; continue
        if is_story_dup(t, stories):            cnt_story += 1; continue
        collected.append((eid, entry, src_name, src_type, is_emb))
        stories = register_story(t, stories)

    log.info(f"  ğŸ“Š Ù‚Ø¯ÛŒÙ…ÛŒ:{cnt_old} Ù†Ø§Ù…Ø±ØªØ¨Ø·:{cnt_irrel} dup:{cnt_dup} story:{cnt_story} âœ…{len(collected)}")

    collected = list(reversed(collected))[:MAX_NEW_PER_RUN]

    # â”€â”€ Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ù‡Ø§ÛŒ Ø¬Ù†Ú¯ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if flight_aircraft:
        map_buf = make_flight_map(flight_aircraft)
        if map_buf:
            regions = set(a["region"] for a in flight_aircraft)
            cap = [f"âœˆï¸ <b>ØªØ­Ø±Ú©Ø§Øª Ù‡ÙˆØ§ÛŒÛŒ Ù†Ø¸Ø§Ù…ÛŒ â€” {' | '.join(regions)}</b>"]
            for ac in flight_aircraft[:8]:
                cap.append(f"â€¢ <code>{ac['callsign']}</code> ({ac['type']}) "
                           f"alt:{int(ac['alt'])//1000 if ac['alt'] else '?'}k  "
                           f"{ac['gs']}kt â€” {ac['region']}")
            cap.append(f"\nğŸ• {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')}")
            await tg_send_photo(client, map_buf, "\n".join(cap))
            await asyncio.sleep(0.8)
        else:
            for msg in flight_msgs[:4]:
                await tg_send_text(client, msg); await asyncio.sleep(0.5)
    elif flight_msgs:
        for msg in flight_msgs[:2]:
            await tg_send_text(client, msg); await asyncio.sleep(0.5)

    if not collected:
        log.info("  ğŸ’¤ Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ù†ÛŒØ³Øª")
        save_seen(seen); save_stories(stories)
        return seen, stories, cycle_start

    # â”€â”€ ØªØ±Ø¬Ù…Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    arts_in = [
        (trim(clean_html(e.get("title","")), 400),
         trim(clean_html(e.get("summary") or e.get("description") or ""), 600))
        for _, e, _, _, _ in collected
    ]
    log.info(f"  ğŸŒ ØªØ±Ø¬Ù…Ù‡ {len(arts_in)} Ø®Ø¨Ø±...")
    translations = await translate_batch(client, arts_in)

    # â”€â”€ Ø§Ø±Ø³Ø§Ù„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    sent = 0
    for i, (eid, entry, src_name, stype, is_emb) in enumerate(collected):
        fa_title, fa_body = translations[i]
        en_title = arts_in[i][0]
        link     = entry.get("link","")
        dt_str   = format_dt(entry)

        title_is_fa = _is_farsi(fa_title) if fa_title else False
        orig_is_fa  = _is_farsi(en_title)
        if not title_is_fa and not orig_is_fa:
            log.info(f"  â­ skip(noFA): {en_title[:50]}"); continue

        display = fa_title.strip() if title_is_fa else en_title.strip()
        body_fa = ""
        if fa_body and _is_farsi(fa_body) and len(fa_body) > 15:
            body_fa = fa_body.strip()
        elif _is_farsi(arts_in[i][1]):
            body_fa = arts_in[i][1].strip()

        s_bar = sentiment_bar(analyze_sentiment(f"{fa_title} {fa_body} {en_title}"))
        cap   = [s_bar, f"<b>{esc(display)}</b>"]
        if body_fa and body_fa[:50] not in display[:50]:
            cap += ["", esc(trim(body_fa, 800))]
        if dt_str: cap.append(f"\nğŸ• {dt_str}")
        caption = "\n".join(cap)

        done = False
        if link and stype == "rss":
            img = await fetch_article_image(client, link)
            if img:
                ok = await tg_send_photo(client, img, caption[:1024])
                if ok: done = True; log.info("    ğŸ“¸ ØªØµÙˆÛŒØ±+ÙØ§Ø±Ø³ÛŒ")

        if not done:
            ok = await tg_send_text(client, caption)
            if ok: done = True; log.info("    âœ‰ï¸ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ")

        if done:
            seen.add(eid); sent += 1
        await asyncio.sleep(SEND_DELAY)

    save_seen(seen); save_stories(stories)
    log.info(f"  ğŸ {sent}/{len(collected)} Ø§Ø±Ø³Ø§Ù„  seen:{len(seen)}")
    return seen, stories, cycle_start


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# main â€” Ø­Ù„Ù‚Ù‡ Ø¯Ø§Ø¦Ù…ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    global _TW_SEMA
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡!"); return

    _TW_SEMA = asyncio.Semaphore(20)

    # cutoff Ø§ÙˆÙ„ÛŒÙ‡
    last_run = load_run_state()
    now_utc  = datetime.now(timezone.utc)
    cutoff   = last_run - timedelta(minutes=CUTOFF_BUFFER_MIN)
    if cutoff < now_utc - timedelta(minutes=MAX_LOOKBACK_MIN):
        cutoff = now_utc - timedelta(minutes=MAX_LOOKBACK_MIN)

    seen    = load_seen()
    stories = load_stories()

    mode = "GitHub CI" if _CI else "Ù…Ø­Ù„ÛŒ â€” Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª"
    log.info("=" * 70)
    log.info(f"ğŸš€ WarBot v19 | {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù† %Y/%m/%d')}")
    log.info(f"   mode={mode}  max={BOT_MAX_RUNTIME_MIN}min  interval={LOOP_INTERVAL_SEC}s")
    log.info(f"   ğŸ“¡ {len(ALL_RSS_FEEDS)} RSS  ğŸ“¢ {len(TELEGRAM_CHANNELS)} TG  ğ• {len(TWITTER_HANDLES)} TW")
    log.info(f"   seen:{len(seen)}  stories:{len(stories)}  PIL:{'âœ…' if PIL_OK else 'âŒ'}")
    log.info("=" * 70)

    wall_start = datetime.now(timezone.utc)
    loop_n     = 0
    limits     = httpx.Limits(max_connections=100, max_keepalive_connections=30)

    async with httpx.AsyncClient(follow_redirects=True, limits=limits) as client:
        await build_twitter_pools(client)

        while True:
            loop_n += 1
            elapsed_min = (datetime.now(timezone.utc) - wall_start).total_seconds() / 60
            log.info(f"\n{'â”'*55}")
            log.info(f"  âŸ³ Loop #{loop_n}  elapsed={elapsed_min:.1f}min"
                     f"  {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')}")

            t0 = datetime.now(timezone.utc)
            try:
                seen, stories, next_cutoff = await _run_cycle(
                    client, seen, stories, cutoff)
                # cutoff Ø¨Ø¹Ø¯ÛŒ = Ø´Ø±ÙˆØ¹ Ø§ÛŒÙ† cycle - buffer
                cutoff = next_cutoff - timedelta(minutes=CUTOFF_BUFFER_MIN)
            except Exception as e:
                log.error(f"  âŒ cycle error: {e}")
                import traceback; log.debug(traceback.format_exc())

            took = (datetime.now(timezone.utc) - t0).total_seconds()
            log.info(f"  â± cycle took {took:.0f}s")

            # Ø¨Ø±Ø±Ø³ÛŒ exit Ø¨Ø±Ø§ÛŒ CI
            elapsed_min = (datetime.now(timezone.utc) - wall_start).total_seconds() / 60
            if elapsed_min >= BOT_MAX_RUNTIME_MIN:
                log.info(f"  â¹ CI timeout ({BOT_MAX_RUNTIME_MIN}min) â€” Ø®Ø±ÙˆØ¬ Ø³Ø§Ù„Ù…")
                break

            # ØµØ¨Ø± ØªØ§ cycle Ø¨Ø¹Ø¯ÛŒ
            wait = max(5.0, LOOP_INTERVAL_SEC - took)
            log.info(f"  ğŸ’¤ {wait:.0f}s ØªØ§ Ú†Ø±Ø®Ù‡ Ø¨Ø¹Ø¯ÛŒ...")
            await asyncio.sleep(wait)


if __name__ == "__main__":
    asyncio.run(main())
