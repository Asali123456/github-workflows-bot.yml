import os, json, hashlib, asyncio, logging
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz
from deep_translator import GoogleTranslator

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN   = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID  = os.environ.get("CHANNEL_ID", "")
SEEN_FILE   = "seen.json"
MAX_NEW_PER_RUN = 60          
SEND_DELAY  = 3               
MAX_MSG_LEN = 4000
TEHRAN_TZ   = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û±. ÙÛŒØ¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ (ØªØ§ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Ùˆ Ø³Ø§Ù„Ù…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [
    # Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    {"name": "ğŸŒ Axios NatSec",       "url": "https://api.axios.com/feed/national-security"},
    {"name": "ğŸŒ Reuters Defense",    "url": "https://feeds.reuters.com/reuters/worldNews"},
    {"name": "ğŸŒ CNN Middle East",    "url": "http://rss.cnn.com/rss/edition_meast.rss"},
    {"name": "ğŸŒ Fox News World",     "url": "https://moxie.foxnews.com/google-publisher/world.xml"},
    {"name": "ğŸŒ Al Jazeera",         "url": "https://www.aljazeera.com/xml/rss/all.xml"},
    {"name": "ğŸŒ AP Defense",         "url": "https://apnews.com/hub/military-and-defense?format=rss"},
    {"name": "ğŸŒ Politico Defense",   "url": "https://rss.politico.com/defense.xml"},
    
    # Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ù†Ø¸Ø§Ù…ÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§
    {"name": "ğŸ‡ºğŸ‡¸ Pentagon News",     "url": "https://www.defense.gov/DesktopModules/ArticleCS/RSS.ashx?ContentType=1&Site=945&max=10"},
    {"name": "ğŸ‡ºğŸ‡¸ CENTCOM",           "url": "https://www.centcom.mil/RSS/"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",  "url": "https://breakingdefense.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense News",      "url": "https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"name": "ğŸ‡ºğŸ‡¸ The War Zone",      "url": "https://www.thedrive.com/feeds/the-war-zone"},
    {"name": "ğŸ‡ºğŸ‡¸ Military.com",      "url": "https://www.military.com/RSS/News/Defense.rss"},
    {"name": "ğŸ‡ºğŸ‡¸ Stars & Stripes",   "url": "https://www.stripes.com/arc/outboundfeeds/rss/?outputType=xml"},
    {"name": "ğŸ‡ºğŸ‡¸ C4ISRNet",          "url": "https://www.c4isrnet.com/arc/outboundfeeds/rss/"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense One",       "url": "https://www.defenseone.com/rss/all/"},
    
    # Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    {"name": "ğŸ‡®ğŸ‡± IDF Official",      "url": "https://www.idf.il/en/mini-sites/idf-spokesperson-english/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Jerusalem Post",    "url": "https://www.jpost.com/rss/rssfeedsmilitary.aspx"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",   "url": "https://www.timesofisrael.com/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Haaretz",          "url": "https://www.haaretz.com/cmlink/1.4455099"},
    {"name": "ğŸ‡®ğŸ‡± Ynetnews",          "url": "https://www.ynetnews.com/category/3082/feed"},
    {"name": "ğŸ‡®ğŸ‡± i24 News",          "url": "https://www.i24news.tv/en/rss"},
    
    # Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† Ùˆ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡
    {"name": "ğŸ‡®ğŸ‡· Iran International","url": "https://www.iranintl.com/en/rss"},
    {"name": "ğŸ‡®ğŸ‡· Radio Farda",       "url": "https://www.radiofarda.com/api/zmqpqopvp"},
    {"name": "ğŸŒ Middle East Eye",    "url": "https://www.middleeasteye.net/rss"},
    {"name": "ğŸŒ Al Monitor",         "url": "https://www.al-monitor.com/rss.xml"},
    
    # Ø§Ù†Ø¯ÛŒØ´Ú©Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ
    {"name": "ğŸ” ISW (War Study)",   "url": "https://www.understandingwar.org/rss.xml"},
    {"name": "ğŸ” Bellingcat",        "url": "https://www.bellingcat.com/feed/"},
    {"name": "ğŸ” CSIS",              "url": "https://www.csis.org/rss"},
    {"name": "ğŸ” Long War Journal",  "url": "https://www.longwarjournal.org/feed"},
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û². Ø¬Ø³ØªØ¬ÙˆÚ¯Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ú¯ÙˆÚ¯Ù„ Ù†ÛŒÙˆØ² (Ù¾ÙˆØ´Ø´ Ù‡Ø²Ø§Ø±Ø§Ù† Ø³Ø§ÛŒØª Ø®Ø¨Ø±ÛŒ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GOOGLE_NEWS_QUERIES = [
    # ØªÙ†Ø´ Ø§ÛŒØ±Ø§Ù† Ùˆ Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„
    ("âš”ï¸ Iran Israel Attack",       "Iran Israel military attack strike revenge"),
    ("âš”ï¸ IDF Strike Iran",          "IDF airstrike Iran IRGC base facilities"),
    ("âš”ï¸ Mossad Operation",         "Mossad covert operation assassination Iran"),
    ("âš”ï¸ Iran Drone Attack",        "Iran drone Shahed ballistic missile attack Israel"),
    
    # Ø¢Ù…Ø±ÛŒÚ©Ø§ Ùˆ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡
    ("âš”ï¸ US Forces Attacked",       "US forces attacked base Iraq Syria CENTCOM"),
    ("âš”ï¸ Pentagon Iran",            "Pentagon warning Iran military action"),
    ("âš”ï¸ US Navy Middle East",      "US Navy carrier strike group 5th Fleet Red Sea Gulf"),
    
    # Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ Ù†ÛŒØ§Ø¨ØªÛŒ
    ("âš”ï¸ Hezbollah Conflict",       "Hezbollah IDF border strike Lebanon rockets"),
    ("âš”ï¸ Houthis Red Sea",          "Houthis Red Sea attack ship US Navy strike"),
    ("âš”ï¸ IRGC Quds Force",          "IRGC Quds Force Syria Iraq weapons smuggling"),
    
    # Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ ØªØ³Ù„ÛŒØ­Ø§ØªÛŒ
    ("â˜¢ï¸ Iran Nuclear",             "Iran nuclear enrichment Natanz Fordow IAEA centrifuge"),
    ("ğŸš€ Hypersonic Missile",       "Iran hypersonic ballistic missile test aerospace"),
    ("ğŸ›¡ï¸ Iron Dome/Arrow",         "Israel Iron Dome Arrow David Sling interception"),
    
    # Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ…Ø±Ú©Ø² Ø¯Ø± Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ (Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ)
    ("ğŸ“° NYT Iran Military",        "site:nytimes.com Iran Israel military"),
    ("ğŸ“° WSJ NatSec",               "site:wsj.com Iran US military defense"),
]

def google_news_url(query: str) -> str:
    q = query.replace(" ", "+")
    return f"https://news.google.com/rss/search?q={q}&hl=en-US&gl=US&ceid=US:en&num=15"

GOOGLE_FEEDS = [{"name": name, "url": google_news_url(q), "is_google": True} for name, q in GOOGLE_NEWS_QUERIES]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û³. ØªÙˆÛŒÛŒØªØ± / Ø´Ø¨Ú©Ù‡ X (Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±Ù‡Ø§)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_ACCOUNTS = [
    # Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ø§Ø±Ø´Ø¯
    ("ğŸ“° Barak Ravid (Axios)",      "BarakRavid"),
    ("ğŸ“° Natasha Bertrand (CNN)",   "NatashaBertrand"),
    ("ğŸ“° Idrees Ali (Reuters)",     "idreesali114"),
    ("ğŸ“° Farnaz Fassihi (NYT)",     "farnazfassihi"),
    ("ğŸ“° Emanuel Fabian (TOI)",     "manniefabian"),
    ("ğŸ“° Trey Yingst (Fox)",        "TreyYingst"),
    ("ğŸ“° Joe Truzman (FDD)",        "JoeTruzman"),
    
    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø² (OSINT) Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø¬Ù†Ú¯
    ("ğŸ” OSINT Defender",    "OSINTdefender"),
    ("ğŸ” Intel Crab",        "IntelCrab"),
    ("ğŸ” Aurora Intel",      "AuroraIntel"),
    ("ğŸ” Clash Report",      "clashreport"),
    ("ğŸ” Faytuks News",      "Faytuks"),
    ("ğŸ” Global: Military",  "Global_Mil_Info"),
    ("ğŸ” War Monitor",       "WarMonitor3"),
    
    # Ù…Ù‚Ø§Ù…Ø§Øª Ø±Ø³Ù…ÛŒ
    ("ğŸ‡®ğŸ‡± IDF Official",    "IDF"),
    ("ğŸ‡ºğŸ‡¸ CENTCOM",         "CENTCOM"),
    ("ğŸ‡ºğŸ‡¸ US Dept Defense", "DeptofDefense"),
]

NITTER_MIRRORS = [
    "https://nitter.poast.org",
    "https://nitter.privacydev.net",
    "https://nitter.1d4.us",
]

def get_nitter_feeds() -> list[dict]:
    feeds = []
    for name, handle in TWITTER_ACCOUNTS:
        for mirror in NITTER_MIRRORS:
            feeds.append({"name": f"ğ• {name}", "url": f"{mirror}/{handle}/rss", "nitter_handle": handle})
            break 
    return feeds

NITTER_FEEDS = get_nitter_feeds()
ALL_FEEDS = RSS_FEEDS + GOOGLE_FEEDS + NITTER_FEEDS

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙˆØ§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ (ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù‚Ø·Ø¹ÛŒ Ùˆ Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def is_fresh_news(entry: dict) -> bool:
    """ ÙÙ‚Ø· Ø®Ø¨Ø±Ù‡Ø§ÛŒ 21 ÙÙˆØ±ÛŒÙ‡ 2026 Ø¨Ù‡ Ø¨Ø¹Ø¯ Ùˆ Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ 24 Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ """
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t: return True 
        
        dt = datetime(*t[:6], tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        
        # Û±. ÙÛŒÙ„ØªØ± Ù‚Ø·Ø¹ÛŒ: Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ù‚Ø¨Ù„ Ø§Ø² 21 ÙÙˆØ±ÛŒÙ‡ 2026 ØªØ§ÛŒÛŒØ¯ Ù†Ø´ÙˆØ¯
        cutoff = datetime(2026, 2, 21, tzinfo=timezone.utc)
        if dt < cutoff:
            return False
            
        # Û². Ø®Ø¨Ø± Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø² 24 Ø³Ø§Ø¹Øª Ù¾ÛŒØ´ Ø¨Ø§Ø´Ø¯
        if (now - dt) > timedelta(hours=24):
            return False
            
        return True
    except:
        return True

def is_relevant(entry: dict, is_twitter: bool = False) -> bool:
    text = " ".join([
        str(entry.get("title", "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()
    
    # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø±Ø¨ÙˆØ· Ø³ÛŒØ§Ø³ÛŒ Ø¯Ø§Ø®Ù„ÛŒ ÛŒØ§ Ø§Ù‚ØªØµØ§Ø¯ÛŒ
    if is_twitter:
        if any(kw in text for kw in ["iran", "israel", "us ", "strike", "war", "gaza", "lebanon", "irgc", "idf", "military", "attack", "missile", "hezbollah", "houthi"]):
            return True
        return False
        
    KEYWORDS = ["iran", "irgc", "tehran", "khamenei", "israel", "idf", "mossad", "tel aviv", "netanyahu",
                "us forces", "centcom", "pentagon", "american base", "strike", "airstrike", "drone", "missile", "war", "Ø­Ù…Ù„Ù‡", "Ù†Ø¸Ø§Ù…ÛŒ", "Ø³Ù¾Ø§Ù‡"]
    return any(kw in text for kw in KEYWORDS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ø²Ù…Ø§Ù† ÙÛŒØ¯Ù‡Ø§ (Asynchronous Fetching) - Ø³Ø±Ø¹Øª Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_single_feed(client: httpx.AsyncClient, cfg: dict) -> list:
    url = cfg["url"]
    try:
        response = await client.get(url, timeout=15.0, headers={"User-Agent": "Mozilla/5.0 MilNewsBot/5.0"})
        if response.status_code == 200:
            parsed = feedparser.parse(response.text)
            return parsed.entries
    except Exception as e:
        # Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ø§Ø±ÙˆØ±Ù‡Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒØ´ÙˆØ¯ ØªØ§ ØµÙØ­Ù‡ Ø´Ù„ÙˆØº Ù†Ø´ÙˆØ¯
        pass
    return []

async def fetch_all_feeds_concurrently(client: httpx.AsyncClient, feeds: list) -> list:
    tasks = [fetch_single_feed(client, cfg) for cfg in feeds]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    entries_with_cfg = []
    for i, entries in enumerate(results):
        if isinstance(entries, list):
            for entry in entries:
                entries_with_cfg.append((entry, feeds[i]))
    return entries_with_cfg

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…ÙˆØªÙˆØ± ØªØ±Ø¬Ù…Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def translate_to_fa(text: str) -> str:
    if not text or len(text.strip()) < 3:
        return ""
    try:
        # Ø¯Ø± ØµÙˆØ±Øª Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÙˆØ¯Ù† Ù…ØªÙ†ØŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø±ÙˆØ± Ù…ØªØ±Ø¬Ù… Ú©ÙˆØªØ§Ù‡ Ù…ÛŒØ´ÙˆØ¯
        text = text[:4000]
        return GoogleTranslator(source='auto', target='fa').translate(text)
    except Exception:
        return text 

def clean_html(text: str) -> str:
    if not text: return ""
    return BeautifulSoup(str(text), "html.parser").get_text(" ", strip=True)

def truncate(text: str, n: int = 300) -> str:
    if len(text) <= n: return text
    return text[:n].rsplit(" ", 1)[0] + "â€¦"

def make_id(entry: dict) -> str:
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def format_dt(entry: dict) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except:
        pass
    return ""

def escape_html(text: str) -> str:
    return text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def build_message(entry: dict, source: str, is_twitter: bool = False) -> str:
    en_title   = clean_html(entry.get("title", "No Title")).strip()
    en_summary = clean_html(entry.get("summary") or entry.get("description") or "")
    link       = entry.get("link", "")
    dt         = format_dt(entry)

    fa_title = escape_html(translate_to_fa(en_title))
    fa_summary_short = escape_html(translate_to_fa(truncate(en_summary, 350)))
    en_title_escaped = escape_html(en_title)

    icon = "ğ•" if is_twitter else "ğŸ“¡"

    lines = [f"ğŸ”´ <b>{fa_title}</b>", ""]
    
    if fa_summary_short and fa_summary_short.lower() not in fa_title.lower():
        lines += [f"ğŸ”¹ <i>{fa_summary_short}</i>", ""]
        
    lines += [
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
        f"ğŸ‡ºğŸ‡¸ <b>Ù…ØªÙ† Ø§ØµÙ„ÛŒ:</b>",
        f"<blockquote expandable>{en_title_escaped}</blockquote>"
    ]

    if dt: lines.append(dt)
    lines.append(f"{icon} <b>{source}</b>")
    if link: lines.append(f'ğŸ”— <a href="{link}">Ù„ÛŒÙ†Ú© Ø®Ø¨Ø± Ø§ØµÙ„ÛŒ</a>')

    return "\n".join(lines)

def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen: set):
    recent = list(seen)[-15000:] # Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù‡ Û±Ûµ Ù‡Ø²Ø§Ø± Ø®Ø¨Ø± Ø¨Ø®Ø§Ø·Ø± Ù…Ù†Ø§Ø¨Ø¹ Ø²ÛŒØ§Ø¯
    with open(SEEN_FILE, "w") as f: json.dump(recent, f)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client: httpx.AsyncClient, text: str) -> bool:
    for attempt in range(4):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID,
                "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            }, timeout=25)
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                wait = data.get("parameters", {}).get("retry_after", 30)
                await asyncio.sleep(wait)
            else:
                return False
        except Exception:
            await asyncio.sleep(8)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø¨Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ ØªÙˆÚ©Ù† Ø¨Ø§Øª ÛŒØ§ Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        return

    seen = load_seen()
    
    log.info(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² {len(ALL_FEEDS)} Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ...")
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ø²Ù…Ø§Ù† ØªÙ…Ø§Ù… Ø®Ø¨Ø±Ù‡Ø§ Ø¯Ø± Ú†Ù†Ø¯ Ø«Ø§Ù†ÛŒÙ‡
        raw_entries = await fetch_all_feeds_concurrently(client, ALL_FEEDS)
        
        collected: list[tuple] = [] 

        # Ù…Ø±Ø­Ù„Ù‡ Û²: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†
        for entry, cfg in raw_entries:
            is_tw = bool(cfg.get("nitter_handle"))
            eid = make_id(entry)
            
            if eid in seen:
                continue
            
            # ÙÛŒÙ„ØªØ± ØªØ§Ø±ÛŒØ®: ÙÙ‚Ø· Û²Û± ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶ Ø¨Ù‡ Ø¨Ø¹Ø¯
            if not is_fresh_news(entry):
                seen.add(eid)
                continue
            
            # ÙÛŒÙ„ØªØ± Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
            if not is_relevant(entry, is_twitter=is_tw):
                seen.add(eid)
                continue
                
            collected.append((eid, entry, cfg, is_tw))

        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø®Ø¨Ø±Ù‡Ø§ Ø§Ø² Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
        collected = collected[::-1]

        if len(collected) > MAX_NEW_PER_RUN:
            collected = collected[-MAX_NEW_PER_RUN:]

        # Ù…Ø±Ø­Ù„Ù‡ Û³: ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„
        sent = 0
        for eid, entry, cfg, is_tw in collected:
            msg = build_message(entry, cfg["name"], is_tw)
            if await tg_send(client, msg):
                seen.add(eid)
                sent += 1
                log.info(f"  âœ… [{cfg['name']}] Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"âœ”ï¸ Ù¾Ø§ÛŒØ§Ù† | {sent} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ (Ø§Ù…Ø±ÙˆØ² Ø¨Ù‡ Ø¨Ø¹Ø¯) Ø§Ø² Ø¯Ù‡â€ŒÙ‡Ø§ Ù…Ù†Ø¨Ø¹ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")

if __name__ == "__main__":
    asyncio.run(main())
