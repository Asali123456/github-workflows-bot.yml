"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ›¡ï¸ Military Intel Bot â€” AI LLM Translation Edition              â•‘
â•‘     Iran Â· Israel Â· USA  |  RSSHub + Google News + Twitter/X            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os, json, hashlib, asyncio, logging
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz
import google.generativeai as genai

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE   = "seen.json"
MAX_NEW_PER_RUN = 30          
SEND_DELAY  = 5  # ØªØ§Ø®ÛŒØ± Ûµ Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø±Ø¹Ø§ÛŒØª Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø±Ø§ÛŒÚ¯Ø§Ù† Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ (15 RPM)
TEHRAN_TZ   = pytz.timezone("Asia/Tehran")

# Ú©Ø§Ù†ÙÛŒÚ¯ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú¯ÙˆÚ¯Ù„
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹ Ùˆ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ÙÙ„Ø´
    generation_config = {"temperature": 0.2, "top_p": 0.95} 
    ai_model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)
else:
    ai_model = None
    log.error("âš ï¸ GEMINI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. ØªØ±Ø¬Ù…Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û±. Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†â€ŒØ¨Ø§Ø² Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [
    {"name": "ğŸŒ Axios NatSec",       "url": "https://api.axios.com/feed/national-security"},
    {"name": "ğŸŒ Reuters Defense",    "url": "https://feeds.reuters.com/reuters/worldNews"},
    {"name": "ğŸŒ CNN Middle East",    "url": "http://rss.cnn.com/rss/edition_meast.rss"},
    {"name": "ğŸŒ Fox News World",     "url": "https://moxie.foxnews.com/google-publisher/world.xml"},
    {"name": "ğŸŒ Al Jazeera",         "url": "https://www.aljazeera.com/xml/rss/all.xml"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",  "url": "https://breakingdefense.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense News",      "url": "https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"name": "ğŸ‡®ğŸ‡± IDF Official",      "url": "https://www.idf.il/en/mini-sites/idf-spokesperson-english/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",   "url": "https://www.timesofisrael.com/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Haaretz",          "url": "https://www.haaretz.com/cmlink/1.4455099"},
    {"name": "ğŸ‡®ğŸ‡· Iran International","url": "https://www.iranintl.com/en/rss"},
    {"name": "ğŸ” ISW (War Study)",   "url": "https://www.understandingwar.org/rss.xml"},
]

# Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯ÙˆÚ¯Ù„ Ù†ÛŒÙˆØ²
GOOGLE_NEWS_QUERIES = [
    ("âš”ï¸ Iran Israel Attack",       "Iran Israel military attack strike revenge"),
    ("âš”ï¸ IDF Strike Iran",          "IDF airstrike Iran IRGC base facilities"),
    ("âš”ï¸ US Forces Attacked",       "US forces attacked base Iraq Syria CENTCOM"),
    ("âš”ï¸ Hezbollah Conflict",       "Hezbollah IDF border strike Lebanon rockets"),
]

def google_news_url(query: str) -> str:
    return f"https://news.google.com/rss/search?q={query.replace(' ', '+')}&hl=en-US&gl=US&ceid=US:en&num=15"

GOOGLE_FEEDS = [{"name": name, "url": google_news_url(q), "is_google": True} for name, q in GOOGLE_NEWS_QUERIES]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û². ØªÙˆÛŒÛŒØªØ± Ø§Ø² Ø·Ø±ÛŒÙ‚ RSSHub (Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ØªØ±ÛŒÙ† Ù¾Ù„ØªÙØ±Ù… Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨) Ùˆ Nitter
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_ACCOUNTS = [
    ("ğŸ“° Barak Ravid",      "BarakRavid"),
    ("ğŸ“° Natasha Bertrand", "NatashaBertrand"),
    ("ğŸ“° Idrees Ali",       "idreesali114"),
    ("ğŸ“° Farnaz Fassihi",   "farnazfassihi"),
    ("ğŸ” OSINT Defender",   "OSINTdefender"),
    ("ğŸ” Intel Crab",       "IntelCrab"),
    ("ğŸ” War Monitor",      "WarMonitor3"),
    ("ğŸ‡®ğŸ‡± IDF Official",   "IDF"),
    ("ğŸ‡ºğŸ‡¸ CENTCOM",        "CENTCOM"),
]

# ØªØ±Ú©ÛŒØ¨ RSSHub (Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø±ØªØ± Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨) Ùˆ Nitter Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù‡ÛŒÚ† ØªÙˆÛŒÛŒØªÛŒ Ù…Ø³Ø¯ÙˆØ¯ Ù†Ø´ÙˆØ¯
TWITTER_MIRRORS = [
    "https://rsshub.app/twitter/user",     # RSSHub Ø§ØµÙ„ÛŒ
    "https://nitter.poast.org",            # Nitter Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
    "https://nitter.privacydev.net",       # Nitter Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Û²
]

def get_twitter_feeds() -> list[dict]:
    feeds = []
    for name, handle in TWITTER_ACCOUNTS:
        for mirror in TWITTER_MIRRORS:
            url = f"{mirror}/{handle}" if "rsshub" in mirror else f"{mirror}/{handle}/rss"
            feeds.append({"name": f"ğ• {name}", "url": url, "nitter_handle": handle})
            break # Ø§ÙˆÙ„ÛŒ Ø±Ùˆ Ø¨Ø±Ù…ÛŒØ¯Ø§Ø±Ù‡ØŒ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø±Ø§Ø¨ÛŒ Ø¨Ø¹Ø¯Ø§ ØªÙˆ ØªØ§Ø¨Ø¹ fetch Ù‡Ù†Ø¯Ù„ Ù…ÛŒØ´Ù‡
    return feeds

ALL_FEEDS = RSS_FEEDS + GOOGLE_FEEDS + get_twitter_feeds()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û³. ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def is_fresh_news(entry: dict) -> bool:
    """ ÙÙ‚Ø· Ø®Ø¨Ø±Ù‡Ø§ÛŒ 21 ÙÙˆØ±ÛŒÙ‡ 2026 Ø¨Ù‡ Ø¨Ø¹Ø¯ Ùˆ Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ 24 Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ """
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t: return True 
        
        dt = datetime(*t[:6], tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        
        # Û±. ÙÛŒÙ„ØªØ± Ù‚Ø·Ø¹ÛŒ ØªØ§Ø±ÛŒØ® Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ Ú©Ø§Ø±Ø¨Ø±: 21 Feb 2026
        cutoff = datetime(2026, 2, 21, tzinfo=timezone.utc)
        if dt < cutoff:
            return False
            
        # Û². ÙÛŒÙ„ØªØ± Û²Û´ Ø³Ø§Ø¹Øª: Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø² Û²Û´ Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ Ø±Ø¯ Ù…ÛŒØ´Ù†
        if (now - dt) > timedelta(hours=24):
            return False
            
        return True
    except:
        return True

def is_relevant(entry: dict, is_twitter: bool = False) -> bool:
    text = " ".join([
        str(entry.get("title", "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()
    
    if is_twitter:
        if any(kw in text for kw in ["iran", "israel", "us ", "strike", "war", "gaza", "lebanon", "irgc", "idf", "military", "attack", "missile"]):
            return True
        return False
        
    KEYWORDS = ["iran", "irgc", "tehran", "khamenei", "israel", "idf", "mossad", "tel aviv", "netanyahu",
                "us forces", "centcom", "pentagon", "american base", "strike", "airstrike", "drone", "missile"]
    return any(kw in text for kw in KEYWORDS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û´. Ù…ØªØ±Ø¬Ù… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ (Google Gemini) - Ù„Ø­Ù† Ø®Ø¨Ø±ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def ai_translate(text: str) -> str:
    if not text or len(text.strip()) < 5 or not ai_model:
        return text
    
    prompt = f"""
Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ±Ø¬Ù… Ø§Ø±Ø´Ø¯ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ Ùˆ Ú˜Ø¦ÙˆÙ¾Ù„ÛŒØªÛŒÚ© Ù‡Ø³ØªÛŒØ¯.
Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù†ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¨Ø§ Ù„Ø­Ù† Ú©Ø§Ù…Ù„Ø§Ù‹ Ø®Ø¨Ø±ÛŒ ØªØ±Ø¬Ù…Ù‡ Ú©Ù†ÛŒØ¯.
Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† Ú©Ù„Ù…Ù‡ Ø§Ø¶Ø§ÙÙ‡ØŒ Ø¨Ø¯ÙˆÙ† Ø³Ù„Ø§Ù… Ùˆ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒØŒ Ùˆ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ø¯ÛŒ (Ù…Ø«Ù„ ```). ÙÙ‚Ø· Ù…ØªÙ† ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.

Ù…ØªÙ†:
{text}
    """
    try:
        response = await asyncio.to_thread(ai_model.generate_content, prompt)
        translated = response.text.strip().replace("```", "").strip()
        return translated if translated else text
    except Exception as e:
        log.error(f"Ø®Ø·Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ: {e}")
        return text

def clean_html(text: str) -> str:
    if not text: return ""
    return BeautifulSoup(str(text), "html.parser").get_text(" ", strip=True)

def make_id(entry: dict) -> str:
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„ÛŒÙ†Ú© Ø¨Ø±Ø§ÛŒ MD5 Ø¹Ø´Ø§Ù† Ø¹Ø¯Ù… Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def format_dt(entry: dict) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except:
        pass
    return ""

def escape_html(text: str) -> str:
    return text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ø²Ù…Ø§Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_single_feed(client: httpx.AsyncClient, cfg: dict) -> list:
    url = cfg["url"]
    try:
        response = await client.get(url, timeout=15.0, headers={"User-Agent": "Mozilla/5.0 MilNewsBot/6.0"})
        if response.status_code == 200:
            return feedparser.parse(response.text).entries
    except: pass
    return []

async def fetch_all_feeds_concurrently(client: httpx.AsyncClient, feeds: list) -> list:
    tasks = [fetch_single_feed(client, cfg) for cfg in feeds]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    entries_with_cfg = []
    for i, entries in enumerate(results):
        if isinstance(entries, list):
            for entry in entries:
                entries_with_cfg.append((entry, feeds[i]))
    return entries_with_cfg

def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen: set):
    recent = list(seen)[-15000:]
    with open(SEEN_FILE, "w") as f: json.dump(recent, f)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client: httpx.AsyncClient, text: str) -> bool:
    for _ in range(3):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID,
                "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            }, timeout=25)
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                await asyncio.sleep(data.get("parameters", {}).get("retry_after", 30))
            else:
                return False
        except Exception:
            await asyncio.sleep(5)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø¨Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ ØªÙˆÚ©Ù† Ø¨Ø§Øª ÛŒØ§ Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        return

    seen = load_seen()
    log.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹...")
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        raw_entries = await fetch_all_feeds_concurrently(client, ALL_FEEDS)
        collected = []

        for entry, cfg in raw_entries:
            is_tw = bool(cfg.get("nitter_handle"))
            eid = make_id(entry)
            
            if eid in seen: continue
            if not is_fresh_news(entry):
                seen.add(eid)
                continue
            if not is_relevant(entry, is_twitter=is_tw):
                seen.add(eid)
                continue
                
            collected.append((eid, entry, cfg, is_tw))

        collected = collected[::-1] # Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
        if len(collected) > MAX_NEW_PER_RUN:
            collected = collected[-MAX_NEW_PER_RUN:]

        sent = 0
        for eid, entry, cfg, is_tw in collected:
            # Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ
            en_title = clean_html(entry.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")).strip()
            en_summary = clean_html(entry.get("summary") or entry.get("description") or "")
            link = entry.get("link", "")
            dt = format_dt(entry)
            icon = "ğ•" if is_tw else "ğŸ“¡"

            # ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú¯ÙˆÚ¯Ù„
            fa_title = escape_html(await ai_translate(en_title))
            
            summary_short = en_summary[:400].rsplit(" ", 1)[0] + "â€¦" if len(en_summary) > 400 else en_summary
            fa_summary = escape_html(await ai_translate(summary_short))
            
            en_title_escaped = escape_html(en_title)

            # Ø³Ø§Ø®ØªØ§Ø± Ù¾ÛŒØ§Ù…
            lines = [f"ğŸ”´ <b>{fa_title}</b>", ""]
            if fa_summary and fa_summary.lower() not in fa_title.lower():
                lines += [f"ğŸ”¹ <i>{fa_summary}</i>", ""]
                
            lines += [
                "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
                f"ğŸ‡ºğŸ‡¸ <b>Ù…ØªÙ† Ø§ØµÙ„ÛŒ:</b>",
                f"<blockquote expandable>{en_title_escaped}</blockquote>"
            ]
            if dt: lines.append(dt)
            lines.append(f"{icon} <b>{cfg['name']}</b>")
            if link: lines.append(f'ğŸ”— <a href="{link}">Ù„ÛŒÙ†Ú© Ø®Ø¨Ø± Ø§ØµÙ„ÛŒ</a>')

            msg = "\n".join(lines)
            
            if await tg_send(client, msg):
                seen.add(eid)
                sent += 1
                log.info(f"  âœ… [{cfg['name']}] Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")
            
            # ØªØ§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø±Ø¹Ø§ÛŒØª Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø³Ø±Ø¹Øª ØªÙ„Ú¯Ø±Ø§Ù… Ùˆ API Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú¯ÙˆÚ¯Ù„
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"âœ”ï¸ Ù¾Ø§ÛŒØ§Ù† | {sent} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ (Ø§Ù…Ø±ÙˆØ² Ø¨Ù‡ Ø¨Ø¹Ø¯) Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")

if __name__ == "__main__":
    asyncio.run(main())
