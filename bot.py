import os, json, hashlib, asyncio, logging, re, io
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_OK = True
except ImportError:
    PIL_OK = False

try:
    from hazm import Normalizer as HazmNorm
    _hazm = HazmNorm()
    def nfa(t): return _hazm.normalize(t or "")
except ImportError:
    def nfa(t): return re.sub(r' +', ' ', (t or "").replace("ÙŠ","ÛŒ").replace("Ùƒ","Ú©")).strip()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("WarBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE         = "seen.json"
STORIES_FILE      = "stories.json"
GEMINI_STATE_FILE = "gemini_state.json"
RUN_STATE_FILE    = "run_state.json"
NITTER_CACHE_FILE = "nitter_cache.json"

# â”€â”€ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ø­Ù„Ù‚Ù‡ Ø¯Ø§Ø¦Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CUTOFF_BUFFER_MIN  = 4    # overlap â€” Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ø§Ø¬Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ù†
MAX_LOOKBACK_MIN   = 90   # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ø±Ú¯Ø´Øª (Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§ / crash)
SEEN_TTL_HOURS     = 6
NITTER_CACHE_TTL   = 900

LOOP_INTERVAL_SEC  = 60   # Ù‡Ø± Û¶Û° Ø«Ø§Ù†ÛŒÙ‡ â€” Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ fetch Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹
# Ø¯Ø± GitHub Actions: bot Ø±Ø§ Û³ÛµÛ° Ø¯Ù‚ÛŒÙ‚Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†ØŒ Actions Ù‡Ø± Û¶ Ø³Ø§Ø¹Øª restart Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø­Ù„ÛŒ (CI=False): Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
_CI = bool(os.environ.get("CI") or os.environ.get("GITHUB_ACTIONS"))
BOT_MAX_RUNTIME_MIN = 350 if _CI else 99999

MAX_NEW_PER_RUN    = 50   # Ù‡Ø± Ú†Ø±Ø®Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± ÛµÛ° Ø®Ø¨Ø±
MAX_MSG_LEN        = 4096
SEND_DELAY         = 0.3
JACCARD_THRESHOLD  = 0.62  # Ø¢Ø²Ø§Ø¯ â€” ÙÙ‚Ø· Ø®Ø¨Ø±Ù‡Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù† Ø±Ø¯ Ø´ÙˆÙ†Ø¯
MAX_STORIES        = 150   # Ú©Ù…ØªØ± = dedup Ù…Ø­Ø¯ÙˆØ¯ØªØ± = Ø®Ø¨Ø± Ø¨ÛŒØ´ØªØ±
RSS_TIMEOUT        = 8.0
TG_TIMEOUT         = 10.0
TW_TIMEOUT         = 6.0
RICH_CARD_THRESHOLD = 5

TEHRAN_TZ = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ù†Ø§Ø¨Ø¹ RSS â€” Feb 27 2026 â€” Ù…Ø°Ø§Ú©Ø±Ø§Øª Ú˜Ù†Ùˆ Ø¯ÙˆØ± Ø³ÙˆÙ… / Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IRAN_FEEDS = [
    # â”€â”€â”€ ÙØ§Ø±Ø³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡®ğŸ‡· Ø§ÛŒØ±Ù†Ø§",          "u":"https://www.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· ØªØ³Ù†ÛŒÙ…",         "u":"https://www.tasnimnews.com/fa/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Ù…Ù‡Ø±",           "u":"https://www.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· ÙØ§Ø±Ø³",          "u":"https://www.farsnews.ir/rss/fa"},
    {"n":"ğŸ‡®ğŸ‡· Ù…Ø´Ø±Ù‚",          "u":"https://www.mashreghnews.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Ø¯ÙØ§Ø¹ Ù¾Ø±Ø³",      "u":"https://www.defapress.ir/fa/rss"},
    {"n":"ğŸ‡®ğŸ‡· YJC",           "u":"https://www.yjc.ir/fa/rss/allnews"},
    # â”€â”€â”€ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡®ğŸ‡· IRNA EN",       "u":"https://en.irna.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Mehr EN",       "u":"https://en.mehrnews.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tasnim EN",     "u":"https://www.tasnimnews.com/en/rss/feed/0/8/0"},
    {"n":"ğŸ‡®ğŸ‡· Press TV",      "u":"https://www.presstv.ir/rss"},
    {"n":"ğŸ‡®ğŸ‡· Tehran Times",  "u":"https://www.tehrantimes.com/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran Intl EN",  "u":"https://www.iranintl.com/en/rss"},
    {"n":"ğŸ‡®ğŸ‡· Iran Wire",     "u":"https://iranwire.com/en/feed/"},
    {"n":"ğŸ‡®ğŸ‡· Radio Farda",   "u":"https://en.radiofarda.com/api/zqpqetrruqo"},
    # â”€â”€â”€ Google News ÙØ§Ø±Ø³ÛŒ â€” Ø§Ù…Ø±ÙˆØ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ“° GN Ú˜Ù†Ùˆ Ø§Ù…Ø±ÙˆØ²",   "u":"https://news.google.com/rss/search?q=Ø§ÛŒØ±Ø§Ù†+Ù…Ø°Ø§Ú©Ø±Ø§Øª+Ú˜Ù†Ùˆ+Ø¹Ø±Ø§Ù‚Ú†ÛŒ+ÙˆÛŒØªÚ©ÙˆÙ&hl=fa&gl=IR&ceid=IR:fa&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN Ø³Ù¾Ø§Ù‡ Ø§Ù…Ø±ÙˆØ²",  "u":"https://news.google.com/rss/search?q=Ø³Ù¾Ø§Ù‡+Ù¾Ø§Ø³Ø¯Ø§Ø±Ø§Ù†+Ø­Ù…Ù„Ù‡+Ù…ÙˆØ´Ú©+Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ&hl=fa&gl=IR&ceid=IR:fa&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GN Ø§Ø¹ØªØ±Ø§Ø¶ Ø§ÛŒØ±Ø§Ù†","u":"https://news.google.com/rss/search?q=Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª+Ø§ÛŒØ±Ø§Ù†+Ø³Ø±Ú©ÙˆØ¨+Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ+Û±Û´Û°Û´&hl=fa&gl=IR&ceid=IR:fa&num=10&tbs=qdr:d"},
]

ISRAEL_FEEDS = [
    {"n":"ğŸ‡®ğŸ‡± Times of Israel","u":"https://www.timesofisrael.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± Jerusalem Post", "u":"https://rss.jpost.com/rss/rssfeedsheadlines"},
    {"n":"ğŸ‡®ğŸ‡± Haaretz EN",     "u":"https://www.haaretz.com/srv/haaretz-latest-articles.rss"},
    {"n":"ğŸ‡®ğŸ‡± Israel Hayom",   "u":"https://www.israelhayom.com/feed/"},
    {"n":"ğŸ‡®ğŸ‡± i24 News",       "u":"https://www.i24news.tv/en/rss"},
    # â”€â”€â”€ Google News Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ“° GN Netanyahu",   "u":"https://news.google.com/rss/search?q=Netanyahu+Iran+nuclear+deal+war+2026&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN IDF Iran",    "u":"https://news.google.com/rss/search?q=IDF+Israel+Iran+strike+military&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]

USA_FEEDS = [
    # â”€â”€â”€ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡ºğŸ‡¸ AP World",        "u":"https://apnews.com/hub/world-news.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ AP Middle East",  "u":"https://apnews.com/hub/middle-east.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ AP Nuclear",      "u":"https://apnews.com/hub/nuclear-weapons.rss"},
    {"n":"ğŸ‡ºğŸ‡¸ NBC World",       "u":"https://feeds.nbcnews.com/feeds/worldnews"},
    {"n":"ğŸ‡ºğŸ‡¸ PBS NewsHour",    "u":"https://www.pbs.org/newshour/feed"},
    # â”€â”€â”€ Ù†Ø¸Ø§Ù…ÛŒ/Ø¯ÙØ§Ø¹ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡ºğŸ‡¸ USNI News",       "u":"https://news.usni.org/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Breaking Defense","u":"https://breakingdefense.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ The War Zone",    "u":"https://www.twz.com/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Defense News",    "u":"https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"n":"ğŸ‡ºğŸ‡¸ Stars & Stripes", "u":"https://www.stripes.com/rss/arc/outboundfeeds/news/"},
    {"n":"ğŸ‡ºğŸ‡¸ CTP-ISW Iran",    "u":"https://www.criticalthreats.org/feed"},
    {"n":"ğŸ‡ºğŸ‡¸ Long War Journal","u":"https://www.longwarjournal.org/feed"},
    # â”€â”€â”€ ØªØ­Ù„ÛŒÙ„/Ø³ÛŒØ§Ø³Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ‡ºğŸ‡¸ Foreign Policy",  "u":"https://foreignpolicy.com/feed/"},
    {"n":"ğŸ‡ºğŸ‡¸ CFR",             "u":"https://www.cfr.org/rss/feeds/news.xml"},
    {"n":"ğŸ‡ºğŸ‡¸ Axios World",     "u":"https://api.axios.com/feed/"},
    # â”€â”€â”€ Google News â€” Ø¨Ø­Ø±Ø§Ù† Ø§Ù…Ø±ÙˆØ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {"n":"ğŸ“° GN Witkoff Geneva","u":"https://news.google.com/rss/search?q=Witkoff+Kushner+Iran+nuclear+Geneva+talks&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN Trump Iran war","u":"https://news.google.com/rss/search?q=Trump+Iran+military+strike+war+2026&hl=en-US&gl=US&ceid=US:en&num=15&tbs=qdr:d"},
    {"n":"ğŸ“° GN USS Lincoln",   "u":"https://news.google.com/rss/search?q=USS+Abraham+Lincoln+carrier+Iran+Persian+Gulf&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GN Vance Iran",    "u":"https://news.google.com/rss/search?q=Vance+Rubio+Hegseth+Iran+military+nuclear&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
    {"n":"ğŸ“° GN Hormuz",        "u":"https://news.google.com/rss/search?q=Strait+Hormuz+Iran+US+navy+oil&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]

EMBASSY_FEEDS = [
    # ØªØ®Ù„ÛŒÙ‡ Ø¯ÛŒÙ¾Ù„Ù…Ø§Øªâ€ŒÙ‡Ø§ â€” ÙˆØ¶Ø¹ÛŒØª Ø§Ù…Ø±ÙˆØ² Ø­Ø§Ø¯ Ø§Ø³Øª
    {"n":"ğŸ›ï¸ US State Dept",   "u":"https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html.rss"},
    {"n":"ğŸ›ï¸ UK FCDO",         "u":"https://www.gov.uk/foreign-travel-advice/iran.atom"},
    {"n":"ğŸ“° GN Evacuation",   "u":"https://news.google.com/rss/search?q=embassy+evacuation+diplomats+Iran+Lebanon+2026&hl=en-US&gl=US&ceid=US:en&num=10&tbs=qdr:d"},
]

INTL_FEEDS = [
    {"n":"ğŸŒ BBC Middle East", "u":"https://feeds.bbci.co.uk/news/world/middle_east/rss.xml"},
    {"n":"ğŸŒ Al Jazeera",      "u":"https://www.aljazeera.com/xml/rss/all.xml"},
    {"n":"ğŸŒ Middle East Eye", "u":"https://www.middleeasteye.net/rss"},
    {"n":"ğŸŒ The Guardian ME", "u":"https://www.theguardian.com/world/middleeast/rss"},
    {"n":"ğŸŒ MEI",             "u":"https://www.mei.edu/rss.xml"},
]


ALL_RSS_FEEDS = IRAN_FEEDS + ISRAEL_FEEDS + USA_FEEDS + EMBASSY_FEEDS + INTL_FEEDS
EMBASSY_SET   = {id(f) for f in EMBASSY_FEEDS}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X handles
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TWITTER_HANDLES = [
    # â”€â”€â”€ OSINT / Breaking â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # âŒ "OSINTdefender" Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨ÙˆØ¯ â€” handle ÙˆØ§Ù‚Ø¹ÛŒ @sentdefender Ø§Ø³Øª
    ("ğŸ” OSINTdefender",        "sentdefender"),
    ("ğŸ” OSINTtechnical",       "Osinttechnical"),
    ("ğŸ” IntelCrab",            "IntelCrab"),
    ("ğŸ” GeoConfirmed",         "GeoConfirmed"),
    ("ğŸ” WarMonitor",           "WarMonitor3"),
    ("ğŸ” AuroraIntel",          "AuroraIntel"),
    ("ğŸ” Faytuks",              "Faytuks"),
    ("ğŸ” Clash Report",         "clashreport"),
    ("ğŸ” Megatron",             "Megatron_Ron"),
    ("ğŸ” ELINT News",           "ELINTNews"),
    ("ğŸ” War Zone TW",          "TheWarZoneTW"),
    # â”€â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¯ÙˆÙ„ØªÛŒ / Ù†Ø¸Ø§Ù…ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡ºğŸ‡¸ CENTCOM",              "CENTCOM"),
    ("ğŸ‡ºğŸ‡¸ DoD",                  "DeptofDefense"),
    ("ğŸ‡ºğŸ‡¸ Natasha Bertrand",     "NatashaBertrand"),
    ("ğŸ‡ºğŸ‡¸ Barak Ravid",          "BarakRavid"),
    ("ğŸ‡ºğŸ‡¸ Idrees Ali",           "idreesali114"),
    ("ğŸ‡ºğŸ‡¸ Jack Detsch",          "JackDetsch"),
    ("ğŸ‡ºğŸ‡¸ Lara Seligman",        "laraseligman"),
    ("ğŸ‡ºğŸ‡¸ Jim Sciutto",          "jimsciutto"),
    # â”€â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡± IDF",                  "IDF"),
    ("ğŸ‡®ğŸ‡± Israeli PM",           "IsraeliPM"),
    ("ğŸ‡®ğŸ‡± Yossi Melman",         "yossi_melman"),
    ("ğŸ‡®ğŸ‡± Seth Frantzman",       "sfrantzman"),
    ("ğŸ‡®ğŸ‡± Emanuel Fabian",       "manniefabian"),
    ("ğŸ‡®ğŸ‡± Anna Ahronheim",       "AAhronheim"),
    # â”€â”€â”€ Ø§ÛŒØ±Ø§Ù† / Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡®ğŸ‡· IranIntl EN",          "IranIntl_En"),
    ("ğŸ‡®ğŸ‡· IRNA EN",              "IRNA_English"),
    ("ğŸ‡®ğŸ‡· Press TV",             "PressTV"),
    ("ğŸ‡®ğŸ‡· Farnaz Fassihi",       "farnazfassihi"),
    ("ğŸ‡®ğŸ‡· Kasra Aarabi",         "KasraAarabi"),
    # â”€â”€â”€ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Brk",       "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ AFP News",              "AFPnews"),
    # â”€â”€â”€ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ("ğŸ” Ian Bremmer",           "ianbremmer"),
    ("ğŸ” Ellie Geranmayeh",      "EllieGeranmayeh"),
    ("ğŸ” Michael Knights",       "Mikeknightsiraq"),
    ("ğŸ” Aric Toler",            "AricToler"),
    ("âš ï¸ DEFCONLevel",           "DEFCONLevel"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TELEGRAM_CHANNELS = [
    # OSINT â€” Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
    ("ğŸ”´ Middle East Spectator", "Middle_East_Spectator"),
    ("ğŸ”´ Intel Slava Z",         "intelslava"),
    ("ğŸ”´ ELINT News",            "ELINTNews"),
    ("ğŸ”´ Clash Report",          "ClashReport"),
    ("ğŸ”´ Megatron OSINT",        "Megatron_Ron"),
    ("ğŸ”´ Disclose TV",           "disclosetv"),
    ("ğŸ” OSINTtechnical",        "Osinttechnical"),
    ("ğŸ” Aurora Intel",          "Aurora_Intel"),
    ("ğŸ” War Monitor",           "WarMonitor3"),
    # Ø§ÛŒØ±Ø§Ù† ÙØ§Ø±Ø³ÛŒ
    # âŒ "IranIntlPersian" Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨ÙˆØ¯ â€” handle ÙˆØ§Ù‚Ø¹ÛŒ @IranintlTV Ø§Ø³Øª (Û± Ù…ÛŒÙ„ÛŒÙˆÙ† Ø¹Ø¶Ùˆ)
    ("ğŸ‡®ğŸ‡· Iran Intl Persian",   "IranintlTV"),
    ("ğŸ‡®ğŸ‡· ØªØ³Ù†ÛŒÙ… ÙØ§Ø±Ø³ÛŒ",          "tasnimnewsfa"),
    ("ğŸ‡®ğŸ‡· Ù…Ù‡Ø± ÙØ§Ø±Ø³ÛŒ",             "mehrnews_fa"),
    ("ğŸ‡®ğŸ‡· Ø§ÛŒØ±Ù†Ø§ ÙØ§Ø±Ø³ÛŒ",           "irnafarsi"),
    ("ğŸ‡®ğŸ‡· Press TV",              "PressTVnews"),
    ("ğŸ‡®ğŸ‡· Ø§ÛŒÚ©Ø³â€ŒÙ†ÛŒÙˆØ² ÙØ§Ø±Ø³ÛŒ",         "FarsiOfficialx"),
    ("ğŸ‡®ğŸ‡· BBC PERSIAN",              "bbcpersian"),
    # Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
    ("ğŸ‡®ğŸ‡± Kann News",            "israelhayomofficial"),
    ("ğŸ‡®ğŸ‡± Times of Israel",      "timesofisrael"),
    # Ù…Ù†Ø·Ù‚Ù‡
    ("ğŸ‡¸ğŸ‡¦ Al Arabiya Breaking",  "AlArabiya_Brk"),
    ("ğŸ‡¶ğŸ‡¦ Al Jazeera EN",        "AlJazeeraEnglish"),
    ("ğŸ‡¾ğŸ‡² Masirah TV",           "AlMasirahNet"),
    ("ğŸ‡±ğŸ‡§ Naharnet",             "Naharnet"),
    # Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    ("ğŸŒ Reuters Breaking",      "ReutersBreaking"),
    ("ğŸŒ AP News",               "APnews"),
    ("ğŸŒ BBC Breaking",          "BBCBreaking"),
    ("ğŸŒ GeoConfirmed",          "GeoConfirmed"),
    ("ğŸŒ IntelCrab",             "IntelCrab"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Û²Û· ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶ â€” ÙÙ‚Ø· Ø¬Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†/Ø¢Ù…Ø±ÛŒÚ©Ø§/Ø§Ø³Ø±Ø§ÛŒÛŒÙ„
# Ù…Ù†Ø·Ù‚ AND: Ø§ÛŒØ±Ø§Ù† Ø¨Ù‡ ØªÙ†Ù‡Ø§ÛŒÛŒ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª â€” Ø¨Ø§ÛŒØ¯ Ø·Ø±Ù Ù…Ù‚Ø§Ø¨Ù„ ÛŒØ§ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù†Ú¯ÛŒ Ø¨Ø§Ø´Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ/Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Ø§ÛŒØ±Ø§Ù† Ú©Ù‡ Ø¨Ù‡ ØªÙ†Ù‡Ø§ÛŒÛŒ Ú©Ø§ÙÛŒâ€ŒØ§Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (ÙÙ‚Ø· Ø§Ú¯Ù‡ Ø§ÛŒÙ†â€ŒÙ‡Ø§ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§/Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â†’ pass)
IRAN_MILITARY_KW = [
    # Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ
    "irgc","sepah","quds force","basij military","irgc navy","irgc aerospace",
    "Ø³Ù¾Ø§Ù‡ Ù¾Ø§Ø³Ø¯Ø§Ø±Ø§Ù†","Ø³Ù¾Ø§Ù‡ Ù‚Ø¯Ø³","Ø¨Ø³ÛŒØ¬","Ù†ÛŒØ±ÙˆÛŒ Ù‡ÙˆØ§ÙØ¶Ø§ Ø³Ù¾Ø§Ù‡",
    # Ù…ÙˆØ´Ú© Ùˆ Ù¾Ù‡Ù¾Ø§Ø¯
    "ballistic missile iran","iran missile","iran drone attack","shahab",
    "fateh missile","kheybar","emad","khorramshahr","paveh","arash drone",
    "shahed drone","shahed-136","mohajer","gaza","soumar cruise missile",
    "Ù…ÙˆØ´Ú© Ø¨Ø§Ù„Ø³ØªÛŒÚ©","Ù…ÙˆØ´Ú© Ø§ÛŒØ±Ø§Ù†","Ù¾Ù‡Ù¾Ø§Ø¯ Ø´Ø§Ù‡Ø¯","Ù¾Ù‡Ù¾Ø§Ø¯ Ù…Ù‡Ø§Ø¬Ø±","Ù…ÙˆØ´Ú© Ø®ÛŒØ¨Ø±",
    "Ù…ÙˆØ´Ú© ÙØªØ­","Ù…ÙˆØ´Ú© Ú©Ø±ÙˆØ²","Ú©Ø±ÙˆØ² Ø§ÛŒØ±Ø§Ù†",
    # ØªØ£Ø³ÛŒØ³Ø§Øª Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ (Ø§Ù‡Ø¯Ø§Ù Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø­Ù…Ù„Ù‡)
    "natanz","fordow","arak heavy water","isfahan nuclear","parchin",
    "Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ø§Ø±Ø§Ú©","Ù¾Ø§Ø±Ú†ÛŒÙ†","Ø§ØµÙÙ‡Ø§Ù† Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ",
    # Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ
    "uranium enrichment iran","iran centrifuge","iran nuclear","60 percent",
    "90 percent enrichment","weapons grade uranium","nuclear breakout iran",
    "rebuild natanz","iran nuclear weapon","iran bomb",
    "ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…","Ø³Ø§Ù†ØªØ±ÛŒÙÛŒÙˆÚ˜ Ø§ÛŒØ±Ø§Ù†","Ø¨Ù…Ø¨ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Ø§ÛŒØ±Ø§Ù†","Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ… Û¹Û° Ø¯Ø±ØµØ¯",
    # Ø¹Ù…Ù„ÛŒØ§Øª Ù†Ø¸Ø§Ù…ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
    "irgc attack","iran attack","iran strike","iran fires","iran launches",
    "iran naval","iran warship","iran speedboat","iran intercept",
    "Ø­Ù…Ù„Ù‡ Ø³Ù¾Ø§Ù‡","Ø­Ù…Ù„Ù‡ Ø§ÛŒØ±Ø§Ù†","Ø§ÛŒØ±Ø§Ù† Ø´Ù„ÛŒÚ© Ú©Ø±Ø¯","Ù†Ø§Ùˆ Ø§ÛŒØ±Ø§Ù†",
    # ØªØ­Ø±ÛŒÙ… Ù†Ø¸Ø§Ù…ÛŒ/Ù†ÙØªÛŒ
    "iran oil sanctions","iran oil embargo","iran oil exports blocked",
    "ØªØ­Ø±ÛŒÙ… Ù†ÙØª Ø§ÛŒØ±Ø§Ù†","Ù†ÙØª Ø§ÛŒØ±Ø§Ù† ØªØ­Ø±ÛŒÙ…",
    # Ø¬Ù†Ú¯ Ú˜ÙˆØ¦Ù† Û²Û°Û²Ûµ â€” Ù¾ÛŒØ§Ù…Ø¯Ù‡Ø§
    "twelve-day war","iran-israel war aftermath","iran reconstitute",
    "iran rebuild nuclear","post-war iran","iran nuclear ruins",
    "Ø¬Ù†Ú¯ Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ø±ÙˆØ²Ù‡","Ø§ÛŒØ±Ø§Ù† Ù¾Ø³ Ø§Ø² Ø¬Ù†Ú¯","Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ ØªØ£Ø³ÛŒØ³Ø§Øª Ø§ÛŒØ±Ø§Ù†",
]

# â”€â”€â”€ Ø¢Ù…Ø±ÛŒÚ©Ø§ â€” ØªÛŒÙ… ØªØ±Ø§Ù…Ù¾ Û²Û°Û²Û¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USA_KW = [
    # Ø´Ø®ØµÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    "trump","donald trump","white house administration",
    "jd vance","vice president vance",
    "marco rubio","secretary rubio",                     # ÙˆØ²ÛŒØ± Ø®Ø§Ø±Ø¬Ù‡
    "pete hegseth","defense secretary hegseth",          # ÙˆØ²ÛŒØ± Ø¯ÙØ§Ø¹
    "scott bessent","treasury secretary bessent",
    "tulsi gabbard","dni gabbard",                       # Ø±Ø¦ÛŒØ³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    # Ù…Ø°Ø§Ú©Ø±Ù‡â€ŒÚ©Ù†Ù†Ø¯Ú¯Ø§Ù† Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Û²Û°Û²Û¶ (Ø¨Ø­Ø±Ø§Ù† Ø¬Ø§Ø±ÛŒ)
    "steve witkoff","witkoff","trump envoy iran",
    "jared kushner","kushner iran",
    "special envoy iran","us iran negotiations",
    "iran nuclear deal 2026","trump iran deal",
    # Ù†Ø¸Ø§Ù…ÛŒ
    "pentagon","centcom","us military iran","us navy iran",
    "us air force iran","us forces middle east",
    "carrier strike group","uss abraham lincoln","lincoln carrier",
    "uss gerald r ford","gerald ford carrier","uss dwight eisenhower",
    "b-52 iran","b-2 bomber iran","f-35 iran",
    "gbu-57","mop bomb","bunker buster iran",
    "al udeid","al-udeid air base","diego garcia iran",
    # ØªÙ‡Ø¯ÛŒØ¯/Ù‡Ø´Ø¯Ø§Ø± Û²Û°Û²Û¶
    "trump threatens iran","us threatens iran","trump ultimatum iran",
    "us strike iran","us attack iran","us bomb iran",
    "trump warn iran","final warning iran",
    # ØªØ­Ø±ÛŒÙ…
    "iran sanctions 2026","maximum pressure iran","us treasury iran",
    "trump sanctions iran","oil sanction iran","china iran tariff",
    "secondary sanctions iran","snap-back sanctions",
    # Ø³ÛŒØ§Ø³Øª
    "war authorization iran","aumf iran","congress iran war",
    "senate iran","state of the union iran",
    # ÙØ§Ø±Ø³ÛŒ
    "ØªØ±Ø§Ù…Ù¾","Ù¾Ù†ØªØ§Ú¯ÙˆÙ†","Ú©Ø§Ø® Ø³ÙÛŒØ¯","ÙˆÛŒØªÚ©ÙˆÙ","Ú©ÙˆØ´Ù†Ø±","Ø±ÙˆØ¨ÛŒÙˆ","Ù‡Ú¯Ø³Øª","ÙˆÙ†Ø³","Ø¨Ø³Ù†Øª","Ú¯Ø¨Ø§Ø±Ø¯",
    "Ù†Ø§Ùˆ Ø¢Ø¨Ø±Ø§Ù‡Ø§Ù… Ù„ÛŒÙ†Ú©Ù„Ù†","Ù†Ø§Ùˆ Ø¬Ø±Ø§Ù„Ø¯ ÙÙˆØ±Ø¯","Ù†Ø§Ùˆ Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§Ø¨Ø± Ø¢Ù…Ø±ÛŒÚ©Ø§",
    "ØªØ­Ø±ÛŒÙ… Ø§ÛŒØ±Ø§Ù†","ÙØ´Ø§Ø± Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ","Ø¶Ø±Ø¨Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø­Ù…Ù„Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†",
    "Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø§Ù„Ø¹Ø¯ÛŒØ¯","Ø¨Ù…Ø¨â€ŒØ§ÙÚ©Ù† B52","Ø¬Ù†Ú¯Ù†Ø¯Ù‡ F35",
]

# â”€â”€â”€ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â€” Ø±Ù‡Ø¨Ø±ÛŒ + Ù†Ø¸Ø§Ù…ÛŒ Û²Û°Û²Û¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ISRAEL_KW = [
    # Ú©Ù„Ù…Ø§Øª Ù¾Ø§ÛŒÙ‡
    "israel","israeli",
    # Ø±Ù‡Ø¨Ø±ÛŒ
    "netanyahu","benjamin netanyahu","pm netanyahu",
    "eyal zamir","idf chief zamir",
    "bezalel smotrich","smotrich",
    "itamar ben gvir","ben gvir",
    "israel katz",
    # Ù†Ø¸Ø§Ù…ÛŒ
    "idf","mossad operation","shin bet","aman intelligence",
    "israeli air force","iaf strike","israeli airstrike",
    "israeli strike iran","israel bomb iran","israel attack iran",
    "iron dome","arrow 3","arrow-3 missile","david's sling",
    "israel iran war","israel iran military",
    "operation against iran","israel warns iran",
    # ÙØ§Ø±Ø³ÛŒ
    "Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ","Ù…ÙˆØ³Ø§Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†","Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„",
    "Ù†ÛŒØ±ÙˆÛŒ Ù‡ÙˆØ§ÛŒÛŒ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„","Ø­Ù…Ù„Ù‡ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†","Ø¶Ø±Ø¨Ù‡ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„",
    "Ø§Ø³Ù…ÙˆØªØ±ÛŒÚ†","Ø¨Ù†â€ŒÚ¯ÙˆÛŒØ±","ÙˆØ²ÛŒØ± Ø¯ÙØ§Ø¹ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„",
]

# â”€â”€â”€ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ + Ù…ÛŒØ§Ù†Ø¬ÛŒØ§Ù† Û²Û°Û²Û¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROXY_KW = [
    # Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† â€” Ú©Ù„Ù…Ø§Øª Ø³Ø§Ø¯Ù‡ (Ù…Ù‡Ù…: Ø¨Ø§ÛŒØ¯ match Ú©Ù†Ù†Ø¯)
    "houthi","ansar allah","hamas","hezbollah","kataib",
    "pij","islamic jihad","popular mobilization",
    "Ø­ÙˆØ«ÛŒ","Ø§Ù†ØµØ§Ø±Ø§Ù„Ù„Ù‡","Ø­Ù…Ø§Ø³","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡","Ú©ØªØ§Ø¦Ø¨","Ø¬Ù‡Ø§Ø¯ Ø§Ø³Ù„Ø§Ù…ÛŒ",
    # Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø±Ú©Ø¨
    "houthi attack","houthi missile","houthi drone","houthi red sea",
    "houthi ship","hezbollah attack","hezbollah missile","hamas attack",
    "Ø­ÙˆØ«ÛŒ Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø®","Ø­ÙˆØ«ÛŒ Ù…ÙˆØ´Ú©","Ø­Ù…Ù„Ù‡ Ø­Ù…Ø§Ø³","Ø­Ù…Ù„Ù‡ Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡",
    # Ù…ÛŒØ§Ù†Ø¬ÛŒØ§Ù† Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Û²Û°Û²Û¶
    "badr al-busaidi","al-busaidi","grossi","iaea iran",
    "iran iaea","iran nuclear inspection","iran iaea deal",
    "oman mediation","Ø¹Ù…Ø§Ù† Ù…Ø°Ø§Ú©Ø±Ø§Øª","Ú¯Ø±ÙˆØ³ÛŒ","Ø¢Ú˜Ø§Ù†Ø³ Ø§ØªÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†",
    "Ø¨Ø§Ø²Ø±Ø³ÛŒ Ø¢Ú˜Ø§Ù†Ø³","Ù…Ø°Ø§Ú©Ø±Ø§Øª Ø¹Ù…Ø§Ù†",
]

# â”€â”€â”€ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¨Ø­Ø±Ø§Ù† Û²Û°Û²Û¶ â€” Ø¨Ø±Ø§ÛŒ AND logic Ø¨Ø§ "Ø§ÛŒØ±Ø§Ù†/iran" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WAR_CONTEXT_KW = [
    # Ú©Ù„Ù…Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø¬Ù†Ú¯ÛŒ â€” Ø¨Ø§ Ø§ÛŒØ±Ø§Ù†/Ø§Ø³Ø±Ø§ÛŒÛŒÙ„/Ø¢Ù…Ø±ÛŒÚ©Ø§ â†’ pass
    "war","attack","strike","airstrike","bombing","nuclear",
    "military","missile","weapon","threat","conflict","crisis",
    "sanction","invasion","escalation","retaliation","offensive",
    "Ø¬Ù†Ú¯","Ø­Ù…Ù„Ù‡","Ø¶Ø±Ø¨Ù‡","Ù‡Ø³ØªÙ‡","Ù†Ø¸Ø§Ù…ÛŒ","Ù…ÙˆØ´Ú©","ØªÙ‡Ø¯ÛŒØ¯","Ø¨Ø­Ø±Ø§Ù†","ØªØ­Ø±ÛŒÙ…",
    # Ù…Ø°Ø§Ú©Ø±Ø§Øª Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ ÙØ¹Ø§Ù„ (Ø¨Ø­Ø±Ø§Ù† Ø¬Ø§Ø±ÛŒ ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶)
    "geneva talks iran","vienna talks iran","nuclear framework iran",
    "iran nuclear agreement","iran deal framework",
    "iran nuclear talks","nuclear negotiations iran",
    "fourth round","fifth round talks","iran negotiations",
    "Ù…Ø°Ø§Ú©Ø±Ø§Øª Ú˜Ù†Ùˆ","Ù…Ø°Ø§Ú©Ø±Ø§Øª ÙˆÛŒÙ†","Ú†Ø§Ø±Ú†ÙˆØ¨ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ù…Ø°Ø§Ú©Ø±Ø§Øª Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ",
    "ØªÙˆØ§ÙÙ‚ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ø¨Ø³ØªÙ‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ",
    # ØªÙ†Ø´ Ùˆ Ø­Ù…Ù„Ù‡
    "strike iran","attack iran","bomb iran",
    "military strike iran","us strike iran","israel strike iran",
    "Ø­Ù…Ù„Ù‡ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†","Ø¶Ø±Ø¨Ù‡ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù† Ø§ÛŒØ±Ø§Ù†",
    # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯
    "war iran","iran war","iran conflict","iran military crisis",
    "last chance iran","iran ultimatum","countdown iran",
    "iran war clock","iran deadline",
    "Ø¬Ù†Ú¯ Ø¨Ø§ Ø§ÛŒØ±Ø§Ù†","Ø¨Ø­Ø±Ø§Ù† Ø§ÛŒØ±Ø§Ù†","Ø§ØªÙ…Ø§Ù… Ø­Ø¬Øª Ø§ÛŒØ±Ø§Ù†",
    # ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²
    "strait of hormuz iran","hormuz closure","hormuz blockade",
    "ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²","Ø¨Ø³ØªÙ† Ù‡Ø±Ù…Ø²","Ø§Ù†Ø³Ø¯Ø§Ø¯ Ù‡Ø±Ù…Ø²",
    # ØªØ­Ø±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
    "iran oil sanctions","iran sanctions","iran nuclear sanctions",
    "ØªØ­Ø±ÛŒÙ… Ø§ÛŒØ±Ø§Ù†","ØªØ­Ø±ÛŒÙ… Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ",
]

# â”€â”€â”€ Ø­Ø°Ù Ù‚Ø·Ø¹ÛŒ â€” Ø§Ø®Ø¨Ø§Ø± Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨ÛŒâ€ŒØ±Ø¨Ø· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HARD_EXCLUDE = [
    # ÙˆØ±Ø²Ø´
    "nba","nfl","nhl","mlb","premier league","la liga","serie a",
    "football match","soccer game","basketball game","world cup",
    "olympic games","marathon race","tennis tournament","golf tournament",
    "ÙÙˆØªØ¨Ø§Ù„","Ø¨Ø³Ú©ØªØ¨Ø§Ù„","ÙˆØ§Ù„ÛŒØ¨Ø§Ù„","Ú©Ø´ØªÛŒ","Ø§Ù„Ù…Ù¾ÛŒÚ©","Ù„ÛŒÚ¯ Ø¨Ø±ØªØ±",
    # Ø³Ø±Ú¯Ø±Ù…ÛŒ/ÙØ±Ù‡Ù†Ú¯
    "box office","grammy","grammy awards","oscar","oscar ceremony","film festival",
    "music video","celebrity news","reality show","fashion week",
    "Ø³ÛŒÙ†Ù…Ø§","Ù…ÙˆØ³ÛŒÙ‚ÛŒ","Ø¬ÙˆØ§ÛŒØ² ÙÛŒÙ„Ù…","ÙØ´Ù†","Ø³Ø±ÛŒØ§Ù„",
    # Ø§Ù‚ØªØµØ§Ø¯ Ø¯Ø§Ø®Ù„ÛŒ Ø¨ÛŒâ€ŒØ±Ø¨Ø·
    "bitcoin","cryptocurrency","crypto market","ethereum","blockchain",
    "stock market crash","dow jones","nasdaq","s&p 500",
    "Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ†","Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„","Ø¨ÙˆØ±Ø³",
    # Ø¨Ù„Ø§ÛŒØ§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ
    "earthquake disaster","flood victims","hurricane damage","wildfire",
    "Ø²Ù„Ø²Ù„Ù‡","Ø³ÛŒÙ„","Ø¢ØªØ´ÙØ´Ø§Ù†",
    # Ø³ÛŒØ§Ø³Øª Ø¯Ø§Ø®Ù„ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¨ÛŒâ€ŒØ±Ø¨Ø· Ø¨Ù‡ Ø¬Ù†Ú¯
    "iran economy inflation","iran domestic","iran parliament vote",
    "iran budget law","iran judiciary","iran court ruling",
    "iran road accident","iran plane crash","iran traffic",
    "ØªÙˆØ±Ù… Ø§ÛŒØ±Ø§Ù†","Ø¨ÙˆØ¯Ø¬Ù‡ Ø¯Ø§Ø®Ù„ÛŒ","Ù…Ø¬Ù„Ø³ Ø§ÛŒØ±Ø§Ù† Ø¨ÙˆØ¯Ø¬Ù‡","Ø¯Ø§Ø¯Ú¯Ø§Ù‡ Ø¯Ø§Ø®Ù„ÛŒ Ø§ÛŒØ±Ø§Ù†",
    "ØªØµØ§Ø¯Ù Ø¬Ø§Ø¯Ù‡","Ø³Ø§Ù†Ø­Ù‡ Ù‡ÙˆØ§ÛŒÛŒ Ø¯Ø§Ø®Ù„ÛŒ",
]

EMBASSY_OVERRIDE = [
    "evacuate","leave immediately","travel warning level 4",
    "warden message","embassy closed","consulate closed emergency",
    "us citizens leave","withdraw diplomats",
    "ØªØ®Ù„ÛŒÙ‡","ÙÙˆØ±ÛŒ ØªØ±Ú©","Ù‡Ø´Ø¯Ø§Ø± Ø³ÙØ§Ø±Øª","Ø¯ÛŒÙ¾Ù„Ù…Ø§Øªâ€ŒÙ‡Ø§ Ø®Ø§Ø±Ø¬",
]

# â”€â”€â”€ ÙÛŒÙ„ØªØ± Ø§ØµÙ„ÛŒ Ø¨Ø§ Ù…Ù†Ø·Ù‚ AND Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_war_relevant(text: str, is_embassy=False, is_tg=False, is_tw=False) -> bool:
    """
    ÙÛŒÙ„ØªØ± Û²Û°Û²Û¶ â€” Ø¢Ú¯Ø§Ù‡ Ø¨Ù‡ Ù…Ù†Ø¨Ø¹:

    Twitter/Telegram = Ù…Ù†Ø§Ø¨Ø¹ curated Ø§Ø®ØªØµØ§ØµÛŒ Ø¬Ù†Ú¯:
      â†’ ÙÙ‚Ø· HARD_EXCLUDE Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ù‚ÛŒÙ‡ pass

    RSS = Ù…Ù†Ø§Ø¨Ø¹ Ø¹Ù…ÙˆÙ…ÛŒ (Ø´Ø§Ù…Ù„ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ø§ÛŒØ±Ø§Ù†):
      â†’ ÙÛŒÙ„ØªØ± AND: Ø¨Ø§ÛŒØ¯ Ø§ÛŒØ±Ø§Ù† + Ø·Ø±Ù Ù…Ù‚Ø§Ø¨Ù„/Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù†Ú¯ÛŒ Ø¨Ø§Ø´Ø¯
      â†’ Ø§Ø®Ø¨Ø§Ø± ØµØ±ÙØ§Ù‹ Ø¯Ø§Ø®Ù„ÛŒ Ø§ÛŒØ±Ø§Ù† Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
    """
    txt = text.lower()

    # â”€â”€ Ø­Ø°Ù Ù‚Ø·Ø¹ÛŒ (Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if any(k in txt for k in HARD_EXCLUDE):
        return False

    # â”€â”€ Ø³ÙØ§Ø±Øª + Ù‡Ø´Ø¯Ø§Ø± ÙÙˆØ±ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_embassy and any(k in txt for k in EMBASSY_OVERRIDE):
        return True

    # â”€â”€ Twitter/Telegram: Ù…Ù†Ø§Ø¨Ø¹ curated â€” ÙÛŒÙ„ØªØ± Ø³Ø¨Ú© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ø§ÛŒÙ† Ø§Ú©Ø§Ù†Øªâ€ŒÙ‡Ø§ Ø®ÙˆØ¯Ø´Ø§Ù† ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± Ø¬Ù†Ú¯ Ù¾ÙˆØ³Øª Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯
    # ÙÙ‚Ø· Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ú©Ù„Ù…Ù‡ Ù…Ø±ØªØ¨Ø· Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
    if is_tw or is_tg:
        has_any = (
            any(k in txt for k in IRAN_MILITARY_KW) or
            any(k in txt for k in USA_KW) or
            any(k in txt for k in ISRAEL_KW) or
            any(k in txt for k in PROXY_KW) or
            any(k in txt for k in WAR_CONTEXT_KW) or
            "iran" in txt or "iranian" in txt or "Ø§ÛŒØ±Ø§Ù†" in txt or
            "irgc" in txt or "sepah" in txt or "Ø³Ù¾Ø§Ù‡" in txt or
            "tehran" in txt or "ØªÙ‡Ø±Ø§Ù†" in txt or
            "israel" in txt or "Ø§Ø³Ø±Ø§ÛŒÛŒÙ„" in txt or
            "nuclear" in txt or "Ù‡Ø³ØªÙ‡" in txt or
            "missile" in txt or "Ù…ÙˆØ´Ú©" in txt or
            "trump" in txt or "ØªØ±Ø§Ù…Ù¾" in txt or
            "netanyahu" in txt or "Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ" in txt or
            "war" in txt or "attack" in txt or "strike" in txt or
            "Ø­Ù…Ù„Ù‡" in txt or "Ø¬Ù†Ú¯" in txt
        )
        return has_any

    # â”€â”€ RSS: ÙÛŒÙ„ØªØ± AND â€” Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø®Ø¨Ø§Ø± Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¯Ø§Ø®Ù„ÛŒ Ø§ÛŒØ±Ø§Ù† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    has_iran_mil  = any(k in txt for k in IRAN_MILITARY_KW)
    has_iran_name = ("iran" in txt or "iranian" in txt or "Ø§ÛŒØ±Ø§Ù†" in txt
                     or "ØªÙ‡Ø±Ø§Ù†" in txt or "Ø®Ø§Ù…Ù†Ù‡" in txt or "Ù¾Ø²Ø´Ú©ÛŒØ§Ù†" in txt
                     or "Ø¹Ø±Ø§Ù‚Ú†ÛŒ" in txt or "irgc" in txt or "tehran" in txt
                     or "Ø³Ù¾Ø§Ù‡" in txt or "Ù†Ø·Ù†Ø²" in txt or "ÙØ±Ø¯Ùˆ" in txt)
    has_usa       = any(k in txt for k in USA_KW)
    has_israel    = any(k in txt for k in ISRAEL_KW)
    has_war_ctx   = any(k in txt for k in WAR_CONTEXT_KW)
    has_proxy     = any(k in txt for k in PROXY_KW)

    # Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù†Ø¸Ø§Ù…ÛŒ/Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ Ø§ÛŒØ±Ø§Ù† â†’ Ù‡Ù…ÛŒØ´Ù‡ pass
    if has_iran_mil:
        return True

    # Ù¾Ø±ÙˆÚ©Ø³ÛŒ â†’ pass (Ø­ÙˆØ«ÛŒ/Ø­Ù…Ø§Ø³/Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡)
    if has_proxy:
        return True

    # Ø§ÛŒØ±Ø§Ù† + Ø·Ø±Ù Ù…Ù‚Ø§Ø¨Ù„ ÛŒØ§ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù†Ú¯ â†’ pass
    if has_iran_name and (has_usa or has_israel or has_war_ctx):
        return True

    # Ø¢Ù…Ø±ÛŒÚ©Ø§ + Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ â†’ pass
    if has_usa and has_israel:
        return True

    # Ø¢Ù…Ø±ÛŒÚ©Ø§ ÛŒØ§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ + Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù†Ú¯ÛŒ â†’ pass
    if (has_usa or has_israel) and has_war_ctx:
        return True

    # Ø§ÛŒØ±Ø§Ù† Ø¨Ù‡ ØªÙ†Ù‡Ø§ÛŒÛŒ Ø¨Ø¯ÙˆÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù†Ú¯ÛŒ â†’ REJECT (ØªÙˆØ±Ù…/ØªØ±Ø§ÙÛŒÚ©/Ø¨ÙˆØ¯Ø¬Ù‡ Ø¯Ø§Ø®Ù„ÛŒ)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Twitter/X â€” Feb 2026 â€” ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª Ø§Ø² Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† uptime Ø¯Ø± GitHub Actions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…: Ø§Ú©Ø«Ø± Nitter instances Ø¯Ø± GitHub Actions IPs Ø¨Ù„Ø§Ú© Ù‡Ø³ØªÙ†Ø¯
# RSSHub Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¹ØªÙ…Ø§Ø¯ØªØ± Ø§Ø³Øª â€” Ø§Ø² Ø¢Ù† Ø§Ø¨ØªØ¯Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
RSSHUB_INSTANCES = [
    "https://rsshub.app",               # âœ… Ø§ØµÙ„ÛŒ â€” Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ±ÛŒÙ†
    "https://rsshub.rss.now.sh",       # âœ… mirror
    "https://rss.shab.fun",            # backup
    "https://rsshub.moeyy.xyz",        # backup
    "https://hub.slar.ru",             # backup
]
NITTER_INSTANCES = [
    "https://rss.xcancel.com",         # âœ… subdomain Ù…Ø³ØªÙ‚ÛŒÙ…
    "https://xcancel.com",             # âœ… redirect
    "https://nitter.poast.org",        # âœ… Ø§ØºÙ„Ø¨ Ø¯Ø± CI Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    "https://nitter.privacyredirect.com",
    "https://nitter.tiekoetter.com",
    "https://lightbrd.com",
    "https://nitter.catsarch.com",
    "https://n.ramle.be",
    "https://nitter.space",
    "https://nitter.net",
    "https://nitter.it",
    "https://nitter.unixfox.eu",
]

NITTER_HDR = {
    "User-Agent": "Mozilla/5.0 (compatible; Feedfetcher-Google; +http://www.google.com/feedfetcher.html)",
    "Accept": "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Cache-Control": "no-cache",
}
COMMON_UA = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

_nitter_pool: list[str]  = []
_rsshub_pool: list[str]  = []
_TW_SEMA: asyncio.Semaphore | None = None

def _load_nitter_cache() -> tuple[list, list, float]:
    try:
        if Path(NITTER_CACHE_FILE).exists():
            d = json.load(open(NITTER_CACHE_FILE))
            return d.get("nitter", []), d.get("rsshub", []), d.get("ts", 0.0)
    except: pass
    return [], [], 0.0

def _save_nitter_cache(nitter, rsshub):
    json.dump({"nitter": nitter, "rsshub": rsshub,
               "ts": datetime.now(timezone.utc).timestamp()},
              open(NITTER_CACHE_FILE, "w"))

def _is_rss(body: str, ct: str) -> bool:
    b = body[:600].lower()
    return ("xml" in ct) or ("<rss" in b) or ("<?xml" in b) or ("<feed" in b)

async def _try_rss(client: httpx.AsyncClient, url: str, timeout: float = TW_TIMEOUT) -> list:
    """
    RSS URL Ø±Ø§ fetch Ú©Ø±Ø¯Ù‡ entries Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    follow_redirects=True Ù…Ù‡Ù… Ø§Ø³Øª (xcancel.com â†’ rss.xcancel.com)
    """
    try:
        r = await client.get(url,
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=timeout,
                                                   write=5.0, pool=5.0))
        if r.status_code not in (200, 304):
            return []
        ct = r.headers.get("content-type", "")
        body = r.text or ""
        if not _is_rss(body, ct):
            return []
        parsed = feedparser.parse(body)
        entries = getattr(parsed, "entries", []) or []
        return [e for e in entries if len((e.get("title") or "").strip()) > 3]
    except Exception:
        return []

async def _probe_instance(client: httpx.AsyncClient, url: str,
                          handle: str = "OSINTdefender") -> tuple | None:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ÛŒÚ© instance ÙˆØ§Ù‚Ø¹Ø§Ù‹ RSS Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ù…Ù‡Ù…: ÙÙ‚Ø· Ø³Ø§Ø®ØªØ§Ø± RSS Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù†Ù‡ ØªØ¹Ø¯Ø§Ø¯ entries.
    """
    t0 = asyncio.get_running_loop().time()
    try:
        r = await client.get(f"{url}/{handle}/rss",
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=7.0,
                                                   write=5.0, pool=5.0))
        if r.status_code not in (200, 304):
            return None
        ct   = r.headers.get("content-type", "")
        body = r.text or ""
        # ÙÙ‚Ø· Ú†Ú© Ø³Ø§Ø®ØªØ§Ø± â€” Ù†Ù‡ entries
        if _is_rss(body, ct):
            ms = (asyncio.get_running_loop().time() - t0) * 1000
            return url, ms
    except Exception:
        pass
    return None

async def _probe_rsshub(client: httpx.AsyncClient, inst: str) -> tuple | None:
    t0 = asyncio.get_running_loop().time()
    try:
        r = await client.get(f"{inst}/twitter/user/OSINTdefender",
                             headers=NITTER_HDR,
                             follow_redirects=True,
                             timeout=httpx.Timeout(connect=5.0, read=8.0,
                                                   write=5.0, pool=5.0))
        if r.status_code in (200, 304) and _is_rss(r.text or "", r.headers.get("content-type","")):
            ms = (asyncio.get_running_loop().time() - t0) * 1000
            return inst, ms
    except Exception:
        pass
    return None

async def build_twitter_pools(client: httpx.AsyncClient):
    """
    Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡: probe Ø­Ø°Ù Ø´Ø¯.
    Ù‡Ù…Ù‡ fetch_twitter Ù…Ø³ØªÙ‚ÛŒÙ… RSSHub â†’ Nitter Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.
    ÙÙ‚Ø· cache Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… Ú©Ù‡ Ø¢Ø®Ø±ÛŒÙ† instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø¨ÛŒØ§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
    """
    global _nitter_pool, _rsshub_pool
    cached_n, cached_r, ts = _load_nitter_cache()
    age = datetime.now(timezone.utc).timestamp() - ts
    # Ø§Ú¯Ù‡ cache Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª: Ø¢Ø®Ø±ÛŒÙ† instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø§ÙˆÙ„ Ø¨Ú¯Ø°Ø§Ø±
    if age < NITTER_CACHE_TTL:
        if cached_r: _rsshub_pool = cached_r + [i for i in RSSHUB_INSTANCES if i not in cached_r]
        if cached_n: _nitter_pool = cached_n + [i for i in NITTER_INSTANCES if i not in cached_n]
    if not _rsshub_pool: _rsshub_pool = list(RSSHUB_INSTANCES)
    if not _nitter_pool: _nitter_pool = list(NITTER_INSTANCES)
    log.info(f"ğ• pools: RSSHub={len(_rsshub_pool)} Nitter={len(_nitter_pool)}")

async def fetch_twitter(client: httpx.AsyncClient, label: str, handle: str) -> list:
    """
    Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§:
    1. RSSHub (Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ø¯Ø± GitHub Actions CI)
    2. Nitter instances
    Ø§ÙˆÙ„ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆÙÙ‚ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¯ÙØ¹Ù‡ Ø¨Ø¹Ø¯ Ø§ÙˆÙ„ Ø§Ù…ØªØ­Ø§Ù† Ø´ÙˆØ¯.
    """
    sema = _TW_SEMA or asyncio.Semaphore(15)
    async with sema:
        # â”€â”€ RSSHub Ø§ÙˆÙ„ (Ø¯Ø± CI Ø¨Ù‡ØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for inst in (_rsshub_pool or RSSHUB_INSTANCES):
            for path in (f"/twitter/user/{handle}", f"/x/user/{handle}"):
                e = await _try_rss(client, f"{inst}{path}", timeout=8.0)
                if e:
                    log.debug(f"ğ• {handle} â† RSSHub {inst.split('//')[-1]} ({len(e)})")
                    # Ø§ÛŒÙ† instance Ø±Ø§ Ø¨Ù‡ Ø§ÙˆÙ„ cache Ø¨ÙØ±Ø³Øª
                    _update_pool_cache(inst, is_rsshub=True)
                    return [(x, f"ğ• {label}", "tw", False) for x in e]

        # â”€â”€ Nitter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for inst in (_nitter_pool or NITTER_INSTANCES):
            e = await _try_rss(client, f"{inst}/{handle}/rss", timeout=6.0)
            if e:
                log.debug(f"ğ• {handle} â† Nitter {inst.split('//')[-1]} ({len(e)})")
                _update_pool_cache(inst, is_rsshub=False)
                return [(x, f"ğ• {label}", "tw", False) for x in e]

    log.debug(f"ğ• {handle}: Ù‡Ù…Ù‡ fail")
    return []

def _update_pool_cache(working_inst: str, is_rsshub: bool):
    """instance Ù…ÙˆÙÙ‚ Ø±Ø§ Ø¨Ù‡ Ø§ÙˆÙ„ Ù„ÛŒØ³Øª cache Ù…ÛŒâ€ŒØ¨Ø±Ø¯"""
    global _nitter_pool, _rsshub_pool
    if is_rsshub:
        pool = [working_inst] + [i for i in _rsshub_pool if i != working_inst]
        _rsshub_pool = pool
        json.dump({"nitter": _nitter_pool, "rsshub": pool,
                   "ts": datetime.now(timezone.utc).timestamp()},
                  open(NITTER_CACHE_FILE, "w"))
    else:
        pool = [working_inst] + [i for i in _nitter_pool if i != working_inst]
        _nitter_pool = pool
        json.dump({"nitter": pool, "rsshub": _rsshub_pool,
                   "ts": datetime.now(timezone.utc).timestamp()},
                  open(NITTER_CACHE_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS + Telegram fetch
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_rss(client: httpx.AsyncClient, feed: dict) -> list:
    """RSS Ø¨Ø§ conditional GET (ETag/If-Modified-Since)"""
    try:
        hdrs = dict(COMMON_UA)
        hdrs["Accept"] = "application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8"
        if feed.get("_etag"):      hdrs["If-None-Match"]     = feed["_etag"]
        if feed.get("_last_mod"):  hdrs["If-Modified-Since"] = feed["_last_mod"]
        r = await client.get(feed["u"], timeout=httpx.Timeout(RSS_TIMEOUT), headers=hdrs)
        if r.status_code == 304: return []
        if r.status_code != 200: return []
        if r.headers.get("ETag"):          feed["_etag"]     = r.headers["ETag"]
        if r.headers.get("Last-Modified"): feed["_last_mod"] = r.headers["Last-Modified"]
        entries = feedparser.parse(r.text).entries or []
        is_emb  = id(feed) in EMBASSY_SET
        return [(e, feed["n"], "rss", is_emb) for e in entries]
    except: return []

async def fetch_telegram_channel(client: httpx.AsyncClient, label: str,
                                  handle: str, cutoff: datetime) -> list:
    """
    scrape t.me/s/{handle} â€” ÙˆØ§Ú©Ø´ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
    Ø§Ø² Ú†Ù†Ø¯ User-Agent Ù…Ø®ØªÙ„Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø­ØªÙ…Ø§Ù„ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ù„Ø§ Ø¨Ø±ÙˆØ¯
    """
    url = f"https://t.me/s/{handle}"
    # user agents Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ bypass rate limiting
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0",
        "TelegramBot (like TwitterBot) 2.0",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/122.0 Safari/537.36",
    ]
    hdrs = {
        "User-Agent": ua_list[hash(handle) % len(ua_list)],
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }
    try:
        r = await client.get(url, timeout=httpx.Timeout(TG_TIMEOUT),
                             headers=hdrs, follow_redirects=True)
        if r.status_code not in (200, 301, 302):
            log.debug(f"TG {handle}: HTTP {r.status_code}")
            return []

        html = r.text
        if not html or len(html) < 500:
            log.debug(f"TG {handle}: empty response")
            return []

        soup = BeautifulSoup(html, "html.parser")

        # selector Ø§ØµÙ„ÛŒ Telegram web
        msgs = soup.select(".tgme_widget_message_wrap")
        if not msgs:
            # fallback: selector Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±
            msgs = soup.select(".tgme_widget_message")

        if not msgs:
            log.debug(f"TG {handle}: no messages found ({len(html)} bytes)")
            return []

        results = []
        for msg in msgs[-40:]:  # Ø¢Ø®Ø±ÛŒÙ† Û´Û° Ù¾ÛŒØ§Ù…
            # Ù…ØªÙ† Ù¾ÛŒØ§Ù… â€” Ú†Ù†Ø¯ selector Ù…Ø®ØªÙ„Ù
            txt_el = (msg.select_one(".tgme_widget_message_text")
                      or msg.select_one(".tgme_widget_message_bubble .js-message_text")
                      or msg.select_one("[data-post]"))
            text = txt_el.get_text(" ", strip=True) if txt_el else ""

            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ whitespace Ø²ÛŒØ§Ø¯
            text = re.sub(r'\s+', ' ', text).strip()
            if not text or len(text) < 10:
                continue

            # Ø²Ù…Ø§Ù† Ù¾ÛŒØ§Ù…
            time_el  = msg.select_one("time[datetime]")
            dt_str   = time_el.get("datetime", "") if time_el else ""
            entry_dt = None
            if dt_str:
                try:
                    entry_dt = datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
                except Exception:
                    pass

            # ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ
            if entry_dt and entry_dt < cutoff:
                continue

            # Ù„ÛŒÙ†Ú© Ù¾ÛŒØ§Ù…
            link_el = (msg.select_one("a.tgme_widget_message_date")
                       or msg.select_one("a[href*='t.me']"))
            link = link_el.get("href", "") if link_el else f"https://t.me/{handle}"

            # Ø¹Ù†ÙˆØ§Ù† = Ø§ÙˆÙ„ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ù…ØªÙ†
            first_line = text.split('\n')[0][:300].strip()
            title = first_line if first_line else text[:200]

            results.append(({
                "title":   title,
                "summary": text[:1000],
                "link":    link,
                "_tg_dt":  entry_dt,
            }, label, "tg", False))

        log.debug(f"TG {handle}: {len(results)} messages")
        return results

    except Exception as e:
        log.debug(f"TG {handle}: {e}")
        return []

async def fetch_all(client: httpx.AsyncClient, cutoff: datetime) -> list:
    """
    ÙˆØ§Ú©Ø´ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹ â€” ØªØ±ØªÛŒØ¨: Twitter Ø§ÙˆÙ„ØŒ Ø³Ù¾Ø³ TelegramØŒ Ø³Ù¾Ø³ RSS
    Twitter Ø§ÙˆÙ„ Ú†ÙˆÙ† breaking news Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¯Ø± X Ù…Ù†ØªØ´Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯
    """
    await build_twitter_pools(client)

    # ØªØ±ØªÛŒØ¨ Ø§Ø±Ø³Ø§Ù„: Twitter Ø§ÙˆÙ„ â†’ RSS â†’ Telegram
    # (Ù‡Ù…Ù‡ Ù…ÙˆØ§Ø²ÛŒ fetch Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ÙˆÙ„ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ Ø§ÛŒÙ† ØªØ±ØªÛŒØ¨ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯)
    tw_t  = [fetch_twitter(client, l, h) for l, h in TWITTER_HANDLES]
    rss_t = [fetch_rss(client, f) for f in ALL_RSS_FEEDS]
    tg_t  = [fetch_telegram_channel(client, l, h, cutoff) for l, h in TELEGRAM_CHANNELS]

    all_res = await asyncio.gather(*tw_t, *rss_t, *tg_t, return_exceptions=True)

    out = []; tw_ok = rss_ok = tg_ok = 0
    n_tw  = len(TWITTER_HANDLES)
    n_rss = len(ALL_RSS_FEEDS)
    for i, res in enumerate(all_res):
        if not isinstance(res, list): continue
        out.extend(res)
        if   i < n_tw:              tw_ok  += bool(res)
        elif i < n_tw + n_rss:      rss_ok += bool(res)
        else:                        tg_ok  += bool(res)

    log.info(f"  ğ•:{tw_ok}/{len(TWITTER_HANDLES)}"
             f"  ğŸ“¡ RSS:{rss_ok}/{len(ALL_RSS_FEEDS)}"
             f"  ğŸ“¢ TG:{tg_ok}/{len(TELEGRAM_CHANNELS)}")
    return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¨Ø²Ø§Ø± Ù…ØªÙ†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_html(t): return re.sub(r"<[^>]+>", " ", t or "").strip()
def trim(t, n):
    t = t.strip()
    return t if len(t) <= n else t[:n-1] + "â€¦"
def make_id(entry):
    k = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(k.encode()).hexdigest()
def esc(t):
    return re.sub(r"([<>&])", lambda m: {"<":"&lt;",">":"&gt;","&":"&amp;"}[m.group()], t)

def format_dt(entry) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
        tg_dt = entry.get("_tg_dt")
        if tg_dt:
            return tg_dt.astimezone(TEHRAN_TZ).strftime("%H:%M ØªÙ‡Ø±Ø§Ù†")
    except: pass
    return ""

def is_fresh(entry, cutoff: datetime) -> bool:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t: return datetime(*t[:6], tzinfo=timezone.utc) >= cutoff
        tg_dt = entry.get("_tg_dt")
        if tg_dt: return tg_dt >= cutoff
        return True  # Ø¨Ø¯ÙˆÙ† timestamp â†’ Ù¾Ø§Ø³ Ø¨Ø¯Ù‡ (seen.json ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ù‡)
    except: return True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dedup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_VIOLENCE_CODES  = {"MSL","AIR","ATK","KIA","DEF","EXP"}
_POLITICAL_CODES = {"THR","DIP","SAN","NUC","SPY","STM"}

def _stem(word):
    w = word.lower()
    for suf in ("ing","ed","tion","ment","er","Ù‡Ø§","Ù‡Ø§ÛŒ","\u200cÙ‡Ø§"):
        if w.endswith(suf) and len(w) > len(suf)+3: return w[:-len(suf)]
    return w

def _bag(text):
    return {_stem(w) for w in re.findall(r"[\w\u0600-\u06FF]{3,}", text.lower())}

def _entity_triple(title):
    txt = title.lower()
    actors = (
        ["iran","irgc","khamenei","Ø³Ù¾Ø§Ù‡","Ø§ÛŒØ±Ø§Ù†"],
        ["israel","idf","netanyahu","Ø§Ø³Ø±Ø§ÛŒÛŒÙ„"],
        ["us ","usa","centcom","pentagon","Ø¢Ù…Ø±ÛŒÚ©Ø§"],
        ["hamas","Ø­Ù…Ø§Ø³"], ["hezbollah","Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡"], ["houthi","Ø­ÙˆØ«ÛŒ"],
    )
    action_cats = {
        "MSL": ["missile","rocket","ballistic","Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯"],
        "AIR": ["airstrike","bombing","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†"],
        "ATK": ["attack","strike","Ø­Ù…Ù„Ù‡"],
        "KIA": ["killed","dead","casualties","Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯"],
        "DEF": ["intercept","iron dome","Ø±Ù‡Ú¯ÛŒØ±ÛŒ"],
        "EXP": ["explosion","blast","Ø§Ù†ÙØ¬Ø§Ø±"],
        "THR": ["threat","warn","ØªÙ‡Ø¯ÛŒØ¯"],
        "SAN": ["sanction","ØªØ­Ø±ÛŒÙ…"],
        "NUC": ["nuclear","uranium","Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ"],
    }
    actor1, actor2, act = "", "", ""
    for i, grp in enumerate(actors):
        if any(a in txt for a in grp):
            if not actor1: actor1 = str(i)
            elif not actor2: actor2 = str(i)
    for code, kws in action_cats.items():
        if any(k in txt for k in kws): act = code; break
    return actor1, actor2, act

def is_story_dup(title: str, stories: list) -> bool:
    bag1 = _bag(title)
    if not bag1: return False
    a1, a2, act1 = _entity_triple(title)
    for item in stories:
        if not (isinstance(item, (list, tuple)) and len(item) == 3):
            continue
        _, prev_bag_raw, prev_triple = item
        prev_bag = set(prev_bag_raw) if isinstance(prev_bag_raw, list) else prev_bag_raw
        pa, pb, pact = prev_triple
        if act1 and pact and act1 in _VIOLENCE_CODES and pact in _VIOLENCE_CODES:
            if a1 == pa and a2 == pb: return True
        if act1 and pact and act1 in _POLITICAL_CODES and pact in _POLITICAL_CODES:
            if a1 == pa: return True
        union = bag1 | prev_bag
        if union and len(bag1 & prev_bag) / len(union) >= JACCARD_THRESHOLD:
            return True
    return False

def register_story(title: str, stories: list) -> list:
    stories.append([title, list(_bag(title)), list(_entity_triple(title))])
    return stories[-MAX_STORIES:]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# seen.json â€” Ø¨Ø§ TTL â€” ÙÙ‚Ø· Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    cutoff_ts = datetime.now(timezone.utc).timestamp() - SEEN_TTL_HOURS * 3600
    try:
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                return {k for k, v in raw.items() if v > cutoff_ts}
            elif isinstance(raw, list):
                # migrate Ø§Ø² ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… â€” ÙÙ‚Ø· ÛµÛ°Û° ØªØ§ Ø¢Ø®Ø±
                return set(raw[-500:])
    except: pass
    return set()

def save_seen(seen: set):
    now_ts    = datetime.now(timezone.utc).timestamp()
    cutoff_ts = now_ts - SEEN_TTL_HOURS * 3600
    try:
        existing = {}
        if Path(SEEN_FILE).exists():
            raw = json.load(open(SEEN_FILE))
            if isinstance(raw, dict):
                existing = {k: v for k, v in raw.items() if v > cutoff_ts}
    except: existing = {}
    for eid in seen:
        if eid not in existing: existing[eid] = now_ts
    if len(existing) > 5000:
        existing = dict(sorted(existing.items(), key=lambda x: x[1], reverse=True)[:5000])
    json.dump(existing, open(SEEN_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# run_state â€” last_run Ø¨Ø±Ø§ÛŒ cutoff Ù‡ÙˆØ´Ù…Ù†Ø¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_run_state() -> datetime:
    """Ø¢Ø®Ø±ÛŒÙ† Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ â€” Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ cutoff"""
    try:
        if Path(RUN_STATE_FILE).exists():
            d   = json.load(open(RUN_STATE_FILE))
            ts  = d.get("last_run", 0)
            if ts:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
    except: pass
    # Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§: MAX_LOOKBACK_MIN Ø¨Ù‡ Ø¹Ù‚Ø¨
    return datetime.now(timezone.utc) - timedelta(minutes=MAX_LOOKBACK_MIN)

def save_run_state():
    existing = {}
    try:
        if Path(RUN_STATE_FILE).exists():
            existing = json.load(open(RUN_STATE_FILE))
    except: pass
    existing["last_run"] = datetime.now(timezone.utc).timestamp()
    json.dump(existing, open(RUN_STATE_FILE, "w"))

def load_stories() -> list:
    try:
        if Path(STORIES_FILE).exists():
            raw = json.load(open(STORIES_FILE))
            # migrate ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ… (2-tuple) Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ (3-tuple)
            result = []
            for item in raw:
                if isinstance(item, (list, tuple)) and len(item) == 2:
                    title = item[0]
                    result.append([title, list(_bag(title)), list(_entity_triple(title))])
                elif isinstance(item, (list, tuple)) and len(item) == 3:
                    result.append(item)
            return result
    except: pass
    return []

def save_stories(stories):
    json.dump(stories[-MAX_STORIES:], open(STORIES_FILE, "w"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªØ±Ø¬Ù…Ù‡ â€” Gemini Ø§ÙˆÙ„ØŒ MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† fallback
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_MODELS = [
    "gemini-2.0-flash",
    "gemini-1.5-flash",
    "gemini-1.5-flash-8b",
]

# ØªØ´Ø®ÛŒØµ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
def _is_farsi(text: str) -> bool:
    fa_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
    return fa_chars / max(len(text), 1) > 0.3

# ØªØ±Ø¬Ù…Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù† ÛŒÚ© Ù…ØªÙ† Ø§Ø² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ MyMemory
async def _translate_mymemory(client: httpx.AsyncClient, text: str) -> str:
    """MyMemory API â€” Ø±Ø§ÛŒÚ¯Ø§Ù†ØŒ Ø¨Ø¯ÙˆÙ† Ú©Ù„ÛŒØ¯ØŒ ØªØ§ ÛµÛ°Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¯Ø± Ø±ÙˆØ²"""
    if not text or _is_farsi(text):
        return text
    try:
        url = "https://api.mymemory.translated.net/get"
        r = await client.get(url,
            params={"q": text[:500], "langpair": "en|fa", "de": "warbot@github.com"},
            timeout=httpx.Timeout(8.0))
        if r.status_code == 200:
            data = r.json()
            tr = data.get("responseData", {}).get("translatedText", "")
            # MyMemory Ú¯Ø§Ù‡ÛŒ MYMEMORY WARNING Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
            if tr and "MYMEMORY WARNING" not in tr and len(tr) > 5:
                return tr
    except Exception as e:
        log.debug(f"MyMemory: {e}")
    return text

GEMINI_PROMPT = """ØªÙˆ ÛŒÚ© Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ø¬Ù†Ú¯ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒ. Ø§ÛŒÙ† Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ú©Ù†.

Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø±Ø§ Ø±Ø¹Ø§ÛŒØª Ú©Ù†:
###ITEM_0###
T: [Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ Ø¯Ø± ÛŒÚ© Ø®Ø·]
B: [Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„]
###ITEM_1###
T: [Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ]
B: [Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ]

Ù‚ÙˆØ§Ù†ÛŒÙ†:
- Ø§Ø³Ø§Ù…ÛŒ: Netanyahu=Ù†ØªØ§Ù†ÛŒØ§Ù‡ÙˆØŒ Khamenei=Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒØŒ IRGC=Ø³Ù¾Ø§Ù‡ØŒ IDF=Ø§Ø±ØªØ´ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ØŒ CENTCOM=Ø³ØªØ§Ø¯ Ù…Ø±Ú©Ø²ÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§
- Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¢Ù…Ø§Ø±ØŒ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±
- Ø§Ú¯Ù‡ Ø®Ø¨Ø± ÙØ§Ø±Ø³ÛŒÙ‡: ÙÙ‚Ø· Ù¾Ø§Ú©ÛŒØ²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†

===Ø®Ø¨Ø±Ù‡Ø§===
{items}"""

async def _translate_gemini(client: httpx.AsyncClient, articles: list) -> list | None:
    """ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Gemini â€” None Ø§Ú¯Ù‡ fail Ø´Ø¯"""
    if not GEMINI_API_KEY:
        return None
    items_txt = "".join(
        f"###ITEM_{i}###\nEN_TITLE: {t[:300]}\nEN_BODY: {s[:400]}\n\n"
        for i, (t, s) in enumerate(articles)
    )
    state = {}
    try:
        if Path(GEMINI_STATE_FILE).exists():
            state = json.load(open(GEMINI_STATE_FILE))
    except: pass
    models = state.get("models_order", GEMINI_MODELS)
    base   = "https://generativelanguage.googleapis.com/v1beta/models"

    for model in models:
        try:
            r = await client.post(
                f"{base}/{model}:generateContent?key={GEMINI_API_KEY}",
                json={
                    "contents": [{"parts": [{"text": GEMINI_PROMPT.format(items=items_txt)}]}],
                    "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
                },
                timeout=httpx.Timeout(40.0)
            )
            if r.status_code == 429:
                log.warning(f"Gemini {model}: rate-limit"); continue
            if r.status_code != 200:
                log.warning(f"Gemini {model}: HTTP {r.status_code} â€” {r.text[:200]}"); continue

            text_out = r.json()["candidates"][0]["content"]["parts"][0]["text"]
            log.info(f"ğŸŒ Gemini {model} OK")

            results = list(articles)
            ok_count = 0
            for i, (orig_t, orig_s) in enumerate(articles):
                blk = re.search(rf"###ITEM_{i}###\s*(.*?)(?=###ITEM_\d+###|\Z)", text_out, re.DOTALL)
                if not blk: continue
                block   = blk.group(1)
                t_match = re.search(r"^T:\s*(.+)$", block, re.MULTILINE)
                b_match = re.search(r"^B:\s*([\s\S]+?)$", block, re.MULTILINE)
                fa_t = t_match.group(1).strip() if t_match else ""
                fa_b = b_match.group(1).strip() if b_match else ""
                # fallback: Ù‡Ù…Ù‡ block Ø±Ø§ Ø¹Ù†ÙˆØ§Ù† Ø¨Ú¯ÛŒØ±
                if not fa_t:
                    fa_t = block.strip().split('\n')[0]
                if len(fa_t) > 5:
                    results[i] = (fa_t, fa_b or orig_s)
                    ok_count += 1
            log.info(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡: {ok_count}/{len(articles)} Ø®Ø¨Ø±")
            # Ù…Ø¯Ù„ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø±Ø§ Ø§ÙˆÙ„ Ø¨Ú¯Ø°Ø§Ø±
            state["models_order"] = [model] + [m for m in models if m != model]
            json.dump(state, open(GEMINI_STATE_FILE, "w"))
            return results
        except Exception as e:
            log.warning(f"Gemini {model}: {e}"); continue
    return None

async def translate_batch(client: httpx.AsyncClient, articles: list) -> list:
    """
    ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª:
    1. Gemini (Ø§Ú¯Ù‡ API key Ø¯Ø§Ø±ÛŒÙ…)
    2. MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† (ÙÙ‚Ø· Ø¹Ù†ÙˆØ§Ù†)
    3. Ù…ØªÙ† Ø§ØµÙ„ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØ±Ø¬Ù…Ù‡)
    """
    if not articles:
        return []

    results = list(articles)

    # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û±: Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if GEMINI_API_KEY:
        log.info(f"ğŸŒ Gemini: ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø®Ø¨Ø±...")
        gemini_res = await _translate_gemini(client, articles)
        if gemini_res:
            return gemini_res
        log.warning("ğŸŒ Gemini fail â€” fallback Ø¨Ù‡ MyMemory")
    else:
        log.info("ğŸŒ GEMINI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ â€” Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù†")

    # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û²: MyMemory â€” Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info(f"ğŸŒ MyMemory: ØªØ±Ø¬Ù…Ù‡ {len(articles)} Ø¹Ù†ÙˆØ§Ù†...")
    sema = asyncio.Semaphore(5)

    async def _tr(orig_t, orig_s):
        async with sema:
            if _is_farsi(orig_t):
                return (orig_t, orig_s)
            fa_t = await _translate_mymemory(client, orig_t)
            return (fa_t, orig_s)

    translated = await asyncio.gather(*[_tr(t, s) for t, s in articles])
    ok = sum(1 for i, (fa, _) in enumerate(translated) if fa != articles[i][0])
    log.info(f"ğŸŒ MyMemory: {ok}/{len(articles)} ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯")
    return list(translated)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Sentiment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BREAKING_KEYWORDS = [
    "breaking","urgent","alert","just in","explosion","airstrike","killed","dead",
    "war","attack","strike","nuclear","bomb","missile","invasion",
    "Ø­Ù…Ù„Ù‡","Ú©Ø´ØªÙ‡","Ø§Ù†ÙØ¬Ø§Ø±","Ø´Ù‡ÛŒØ¯","Ù…ÙˆØ´Ú©","ÙÙˆØ±ÛŒ","Ø®Ø¨Ø± ÙÙˆØ±ÛŒ","Ø§Ø¹Ù„Ø§Ù… Ø¬Ù†Ú¯",
]
IMPORTANCE_BOOST = {
    "ğŸ’€":4, "ğŸ”´":3, "ğŸ’¥":3, "ğŸš€":3, "â˜¢ï¸":3,
    "âœˆï¸":2, "ğŸš¢":2, "ğŸ›¡ï¸":2, "ğŸ•µï¸":2,
    "ğŸ”¥":1, "ğŸ’°":1, "âš ï¸":1,
}

SENTIMENT_RULES = [
    ("ğŸ’€", ["killed","dead","casualties","fatalities","wounded","martyred","massacre"],
           ["Ú©Ø´ØªÙ‡","Ø´Ù‡ÛŒØ¯","ØªÙ„ÙØ§Øª","Ú©Ø´ØªØ§Ø±","Ù…Ø¬Ø±ÙˆØ­"]),
    ("ğŸ”´", ["attack","struck","assault","launched attack","opened fire","bombed","targeted"],
           ["Ø­Ù…Ù„Ù‡","Ø¶Ø±Ø¨Ù‡","Ù…ÙˆØ±Ø¯ Ù‡Ø¯Ù","Ø­Ù…Ù„Ù‡ Ú©Ø±Ø¯"]),
    ("ğŸ’¥", ["explosion","blast","detonation","explode","blew up"],
           ["Ø§Ù†ÙØ¬Ø§Ø±","Ù…Ù†ÙØ¬Ø±","ØªØ±Ú©ÛŒØ¯"]),
    ("âœˆï¸", ["airstrike","air strike","air raid","warplane","f-35","f-15","b-52","f-16"],
           ["Ø­Ù…Ù„Ù‡ Ù‡ÙˆØ§ÛŒÛŒ","Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†","Ø¬Ù†Ú¯Ù†Ø¯Ù‡"]),
    ("ğŸš€", ["missile","rocket","ballistic","cruise missile","drone strike","hypersonic"],
           ["Ù…ÙˆØ´Ú©","Ù¾Ù‡Ù¾Ø§Ø¯","Ù…ÙˆØ´Ú© Ø¨Ø§Ù„Ø³ØªÛŒÚ©","Ø±Ø§Ú©Øª"]),
    ("â˜¢ï¸", ["nuclear","uranium","enrichment","natanz","fordow","centrifuge","iaea"],
           ["Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ","Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…","ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ","Ù†Ø·Ù†Ø²","ÙØ±Ø¯Ùˆ","Ø³Ø§Ù†ØªØ±ÛŒÙÛŒÙˆÚ˜"]),
    ("ğŸš¢", ["navy","naval","warship","aircraft carrier","strait of hormuz","red sea"],
           ["Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ","Ù†Ø§Ùˆ","ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²","Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø®"]),
    ("ğŸ•µï¸", ["intelligence","mossad","cia","spy","covert","assassination","sabotage","cyber"],
           ["Ø¬Ø§Ø³ÙˆØ³ÛŒ","Ù…ÙˆØ³Ø§Ø¯","Ø®Ø±Ø§Ø¨Ú©Ø§Ø±ÛŒ","ØªØ±ÙˆØ±","Ø³Ø§ÛŒØ¨Ø±ÛŒ"]),
    ("ğŸ›¡ï¸", ["intercept","shot down","iron dome","air defense","patriot"],
           ["Ø±Ù‡Ú¯ÛŒØ±ÛŒ","Ù¾Ø¯Ø§ÙÙ†Ø¯","Ú¯Ù†Ø¨Ø¯ Ø¢Ù‡Ù†ÛŒÙ†","Ø³Ø±Ù†Ú¯ÙˆÙ†"]),
    ("ğŸ”¥", ["escalat","tension","brink of war","retaliat","provocation"],
           ["ØªØ´Ø¯ÛŒØ¯","ØªÙ†Ø´","ØªÙ„Ø§ÙÛŒ","Ø¢Ø³ØªØ§Ù†Ù‡ Ø¬Ù†Ú¯"]),
    ("ğŸ’°", ["sanction","embargo","swift","freeze assets"],
           ["ØªØ­Ø±ÛŒÙ…","Ù…Ø­Ø§ØµØ±Ù‡ Ø§Ù‚ØªØµØ§Ø¯ÛŒ"]),
    ("âš ï¸", ["threat","warn","warning","ultimatum","red line","will respond"],
           ["ØªÙ‡Ø¯ÛŒØ¯","Ù‡Ø´Ø¯Ø§Ø±","Ø®Ø· Ù‚Ø±Ù…Ø²","Ø§ÙˆÙ„ØªÛŒÙ…Ø§ØªÙˆÙ…"]),
    ("ğŸ¤", ["negotiation","talks","deal","diplomacy","ceasefire","agreement"],
           ["Ù…Ø°Ø§Ú©Ø±Ù‡","ØªÙˆØ§ÙÙ‚","Ø¢ØªØ´â€ŒØ¨Ø³","Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ"]),
    ("ğŸ“œ", ["statement","declared","announced","press conference","spokesperson"],
           ["Ø¨ÛŒØ§Ù†ÛŒÙ‡","Ø§Ø¹Ù„Ø§Ù…","Ù†Ø´Ø³Øª Ø®Ø¨Ø±ÛŒ","Ø³Ø®Ù†Ú¯Ùˆ"]),
]

def analyze_sentiment(text: str) -> list:
    txt = text.lower()
    found = []
    for icon, en_kws, fa_kws in SENTIMENT_RULES:
        if any(kw in txt for kw in en_kws) or any(kw in txt for kw in fa_kws):
            found.append(icon)
        if len(found) >= 3: break
    return found or ["ğŸ“°"]

def calc_importance(title: str, body: str, icons: list, stype: str) -> int:
    txt = (title + " " + body).lower()
    score = sum(IMPORTANCE_BOOST.get(ic, 0) for ic in icons)
    if any(k in txt for k in BREAKING_KEYWORDS): score += 2
    if stype == "tw" and score > 0: score += 1
    return min(score, 10)

def sentiment_bar(icons): return "  ".join(icons)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Telegram Ø§Ø±Ø³Ø§Ù„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _tgapi(path: str) -> str:
    return f"https://api.telegram.org/bot{BOT_TOKEN}/{path}"

async def tg_send_text(client: httpx.AsyncClient, text: str) -> bool:
    text = text[:MAX_MSG_LEN]
    for attempt in range(3):
        try:
            r = await client.post(_tgapi("sendMessage"),
                json={"chat_id": CHANNEL_ID, "text": text,
                      "parse_mode": "HTML", "disable_web_page_preview": False},
                timeout=httpx.Timeout(15.0))
            d = r.json()
            if r.status_code == 200 and d.get("ok"): return True
            if d.get("error_code") == 429:
                wait = d.get("parameters", {}).get("retry_after", 20)
                await asyncio.sleep(wait)
            elif attempt < 2:
                await asyncio.sleep(3)
        except Exception as e:
            log.warning(f"TG send: {e}")
            if attempt < 2: await asyncio.sleep(5)
    return False

async def tg_send_photo(client: httpx.AsyncClient, buf: io.BytesIO,
                         caption: str) -> bool:
    caption = caption[:1024]
    try:
        buf.seek(0)
        r = await client.post(_tgapi("sendPhoto"),
            data={"chat_id": CHANNEL_ID, "caption": caption, "parse_mode": "HTML"},
            files={"photo": ("card.jpg", buf, "image/jpeg")},
            timeout=httpx.Timeout(20.0))
        return r.status_code == 200 and r.json().get("ok", False)
    except Exception as e:
        log.warning(f"TG photo: {e}"); return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIL Ú©Ø§Ø±Øª Ø®Ø¨Ø±ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BG_DARK  = (14, 16, 22)
BG_BAR   = (22, 26, 34)
FG_WHITE = (235, 237, 242)
FG_GREY  = (120, 132, 148)
ACCENT_MAP = {
    "ğŸ‡®ğŸ‡·":(180,40,40), "ğŸ‡®ğŸ‡±":(30,90,180), "ğŸ‡ºğŸ‡¸":(40,80,160),
    "ğŸ”":(60,130,80), "ğŸŒ":(100,60,130), "ğŸ›ï¸":(140,100,40),
}
ICON_BG = {
    "ğŸ’€":(140,20,20),"ğŸ”´":(180,30,30),"ğŸ’¥":(190,80,10),
    "âœˆï¸":(20,90,160),"ğŸš€":(100,20,160),"â˜¢ï¸":(0,130,50),
    "ğŸš¢":(10,80,140),"ğŸ•µï¸":(60,55,70),"ğŸ›¡ï¸":(20,110,80),
    "ğŸ”¥":(180,60,0),"ğŸ’°":(130,110,0),"âš ï¸":(160,110,0),
    "ğŸ¤":(20,120,100),"ğŸ“œ":(60,80,100),"ğŸ“°":(45,58,72),
}

def _get_accent(src, urgent):
    if urgent: return (210, 40, 40)
    for k, v in ACCENT_MAP.items():
        if src.startswith(k) or k in src: return v
    return (80, 110, 140)

def _wrap(text, chars):
    words, lines_out, cur = text.split(), [], ""
    for w in words:
        if len(cur) + len(w) + 1 <= chars: cur = (cur + " " + w).strip()
        else:
            if cur: lines_out.append(cur)
            cur = w
    if cur: lines_out.append(cur)
    return lines_out

def _fonts():
    try:
        bold = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
        reg  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        sm   = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 13)
        return bold, reg, sm
    except:
        d = ImageFont.load_default(); return d, d, d

def make_news_card(headline, fa_text, src, dt_str,
                   urgent=False, sentiment_icons=None):
    if not PIL_OK: return None
    try:
        W, H = 960, 310
        acc = _get_accent(src, urgent)
        img = Image.new("RGB", (W, H), BG_DARK)
        drw = ImageDraw.Draw(img)
        F_H, F_B, F_sm = _fonts()

        drw.rectangle([(0,0),(W,5)], fill=acc)
        drw.rectangle([(0,5),(W,58)], fill=BG_BAR)
        drw.rectangle([(0,58),(W,61)], fill=acc)
        drw.text((18,18), src[:55],     font=F_sm, fill=acc)
        drw.text((W-170,18), dt_str[:25], font=F_sm, fill=FG_GREY)

        display = fa_text if (fa_text and len(fa_text) > 5) else headline
        y = 72
        for line in _wrap(display, 50)[:4]:
            drw.text((W-18, y), line, font=F_H, fill=FG_WHITE, anchor="ra")
            y += 30

        drw.rectangle([(0,H-56),(W,H)], fill=BG_BAR)
        drw.rectangle([(0,H-58),(W,H-56)], fill=acc)
        x_pos = 16
        for ico in (sentiment_icons or ["ğŸ“°"])[:4]:
            bg = ICON_BG.get(ico, (50,65,75))
            drw.rounded_rectangle([(x_pos-2,H-52),(x_pos+38,H-6)], radius=7, fill=bg)
            drw.text((x_pos+2,H-50), ico, font=F_H, fill=(255,255,255))
            x_pos += 50

        if urgent: drw.rectangle([(0,61),(5,H-58)], fill=acc)

        buf = io.BytesIO()
        img.save(buf, "JPEG", quality=85)
        buf.seek(0)
        return buf
    except Exception as e:
        log.debug(f"card: {e}"); return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² Ø³Ø§ÛŒØª (og:image ØªØµÙˆÛŒØ± Ù…Ù‚Ø§Ù„Ù‡ â€” Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± (Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ â€” Ø¹Ú©Ø³ Ù…Ù‚Ø§Ù„Ù‡)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ (Ù†Ù‡ Ù„ÙˆÚ¯Ùˆ â€” Ø¹Ú©Ø³ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# URLâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒ Ù„ÙˆÚ¯Ùˆ Ø¯Ø§Ø±Ù†Ø¯
_SKIP_IMG_PATTERNS = [
    "logo","icon","favicon","sprite","avatar","placeholder",
    "default","blank","spacer","1x1","pixel","brand","masthead",
    "no-image","no-photo","profile","author","byline","signature",
    "/ad/","/ads/","banner","promo","subscribe","newsletter",
]
# CSS selector Ù‡Ø§ÛŒ ordered Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±
_IMG_SELECTORS = [
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ article Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    "article figure img",
    "article .featured-image img",
    "article .hero-image img",
    "[class*='article-image'] img",
    "[class*='news-image'] img",
    "[class*='story-image'] img",
    "[class*='featured-img'] img",
    "[class*='lead-image'] img",
    "[class*='post-image'] img",
    "[class*='entry-image'] img",
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
    ".detail-media img",
    ".news-photo img",
    ".content-media img",
    ".article-img img",
    ".body img",
    # Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØªØ±
    "figure img",
    "picture source",
    "picture img",
    ".content img",
    "article img",
]

async def fetch_article_image(client: httpx.AsyncClient, url: str) -> "io.BytesIO | None":
    """
    ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡:
    Û±. CSS selectors Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† ØªØµÙˆÛŒØ± Ø®Ø¨Ø± Ø¯Ø± Ù…ØªÙ† Ù…Ù‚Ø§Ù„Ù‡
    Û². og:image / twitter:image ÙÙ‚Ø· Ø§Ú¯Ù‡ Ø¹Ø±Ø¶ â‰¥ Û¶Û°Û° Ø¨Ø§Ø´Ø¯
    Û³. ÙÛŒÙ„ØªØ± Ù„ÙˆÚ¯Ùˆ: Ø­Ø¬Ù… < Û±ÛµKB ÛŒØ§ Ø§Ø¨Ø¹Ø§Ø¯ < ÛµÛ°Û°Ã—Û²Û¸Û° ÛŒØ§ ratio < 1.3 â†’ Ø±Ø¯
    """
    if not url or len(url) < 10:
        return None
    skip_domains = ("t.me", "twitter.com", "x.com", "google.com/rss",
                    "feeds.reuters", "feeds.bbci", "feed.", "rss.")
    if any(d in url for d in skip_domains):
        return None

    try:
        r = await client.get(url,
            timeout=httpx.Timeout(10.0),
            headers={**COMMON_UA,
                     "Accept": "text/html,*/*;q=0.8",
                     "Sec-Fetch-Dest": "document"},
            follow_redirects=True)
        if r.status_code != 200:
            return None

        soup = BeautifulSoup(r.text, "html.parser")

        # â”€â”€ Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        candidates: list[tuple[str, int]] = []  # (url, priority)

        # Priority 1: CSS selector Ù‡Ø§ÛŒ article/news
        for sel in _IMG_SELECTORS:
            for el in soup.select(sel)[:3]:
                src = None
                if el.name == "source":
                    src = el.get("srcset", "").split(" ")[0]
                else:
                    # srcset â†’ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ†
                    ss = el.get("srcset", "")
                    if ss:
                        parts = [p.strip().split(" ") for p in ss.split(",") if p.strip()]
                        best = sorted(parts, key=lambda x: int(x[1].rstrip("w")) if len(x)>1 and x[1].rstrip("w").isdigit() else 0, reverse=True)
                        if best: src = best[0][0]
                    if not src:
                        src = el.get("src") or el.get("data-src") or el.get("data-lazy-src")
                if src and not src.startswith("data:"):
                    candidates.append((src, 10))

        # Priority 2: og:image
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            candidates.append((og["content"], 5))

        # og:image:width Ø¨Ø±Ø±Ø³ÛŒ
        og_w = soup.find("meta", property="og:image:width")
        if og_w:
            try:
                w = int(og_w.get("content", 0))
                if w < 500 and candidates:
                    # og:image Ú©ÙˆÚ†Ú© Ø§Ø³Øª â†’ Ø§ÙˆÙ„ÙˆÛŒØª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
                    candidates = [(u, p-3 if u == og.get("content") else p) for u, p in candidates]
            except: pass

        # Priority 3: twitter:image
        for name in ("twitter:image", "twitter:image:src"):
            tw = soup.find("meta", attrs={"name": name})
            if tw and tw.get("content"):
                candidates.append((tw["content"], 4)); break

        if not candidates:
            return None

        # â”€â”€ ÙÛŒÙ„ØªØ± Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        from urllib.parse import urlparse
        base_p = urlparse(r.url)  # URL Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø¹Ø¯ Ø§Ø² redirect)

        # Ù…Ø±ØªØ¨ Ø§Ø² Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
        candidates.sort(key=lambda x: -x[1])
        tried_urls = set()

        for img_url, _ in candidates[:8]:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ URL
            if img_url.startswith("//"):
                img_url = "https:" + img_url
            elif img_url.startswith("/"):
                img_url = f"{base_p.scheme}://{base_p.netloc}{img_url}"
            elif not img_url.startswith("http"):
                continue

            # Ø­Ø°Ù query string Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
            clean_url = img_url.lower().split("?")[0]

            # ÙÛŒÙ„ØªØ± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù„ÙˆÚ¯Ùˆ Ø¯Ø± URL
            if any(p in clean_url for p in _SKIP_IMG_PATTERNS):
                log.debug(f"ğŸ–¼ skip-url: {img_url[:60]}")
                continue

            if img_url in tried_urls:
                continue
            tried_urls.add(img_url)

            # Ø¯Ø§Ù†Ù„ÙˆØ¯
            try:
                ir = await client.get(img_url,
                    timeout=httpx.Timeout(12.0),
                    headers={**COMMON_UA, "Accept": "image/*,*/*;q=0.5"},
                    follow_redirects=True)
                if ir.status_code != 200:
                    continue
            except Exception as de:
                log.debug(f"ğŸ–¼ dl-err: {de}"); continue

            raw   = ir.content
            ctype = ir.headers.get("content-type", "")

            # Ø­Ø¬Ù… Ú©Ù… â†’ Ù„ÙˆÚ¯Ùˆ
            if len(raw) < 15_000:
                log.debug(f"ğŸ–¼ skip-small: {len(raw)}B")
                continue

            # Ú†Ú© Ù†ÙˆØ¹ ØªØµÙˆÛŒØ±
            is_img = (
                ctype.startswith("image/") or
                raw[:3]  == b'\xff\xd8\xff' or
                raw[:8]  == b'\x89PNG\r\n\x1a\n' or
                raw[:6]  in (b'GIF87a', b'GIF89a') or
                raw[:4]  == b'RIFF' or
                raw[:4]  == b'WEBP'
            )
            if not is_img:
                continue

            # PIL: Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ resize
            if PIL_OK:
                try:
                    tmp = Image.open(io.BytesIO(raw))
                    w, h = tmp.size
                    # Ø¹Ø±Ø¶ < ÛµÛ°Û° ÛŒØ§ Ø§Ø±ØªÙØ§Ø¹ < Û²Û¸Û° â†’ Ù„ÙˆÚ¯Ùˆ/Ø¨Ù†Ø±
                    if w < 500 or h < 280:
                        log.debug(f"ğŸ–¼ skip-dim: {w}Ã—{h}")
                        continue
                    # Ù†Ø³Ø¨Øª < 1.3 â†’ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù…Ø±Ø¨Ø¹ ÛŒØ§ Ø¹Ù…ÙˆØ¯ÛŒ = Ù„ÙˆÚ¯Ùˆ
                    ratio = w / max(h, 1)
                    if ratio < 1.3:
                        log.debug(f"ğŸ–¼ skip-ratio: {ratio:.2f} ({w}Ã—{h})")
                        continue
                    img_rgb = tmp.convert("RGB")
                    if w > 1600 or h > 1000:
                        img_rgb.thumbnail((1600, 1000), Image.LANCZOS)
                    out = io.BytesIO()
                    img_rgb.save(out, "JPEG", quality=88, optimize=True)
                    out.seek(0)
                    log.info(f"ğŸ–¼ âœ… {w}Ã—{h} r={ratio:.1f}  {img_url[:55]}")
                    return out
                except Exception as pe:
                    log.debug(f"ğŸ–¼ PIL-err: {pe}"); continue
            else:
                buf = io.BytesIO(raw); buf.seek(0)
                return buf

        return None

    except Exception as e:
        log.debug(f"fetch_img {url[:55]}: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÛŒÚ© Ú†Ø±Ø®Ù‡ fetch â†’ filter â†’ send
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def _run_cycle(client: httpx.AsyncClient,
                     seen: set, stories: list,
                     cutoff: datetime) -> tuple:
    """
    ÛŒÚ© Ú†Ø±Ø®Ù‡ Ú©Ø§Ù…Ù„.
    Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯: (seen, stories, cutoff_for_next)
    """
    cycle_start = datetime.now(timezone.utc)
    save_run_state()

    # â”€â”€ fetch Ù…ÙˆØ§Ø²ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    raw = await fetch_all(client, cutoff)
    log.info(f"  ğŸ“¥ {len(raw)} Ø¢ÛŒØªÙ… Ø®Ø§Ù…")

    # â”€â”€ Ù¾Ø±Ø¯Ø§Ø²Ø´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    collected = []
    cnt_old = cnt_irrel = cnt_dup = cnt_story = 0

    for entry, src_name, src_type, is_emb in raw:
        eid = make_id(entry)
        if eid in seen:                         cnt_dup   += 1; continue
        if not is_fresh(entry, cutoff):         cnt_old   += 1; continue
        t   = clean_html(entry.get("title",""))
        s   = clean_html(entry.get("summary") or entry.get("description") or "")
        if not is_war_relevant(f"{t} {s}", is_embassy=is_emb,
                               is_tg=(src_type=="tg"), is_tw=(src_type=="tw")):
            cnt_irrel += 1; continue
        if is_story_dup(t, stories):            cnt_story += 1; continue
        collected.append((eid, entry, src_name, src_type, is_emb))
        stories = register_story(t, stories)

    log.info(f"  ğŸ“Š Ù‚Ø¯ÛŒÙ…ÛŒ:{cnt_old} Ù†Ø§Ù…Ø±ØªØ¨Ø·:{cnt_irrel} dup:{cnt_dup} story:{cnt_story} âœ…{len(collected)}")

    collected = list(reversed(collected))[:MAX_NEW_PER_RUN]

    if not collected:
        log.info("  ğŸ’¤ Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ù†ÛŒØ³Øª")
        save_seen(seen); save_stories(stories)
        return seen, stories, cycle_start

    # â”€â”€ ØªØ±Ø¬Ù…Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    arts_in = [
        (trim(clean_html(e.get("title","")), 400),
         trim(clean_html(e.get("summary") or e.get("description") or ""), 600))
        for _, e, _, _, _ in collected
    ]
    log.info(f"  ğŸŒ ØªØ±Ø¬Ù…Ù‡ {len(arts_in)} Ø®Ø¨Ø±...")
    translations = await translate_batch(client, arts_in)

    # â”€â”€ Ø§Ø±Ø³Ø§Ù„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    sent = 0
    for i, (eid, entry, src_name, stype, is_emb) in enumerate(collected):
        fa_title, fa_body = translations[i]
        en_title = arts_in[i][0]
        link     = entry.get("link","")
        dt_str   = format_dt(entry)

        title_is_fa = _is_farsi(fa_title) if fa_title else False
        orig_is_fa  = _is_farsi(en_title)
        if not title_is_fa and not orig_is_fa:
            log.info(f"  â­ skip(noFA): {en_title[:50]}"); continue

        display = fa_title.strip() if title_is_fa else en_title.strip()
        body_fa = ""
        if fa_body and _is_farsi(fa_body) and len(fa_body) > 15:
            body_fa = fa_body.strip()
        elif _is_farsi(arts_in[i][1]):
            body_fa = arts_in[i][1].strip()

        s_bar = sentiment_bar(analyze_sentiment(f"{fa_title} {fa_body} {en_title}"))
        cap   = [s_bar, f"<b>{esc(display)}</b>"]
        if body_fa and body_fa[:50] not in display[:50]:
            cap += ["", esc(trim(body_fa, 800))]
        if dt_str: cap.append(f"\nğŸ• {dt_str}")
        caption = "\n".join(cap)

        done = False
        if link and stype == "rss":
            img = await fetch_article_image(client, link)
            if img:
                ok = await tg_send_photo(client, img, caption[:1024])
                if ok: done = True; log.info("    ğŸ“¸ ØªØµÙˆÛŒØ±+ÙØ§Ø±Ø³ÛŒ")

        if not done:
            ok = await tg_send_text(client, caption)
            if ok: done = True; log.info("    âœ‰ï¸ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ")

        if done:
            seen.add(eid); sent += 1
        await asyncio.sleep(SEND_DELAY)

    save_seen(seen); save_stories(stories)
    log.info(f"  ğŸ {sent}/{len(collected)} Ø§Ø±Ø³Ø§Ù„  seen:{len(seen)}")
    return seen, stories, cycle_start


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# main â€” Ø­Ù„Ù‚Ù‡ Ø¯Ø§Ø¦Ù…ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    global _TW_SEMA
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ BOT_TOKEN ÛŒØ§ CHANNEL_ID ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡!"); return

    _TW_SEMA = asyncio.Semaphore(20)

    # cutoff Ø§ÙˆÙ„ÛŒÙ‡
    last_run = load_run_state()
    now_utc  = datetime.now(timezone.utc)
    cutoff   = last_run - timedelta(minutes=CUTOFF_BUFFER_MIN)
    if cutoff < now_utc - timedelta(minutes=MAX_LOOKBACK_MIN):
        cutoff = now_utc - timedelta(minutes=MAX_LOOKBACK_MIN)

    seen    = load_seen()
    stories = load_stories()

    mode = "GitHub CI" if _CI else "Ù…Ø­Ù„ÛŒ â€” Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª"
    log.info("=" * 70)
    log.info(f"ğŸš€ WarBot v20 | {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù† %Y/%m/%d')}")
    log.info(f"   mode={mode}  max={BOT_MAX_RUNTIME_MIN}min  interval={LOOP_INTERVAL_SEC}s")
    log.info(f"   ğŸ“¡ {len(ALL_RSS_FEEDS)} RSS  ğŸ“¢ {len(TELEGRAM_CHANNELS)} TG  ğ• {len(TWITTER_HANDLES)} TW")
    log.info(f"   seen:{len(seen)}  stories:{len(stories)}  PIL:{'âœ…' if PIL_OK else 'âŒ'}")
    log.info("=" * 70)

    wall_start = datetime.now(timezone.utc)
    loop_n     = 0
    limits     = httpx.Limits(max_connections=100, max_keepalive_connections=30)

    async with httpx.AsyncClient(follow_redirects=True, limits=limits) as client:
        await build_twitter_pools(client)

        while True:
            loop_n += 1
            elapsed_min = (datetime.now(timezone.utc) - wall_start).total_seconds() / 60
            log.info(f"\n{'â”'*55}")
            log.info(f"  âŸ³ Loop #{loop_n}  elapsed={elapsed_min:.1f}min"
                     f"  {datetime.now(TEHRAN_TZ).strftime('%H:%M ØªÙ‡Ø±Ø§Ù†')}")

            t0 = datetime.now(timezone.utc)
            try:
                seen, stories, next_cutoff = await _run_cycle(
                    client, seen, stories, cutoff)
                # cutoff Ø¨Ø¹Ø¯ÛŒ = Ø´Ø±ÙˆØ¹ Ø§ÛŒÙ† cycle - buffer
                cutoff = next_cutoff - timedelta(minutes=CUTOFF_BUFFER_MIN)
            except Exception as e:
                log.error(f"  âŒ cycle error: {e}")
                import traceback; log.debug(traceback.format_exc())

            took = (datetime.now(timezone.utc) - t0).total_seconds()
            log.info(f"  â± cycle took {took:.0f}s")

            # Ø¨Ø±Ø±Ø³ÛŒ exit Ø¨Ø±Ø§ÛŒ CI
            elapsed_min = (datetime.now(timezone.utc) - wall_start).total_seconds() / 60
            if elapsed_min >= BOT_MAX_RUNTIME_MIN:
                log.info(f"  â¹ CI timeout ({BOT_MAX_RUNTIME_MIN}min) â€” Ø®Ø±ÙˆØ¬ Ø³Ø§Ù„Ù…")
                break

            # ØµØ¨Ø± ØªØ§ cycle Ø¨Ø¹Ø¯ÛŒ
            wait = max(5.0, LOOP_INTERVAL_SEC - took)
            log.info(f"  ğŸ’¤ {wait:.0f}s ØªØ§ Ú†Ø±Ø®Ù‡ Ø¨Ø¹Ø¯ÛŒ...")
            await asyncio.sleep(wait)


if __name__ == "__main__":
    asyncio.run(main())
