"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ›¡ï¸ Military Intel Bot â€” Anti-Freeze & Fast AI Edition           â•‘
â•‘     Iran Â· Israel Â· USA  |  REST API + Hard Timeouts + RSSHub           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os, json, hashlib, asyncio, logging
from pathlib import Path
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser, httpx, pytz

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("MilBot")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BOT_TOKEN      = os.environ.get("BOT_TOKEN", "")
CHANNEL_ID     = os.environ.get("CHANNEL_ID", "")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

SEEN_FILE       = "seen.json"
MAX_NEW_PER_RUN = 25          
SEND_DELAY      = 3  
TEHRAN_TZ       = pytz.timezone("Asia/Tehran")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û±. Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†â€ŒØ¨Ø§Ø² Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RSS_FEEDS = [
    {"name": "ğŸŒ Axios NatSec",       "url": "https://api.axios.com/feed/national-security"},
    {"name": "ğŸŒ Reuters Defense",    "url": "https://feeds.reuters.com/reuters/worldNews"},
    {"name": "ğŸŒ CNN Middle East",    "url": "http://rss.cnn.com/rss/edition_meast.rss"},
    {"name": "ğŸŒ Fox News World",     "url": "https://moxie.foxnews.com/google-publisher/world.xml"},
    {"name": "ğŸŒ Al Jazeera",         "url": "https://www.aljazeera.com/xml/rss/all.xml"},
    {"name": "ğŸ‡ºğŸ‡¸ Breaking Defense",  "url": "https://breakingdefense.com/feed/"},
    {"name": "ğŸ‡ºğŸ‡¸ Defense News",      "url": "https://www.defensenews.com/arc/outboundfeeds/rss/"},
    {"name": "ğŸ‡®ğŸ‡± IDF Official",      "url": "https://www.idf.il/en/mini-sites/idf-spokesperson-english/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Times of Israel",   "url": "https://www.timesofisrael.com/feed/"},
    {"name": "ğŸ‡®ğŸ‡± Haaretz",          "url": "https://www.haaretz.com/cmlink/1.4455099"},
    {"name": "ğŸ‡®ğŸ‡· Iran International","url": "https://www.iranintl.com/en/rss"},
    {"name": "ğŸ” ISW (War Study)",   "url": "https://www.understandingwar.org/rss.xml"},
]

# Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯ÙˆÚ¯Ù„ Ù†ÛŒÙˆØ²
GOOGLE_NEWS_QUERIES = [
    ("âš”ï¸ Iran Israel Attack",       "Iran Israel military attack strike revenge"),
    ("âš”ï¸ IDF Strike Iran",          "IDF airstrike Iran IRGC base facilities"),
    ("âš”ï¸ US Forces Attacked",       "US forces attacked base Iraq Syria CENTCOM"),
    ("âš”ï¸ Hezbollah Conflict",       "Hezbollah IDF border strike Lebanon rockets"),
]

def google_news_url(query: str) -> str:
    return f"https://news.google.com/rss/search?q={query.replace(' ', '+')}&hl=en-US&gl=US&ceid=US:en&num=10"

GOOGLE_FEEDS = [{"name": name, "url": google_news_url(q), "is_google": True} for name, q in GOOGLE_NEWS_QUERIES]

# ØªÙˆÛŒÛŒØªØ± (Nitter Ùˆ RSSHub)
TWITTER_ACCOUNTS = [
    ("ğŸ“° Barak Ravid",      "BarakRavid"),
    ("ğŸ“° Natasha Bertrand", "NatashaBertrand"),
    ("ğŸ“° Idrees Ali",       "idreesali114"),
    ("ğŸ“° Farnaz Fassihi",   "farnazfassihi"),
    ("ğŸ” OSINT Defender",   "OSINTdefender"),
    ("ğŸ” Intel Crab",       "IntelCrab"),
    ("ğŸ” War Monitor",      "WarMonitor3"),
    ("ğŸ‡®ğŸ‡± IDF Official",   "IDF"),
    ("ğŸ‡ºğŸ‡¸ CENTCOM",        "CENTCOM"),
]

TWITTER_MIRRORS = [
    "https://rsshub.app/twitter/user",     
    "https://nitter.poast.org",            
    "https://nitter.privacydev.net",       
]

def get_twitter_feeds() -> list[dict]:
    feeds = []
    for name, handle in TWITTER_ACCOUNTS:
        for mirror in TWITTER_MIRRORS:
            url = f"{mirror}/{handle}" if "rsshub" in mirror else f"{mirror}/{handle}/rss"
            feeds.append({"name": f"ğ• {name}", "url": url, "nitter_handle": handle})
            break 
    return feeds

ALL_FEEDS = RSS_FEEDS + GOOGLE_FEEDS + get_twitter_feeds()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û². ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def is_fresh_news(entry: dict) -> bool:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if not t: return True 
        
        dt = datetime(*t[:6], tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        
        # ÙÛŒÙ„ØªØ± Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡ Û²Û± ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û¶
        cutoff = datetime(2026, 2, 21, tzinfo=timezone.utc)
        if dt < cutoff:
            return False
            
        # ÙÛŒÙ„ØªØ± Û²Û´ Ø³Ø§Ø¹Øª
        if (now - dt) > timedelta(hours=24):
            return False
            
        return True
    except:
        return True

def is_relevant(entry: dict, is_twitter: bool = False) -> bool:
    text = " ".join([
        str(entry.get("title", "")),
        str(entry.get("summary", "")),
        str(entry.get("description", "")),
    ]).lower()
    
    if is_twitter:
        if any(kw in text for kw in ["iran", "israel", "us ", "strike", "war", "gaza", "lebanon", "irgc", "idf", "military", "attack", "missile"]):
            return True
        return False
        
    KEYWORDS = ["iran", "irgc", "tehran", "khamenei", "israel", "idf", "mossad", "tel aviv", "netanyahu",
                "us forces", "centcom", "pentagon", "american base", "strike", "airstrike", "drone", "missile"]
    return any(kw in text for kw in KEYWORDS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û³. Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ù…Ù† Ùˆ Ø¶Ø¯ Ù‡Ù†Ú¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def fetch_single_feed(client: httpx.AsyncClient, cfg: dict) -> list:
    url = cfg["url"]
    try:
        # ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø³Ø®Øª Û¸ Ø«Ø§Ù†ÛŒÙ‡. Ø§Ú¯Ø± Ø³Ø§ÛŒØªÛŒ Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ø§Ø¯ Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ù‚Ø·Ø¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ù†Ú¯ Ù†Ú©Ù†Ø¯
        response = await client.get(url, timeout=httpx.Timeout(8.0), headers={"User-Agent": "Mozilla/5.0 MilNewsBot/7.0"})
        if response.status_code == 200:
            return feedparser.parse(response.text).entries
    except:
        pass # Ø±Ø¯ Ø´Ø¯Ù† Ø¨ÛŒâ€ŒØµØ¯Ø§ Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨
    return []

async def fetch_all_feeds_concurrently(client: httpx.AsyncClient, feeds: list) -> list:
    tasks = [fetch_single_feed(client, cfg) for cfg in feeds]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    entries_with_cfg = []
    for i, entries in enumerate(results):
        if isinstance(entries, list):
            for entry in entries:
                entries_with_cfg.append((entry, feeds[i]))
    return entries_with_cfg

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Û´. Ù…ØªØ±Ø¬Ù… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ REST API (Ø¨Ø¯ÙˆÙ† Ù‡Ù†Ú¯ Ú©Ø±Ø¯Ù†)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def ai_translate_combined(client: httpx.AsyncClient, title: str, summary: str) -> tuple:
    """ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø¯ÙˆØ± Ø²Ø¯Ù† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú¯ÙˆÚ¯Ù„"""
    if not GEMINI_API_KEY or len(title) < 3:
        return title, summary

    prompt = f"""Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ±Ø¬Ù… Ø§Ø±Ø´Ø¯ Ù†Ø¸Ø§Ù…ÛŒ Ù‡Ø³ØªÛŒØ¯.
Ø¹Ù†ÙˆØ§Ù† Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø®Ø¨Ø± Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø¨Ø§ Ù„Ø­Ù† Ú©Ø§Ù…Ù„Ø§Ù‹ Ø®Ø¨Ø±ÛŒ ØªØ±Ø¬Ù…Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ (Ø¨Ø¯ÙˆÙ† Ú©Ù„Ù…Ù‡ Ø§Ø¶Ø§ÙÙ‡):
[Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ]
---
[Ø®Ù„Ø§ØµÙ‡ ÙØ§Ø±Ø³ÛŒ]

Title: {title}
Summary: {summary}"""

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.2}
    }

    try:
        # ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Û±Ûµ Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
        response = await client.post(url, json=payload, timeout=httpx.Timeout(15.0))
        if response.status_code == 200:
            text = response.json()["candidates"][0]["content"]["parts"][0]["text"]
            parts = text.split("---")
            if len(parts) >= 2:
                return parts[0].strip(), parts[1].strip()
            else:
                return text.strip(), summary
        elif response.status_code == 429:
            log.warning("âš ï¸ Ù„ÛŒÙ…ÛŒØª Ú¯ÙˆÚ¯Ù„! (Ú†Ù†Ø¯ Ø«Ø§Ù†ÛŒÙ‡ ØªÙˆÙ‚Ù)")
            await asyncio.sleep(5) # Ø¯Ø± ØµÙˆØ±Øª Ù„ÛŒÙ…ÛŒØªØŒ Ûµ Ø«Ø§Ù†ÛŒÙ‡ ØµØ¨Ø± Ù…ÛŒÚ©Ù†Ø¯
    except Exception as e:
        log.error(f"Ø®Ø·Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ API: {e}")
        
    return title, summary # Ø¯Ø± ØµÙˆØ±Øª Ø§Ø±ÙˆØ±ØŒ Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÙØ±Ø³ØªØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

def clean_html(text: str) -> str:
    if not text: return ""
    return BeautifulSoup(str(text), "html.parser").get_text(" ", strip=True)

def make_id(entry: dict) -> str:
    key = entry.get("link") or entry.get("id") or entry.get("title") or ""
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def format_dt(entry: dict) -> str:
    try:
        t = entry.get("published_parsed") or entry.get("updated_parsed")
        if t:
            dt = datetime(*t[:6], tzinfo=timezone.utc).astimezone(TEHRAN_TZ)
            return dt.strftime("ğŸ• %H:%M  |  ğŸ“… %Y/%m/%d")
    except:
        pass
    return ""

def escape_html(text: str) -> str:
    return text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ø§ÙØ¸Ù‡ Ø®Ø¨Ø±Ù‡Ø§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f: return set(json.load(f))
        except: pass
    return set()

def save_seen(seen: set):
    recent = list(seen)[-10000:]
    with open(SEEN_FILE, "w") as f: json.dump(recent, f)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TGAPI = f"https://api.telegram.org/bot{BOT_TOKEN}"

async def tg_send(client: httpx.AsyncClient, text: str) -> bool:
    for _ in range(3):
        try:
            r = await client.post(f"{TGAPI}/sendMessage", json={
                "chat_id": CHANNEL_ID,
                "text": text[:MAX_MSG_LEN],
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            }, timeout=httpx.Timeout(10.0))
            
            data = r.json()
            if data.get("ok"): return True
            if data.get("error_code") == 429:
                await asyncio.sleep(data.get("parameters", {}).get("retry_after", 10))
            else:
                return False
        except Exception:
            await asyncio.sleep(5)
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø¨Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        log.error("âŒ ØªÙˆÚ©Ù† Ø¨Ø§Øª ÛŒØ§ Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        return

    seen = load_seen()
    log.info(f"ğŸ”„ Û±. Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø²Ù…Ø§Ù† Ø®Ø¨Ø±Ù‡Ø§ Ø§Ø² Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒâ€ŒÙ‡Ø§...")
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¯Ø±ÛŒØ§ÙØª Ø®Ø¨Ø±Ù‡Ø§
        raw_entries = await fetch_all_feeds_concurrently(client, ALL_FEEDS)
        log.info(f"âœ… Û². Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØª. ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ùˆ Ù‚Ø¯ÛŒÙ…ÛŒ...")
        
        collected = []
        for entry, cfg in raw_entries:
            is_tw = bool(cfg.get("nitter_handle"))
            eid = make_id(entry)
            
            if eid in seen: continue
            if not is_fresh_news(entry):
                seen.add(eid)
                continue
            if not is_relevant(entry, is_twitter=is_tw):
                seen.add(eid)
                continue
                
            collected.append((eid, entry, cfg, is_tw))

        collected = collected[::-1] 
        if len(collected) > MAX_NEW_PER_RUN:
            collected = collected[-MAX_NEW_PER_RUN:]

        log.info(f"ğŸ” Û³. ØªØ¹Ø¯Ø§Ø¯ {len(collected)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„ ÛŒØ§ÙØª Ø´Ø¯.")

        # Ù…Ø±Ø­Ù„Ù‡ Û²: ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§Ø±Ø³Ø§Ù„
        sent = 0
        for eid, entry, cfg, is_tw in collected:
            en_title = clean_html(entry.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")).strip()
            raw_summary = clean_html(entry.get("summary") or entry.get("description") or "")
            en_summary_short = raw_summary[:400].rsplit(" ", 1)[0] + "â€¦" if len(raw_summary) > 400 else raw_summary
            link = entry.get("link", "")
            dt = format_dt(entry)
            icon = "ğ•" if is_tw else "ğŸ“¡"

            log.info(f"â³ Ø¯Ø± Ø­Ø§Ù„ ØªØ±Ø¬Ù…Ù‡ Ø®Ø¨Ø±: {en_title[:40]}...")
            fa_title, fa_summary = await ai_translate_combined(client, en_title, en_summary_short)
            
            fa_title = escape_html(fa_title.replace("**", ""))
            fa_summary = escape_html(fa_summary.replace("**", ""))
            en_title_escaped = escape_html(en_title)

            lines = [f"ğŸ”´ <b>{fa_title}</b>", ""]
            if fa_summary and fa_summary.lower() not in fa_title.lower() and len(fa_summary) > 10:
                lines += [f"ğŸ”¹ <i>{fa_summary}</i>", ""]
                
            lines += [
                "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
                f"ğŸ‡ºğŸ‡¸ <b>Ù…ØªÙ† Ø§ØµÙ„ÛŒ:</b>",
                f"<blockquote expandable>{en_title_escaped}</blockquote>"
            ]
            if dt: lines.append(dt)
            lines.append(f"{icon} <b>{cfg['name']}</b>")
            if link: lines.append(f'ğŸ”— <a href="{link}">Ù„ÛŒÙ†Ú© Ø®Ø¨Ø± Ø§ØµÙ„ÛŒ</a>')

            msg = "\n".join(lines)
            
            if await tg_send(client, msg):
                seen.add(eid)
                sent += 1
                log.info(f"  âœ… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")
            
            await asyncio.sleep(SEND_DELAY)

        save_seen(seen)
        log.info(f"âœ”ï¸ Ù¾Ø§ÛŒØ§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ | {sent} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ (Ø§Ù…Ø±ÙˆØ² Ø¨Ù‡ Ø¨Ø¹Ø¯) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.")

if __name__ == "__main__":
    asyncio.run(main())
