#!/usr/bin/env python3
"""
sources_updater.py â€” Ù‡ÙØªÙ‡â€ŒØ§ÛŒ ÛŒÚ© Ø¨Ø§Ø± Ø¯Ø± GitHub Actions Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯
ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ú©Ù…ØªØ± Ø§Ø² Ûµ Ù…Ú¯Ø§Ø¨Ø§ÛŒØª)
Ø®Ø±ÙˆØ¬ÛŒ: data/extra_sources.json  â† bot.py Ø§ÛŒÙ† Ø±Ø§ Ù‡Ø± Ø¨Ø§Ø± startup Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯
"""

import asyncio, json, re, io, sys
from pathlib import Path
from datetime import datetime, timezone

try:
    import httpx
    import pandas as pd
except ImportError:
    print("Ù†ØµØ¨: pip install httpx pandas")
    sys.exit(1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ÙÙ‚Ø· Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡/Ø¬Ù†Ú¯ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯
RELEVANT_KEYWORDS = [
    "middle east", "iran", "israel", "arab", "gulf", "nuclear",
    "military", "defense", "security", "foreign", "world",
    "international", "geopolit", "war", "conflict", "sanctions",
    "irgc", "idf", "nato", "terrorism", "intelligence",
]

SPAM_KEYWORDS = [
    "crypto", "bitcoin", "forex", "casino", "adult", "dating",
    "movie", "music", "funny", "meme", "game", "sport", "food",
    "fashion", "travel", "shopping", "nft", "invest",
]

# Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ù…Ø·Ù…Ø¦Ù† Ú©Ù‡ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯
TRUSTED_DOMAINS = {
    "apnews.com", "reuters.com", "bbc.com", "aljazeera.com",
    "nbcnews.com", "cnn.com", "foreignpolicy.com", "axios.com",
    "politico.com", "vox.com", "npr.org", "pbs.org",
    "defensenews.com", "stripes.com", "militarytimes.com",
    "nationalinterest.org", "brookings.edu", "cfr.org",
    "mei.edu", "stimson.org", "rand.org", "wilsoncenter.org",
    "atlanticcouncil.org", "middleeasteye.net", "haaretz.com",
    "jpost.com", "timesofisrael.com", "ynetnews.com",
    "arabnews.com", "alaraby.co.uk",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ (Ù‡Ù…Ù‡ Ú©ÙˆÚ†Ú© Ù‡Ø³ØªÙ†Ø¯ â€” Ø²ÛŒØ± Ûµ Ù…Ú¯Ø§Ø¨Ø§ÛŒØª)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SOURCES = {

    # â”€â”€ ercexpo/us-news-domains  (~1.5 MB) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "ercexpo_domains": {
        "url": "https://raw.githubusercontent.com/ercexpo/us-news-domains/main/dataset/us-news-domains-v2.0.0.csv",
        "type": "csv",
        "desc": "Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§",
    },
    "ercexpo_twitter": {
        "url": "https://raw.githubusercontent.com/ercexpo/us-news-domains/main/dataset/us-news-twitter-v1.0.0.csv",
        "type": "csv",
        "desc": "Twitter handles Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ",
    },

    # â”€â”€ TGDataset â€” ÙÙ‚Ø· channel_list (Ú©ÙˆÚ†Ú©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # âŒ TGDataset Ø§ØµÙ„ÛŒ Û´Û¶Û° Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª Ø§Ø³Øª â€” Ø¢Ù† Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    # âœ… ÙÙ‚Ø· ÙØ§ÛŒÙ„ metadata Ú©ÙˆÚ†Ú© Ø¢Ù† Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
    "tgdataset_list": {
        "url": "https://raw.githubusercontent.com/SystemsLab-Sapienza/TGDataset/main/data/channel_list.json",
        "type": "json",
        "desc": "Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… (metadata ÙÙ‚Ø·)",
    },

    # â”€â”€ verified Twitter accounts (~200 KB) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "verified_twitter": {
        "url": "https://raw.githubusercontent.com/thansen0/verified_twitters/main/verified_users.csv",
        "type": "csv",
        "desc": "Ø§Ú©Ø§Ù†Øªâ€ŒÙ‡Ø§ÛŒ verified ØªÙˆÛŒÛŒØªØ±",
    },
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¯Ø§Ù†Ù„ÙˆØ¯ â€” Ø¨Ø§ timeout Ùˆ Ø­Ø¯Ø§Ú©Ø«Ø± Ûµ Ù…Ú¯Ø§Ø¨Ø§ÛŒØª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_FILE_SIZE = 5 * 1024 * 1024   # Ûµ Ù…Ú¯Ø§Ø¨Ø§ÛŒØª Ø³Ù‚Ù

async def safe_get(client: httpx.AsyncClient, url: str, desc: str) -> str | None:
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ù…Ù† â€” Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø§Ø² ÛµMB Ø¨ÙˆØ¯ØŒ Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    print(f"  ğŸ“¥ {desc} ...", end=" ", flush=True)
    try:
        # Ø§ÙˆÙ„ ÙÙ‚Ø· header Ø¨Ú¯ÛŒØ± ØªØ§ Ø­Ø¬Ù… Ø±Ø§ Ø¨Ø¯Ø§Ù†ÛŒÙ…
        head = await client.head(url, timeout=10)
        size = int(head.headers.get("content-length", 0))
        if size > MAX_FILE_SIZE:
            print(f"âŒ Ø­Ø¬Ù… {size//1024//1024}MB â€” Ø±Ø¯ Ø´Ø¯ (Ø³Ù‚Ù ÛµMB)")
            return None

        # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§ØµÙ„ ÙØ§ÛŒÙ„
        r = await client.get(url, timeout=30)
        if r.status_code != 200:
            print(f"âŒ HTTP {r.status_code}")
            return None

        if len(r.content) > MAX_FILE_SIZE:
            print(f"âŒ Ø­Ø¬Ù… {len(r.content)//1024}KB â€” Ø±Ø¯ Ø´Ø¯")
            return None

        print(f"âœ… {len(r.content)//1024}KB")
        return r.text

    except Exception as e:
        print(f"âŒ {type(e).__name__}: {e}")
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù¾Ø±Ø¯Ø§Ø²Ø´ ercexpo_domains â†’ RSS feeds Ø¬Ø¯ÛŒØ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_ercexpo_domains(csv_text: str) -> list[dict]:
    """Ø§Ø² CSV Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ â†’ RSS feeds Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡"""
    feeds = []
    try:
        df = pd.read_csv(io.StringIO(csv_text))
        cols = {c.lower(): c for c in df.columns}
        print(f"    Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(df.columns[:8])}")

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
        domain_col = next((cols[k] for k in cols if "domain" in k or "url" in k), None)
        name_col   = next((cols[k] for k in cols if "name" in k or "outlet" in k or "title" in k), None)
        scope_col  = next((cols[k] for k in cols if "scope" in k or "type" in k or "level" in k or "national" in k), None)

        if not domain_col:
            print("    âš ï¸  Ø³ØªÙˆÙ† Ø¯Ø§Ù…Ù†Ù‡ Ù†ÛŒØ§ÙØªÙ…")
            return []

        for _, row in df.iterrows():
            domain = str(row.get(domain_col, "")).strip().lower()
            domain = re.sub(r'^https?://', '', domain).strip("/")
            if not domain or domain == "nan": continue

            name   = str(row.get(name_col, domain) if name_col else domain).strip()
            scope  = str(row.get(scope_col, "") if scope_col else "").lower()

            # ÙÛŒÙ„ØªØ± Û±: ÙÙ‚Ø· national/international (Ù†Ù‡ purely local)
            if scope and "local" in scope and "national" not in scope:
                continue

            combined = (domain + " " + name).lower()

            # ÙÛŒÙ„ØªØ± Û²: Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø§
            is_trusted = any(td in domain for td in TRUSTED_DOMAINS)
            is_relevant = any(kw in combined for kw in RELEVANT_KEYWORDS)

            if not (is_trusted or is_relevant):
                continue

            # Ø³Ø§Ø®Øª URL ÙÛŒØ¯ â€” Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬
            rss_url = f"https://{domain}/feed/"
            feeds.append({
                "n": f"ğŸ“° {name}",
                "u": rss_url,
                "domain": domain,
                "source": "ercexpo",
            })

        print(f"    â†’ {len(feeds)} ÙÛŒØ¯ Ù…Ø±ØªØ¨Ø·")
    except Exception as e:
        print(f"    âŒ Ø®Ø·Ø§: {e}")

    return feeds


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù¾Ø±Ø¯Ø§Ø²Ø´ ercexpo_twitter â†’ Twitter handles Ø¬Ø¯ÛŒØ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_ercexpo_twitter(csv_text: str) -> list[dict]:
    """Ø§Ø² CSV ØªÙˆÛŒÛŒØªØ± â†’ handleâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·"""
    handles = []
    try:
        df = pd.read_csv(io.StringIO(csv_text))
        cols = {c.lower(): c for c in df.columns}
        print(f"    Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(df.columns[:8])}")

        handle_col = next((cols[k] for k in cols
                           if "handle" in k or "screen" in k or "twitter" in k or "username" in k), None)
        name_col   = next((cols[k] for k in cols
                           if "name" in k or "outlet" in k), None)

        if not handle_col:
            print("    âš ï¸  Ø³ØªÙˆÙ† handle Ù†ÛŒØ§ÙØªÙ…")
            return []

        for _, row in df.iterrows():
            handle = str(row.get(handle_col, "")).strip().lstrip("@")
            if not handle or handle == "nan": continue

            name = str(row.get(name_col, "") if name_col else "").strip()
            combined = (handle + " " + name).lower()

            if any(kw in combined for kw in RELEVANT_KEYWORDS):
                handles.append({
                    "label":  f"ğŸ“° {name}" if name and name != "nan" else f"ğŸ“° @{handle}",
                    "handle": handle,
                    "source": "ercexpo",
                })

        print(f"    â†’ {len(handles)} handle Ù…Ø±ØªØ¨Ø·")
    except Exception as e:
        print(f"    âŒ Ø®Ø·Ø§: {e}")

    return handles


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù¾Ø±Ø¯Ø§Ø²Ø´ verified_twitter â†’ handleâ€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ ØªØ£ÛŒÛŒØ¯Ø´Ø¯Ù‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_verified_twitter(csv_text: str) -> list[dict]:
    """Ø§Ø² Ù„ÛŒØ³Øª verified â€” ÙÙ‚Ø· Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ù…Ø±ØªØ¨Ø·"""
    handles = []
    try:
        df = pd.read_csv(io.StringIO(csv_text))
        cols = {c.lower(): c for c in df.columns}
        print(f"    Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(df.columns[:8])}")

        handle_col = next((cols[k] for k in cols
                           if "screen" in k or "handle" in k or "name" in k or "username" in k), None)
        desc_col   = next((cols[k] for k in cols
                           if "desc" in k or "bio" in k or "category" in k), None)

        if not handle_col:
            print("    âš ï¸  Ø³ØªÙˆÙ† handle Ù†ÛŒØ§ÙØªÙ…")
            return []

        for _, row in df.iterrows():
            handle = str(row.get(handle_col, "")).strip().lstrip("@")
            if not handle or handle == "nan": continue

            desc = str(row.get(desc_col, "") if desc_col else "").lower()

            # ÙÙ‚Ø· Ø±Ø³Ø§Ù†Ù‡/Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±/ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù…Ø±ØªØ¨Ø·
            is_relevant = any(kw in (handle + " " + desc).lower() for kw in RELEVANT_KEYWORDS)
            is_spam     = any(kw in (handle + " " + desc).lower() for kw in SPAM_KEYWORDS)

            if is_relevant and not is_spam:
                handles.append({
                    "label":  f"âœ… @{handle}",
                    "handle": handle,
                    "source": "verified_tw",
                })

        print(f"    â†’ {len(handles)} handle Ø®Ø¨Ø±ÛŒ/ØªØ­Ù„ÛŒÙ„ÛŒ")
    except Exception as e:
        print(f"    âŒ Ø®Ø·Ø§: {e}")

    return handles[:30]   # Ø³Ù‚Ù Û³Û° ØªØ§ Ø§Ø² Ø§ÛŒÙ† Ù…Ù†Ø¨Ø¹


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù¾Ø±Ø¯Ø§Ø²Ø´ TGDataset channel_list â†’ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…ÛŒ Ù…Ø±ØªØ¨Ø·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_tgdataset(json_text: str) -> list[dict]:
    """Ø§Ø² channel_list.json â†’ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§ÛŒØ±Ø§Ù†/Ø¬Ù†Ú¯"""
    channels = []
    try:
        data = json.loads(json_text)

        # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ù…Ú©Ù†
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            items = (data.get("channels") or data.get("data") or
                     data.get("items") or list(data.values()))
            if items and isinstance(items[0], list):
                items = items[0]
        else:
            items = []

        print(f"    {len(items)} Ú©Ø§Ù†Ø§Ù„ Ø¯Ø± Ù„ÛŒØ³Øª")

        for item in items:
            if isinstance(item, str):
                username = item.strip().lstrip("@")
                title = desc = ""
            elif isinstance(item, dict):
                username = (item.get("username") or item.get("handle") or
                            item.get("id") or item.get("name") or "").strip().lstrip("@")
                title    = (item.get("title") or item.get("name") or "").strip()
                desc     = (item.get("description") or item.get("about") or "").strip()
            else:
                continue

            if not username or len(username) < 3: continue

            combined = (username + " " + title + " " + desc).lower()

            # Ø­Ø°Ù Ø§Ø³Ù¾Ù…
            if any(kw in combined for kw in SPAM_KEYWORDS): continue

            # Ø§Ù…ØªÛŒØ§Ø² Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†
            score = sum(1 for kw in RELEVANT_KEYWORDS if kw in combined)
            if score >= 2:
                channels.append({
                    "label":  f"ğŸ”´ {title}" if title else f"ğŸ”´ @{username}",
                    "handle": username,
                    "score":  score,
                    "source": "tgdataset",
                })

        channels.sort(key=lambda x: -x["score"])
        print(f"    â†’ {len(channels)} Ú©Ø§Ù†Ø§Ù„ Ù…Ø±ØªØ¨Ø· (scoreâ‰¥2)")
        return channels[:60]   # Ø³Ù‚Ù Û¶Û° Ú©Ø§Ù†Ø§Ù„ Ø¨Ø±ØªØ±

    except Exception as e:
        print(f"    âŒ Ø®Ø·Ø§: {e}")
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ bot.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_existing() -> tuple[set, set, set]:
    """Ø¢Ù†Ú†Ù‡ Ø¯Ø± bot.py Ù‡Ø³Øª Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ ØªØ§ ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†ÛŒÙ…"""
    existing_rss = set()
    existing_tw  = set()
    existing_tg  = set()

    for fname in ["bot.py", "warbot.py", "main.py"]:
        if not Path(fname).exists(): continue
        text = Path(fname).read_text(encoding="utf-8")
        for m in re.finditer(r'"u"\s*:\s*"([^"]+)"', text):
            existing_rss.add(m.group(1).split("?")[0].rstrip("/").lower())
        for m in re.finditer(r'\(\s*"[^"]*"\s*,\s*"([A-Za-z0-9_]{3,50})"\s*\)', text):
            h = m.group(1).lower()
            existing_tw.add(h)
            existing_tg.add(h)
        break

    print(f"  Ù…ÙˆØ¬ÙˆØ¯: {len(existing_rss)} RSS  {len(existing_tw)} Twitter  {len(existing_tg)} TG")
    return existing_rss, existing_tw, existing_tg


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save(rss: list, tw: list, tg: list):
    Path("data").mkdir(exist_ok=True)
    out = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "stats": {
            "rss": len(rss),
            "twitter": len(tw),
            "telegram": len(tg),
        },
        "rss_feeds": rss,
        "twitter":   tw,
        "telegram":  tg,
    }
    Path("data/extra_sources.json").write_text(
        json.dumps(out, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    print(f"\nâœ… data/extra_sources.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    print(f"   RSS: {len(rss)}  Twitter: {len(tw)}  Telegram: {len(tg)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    print("=" * 60)
    print("  sources_updater â€” Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ (ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©)")
    print("=" * 60)

    existing_rss, existing_tw, existing_tg = load_existing()

    all_rss = []
    all_tw  = []
    all_tg  = []

    async with httpx.AsyncClient(
        follow_redirects=True,
        headers={"User-Agent": "WarBot-sources/1.0 (+github.com)"},
        timeout=30,
    ) as client:

        for key, meta in SOURCES.items():
            print(f"\nâ”€â”€ {key} â€” {meta['desc']}")
            text = await safe_get(client, meta["url"], meta["desc"])
            if not text:
                continue

            if key == "ercexpo_domains":
                all_rss.extend(process_ercexpo_domains(text))

            elif key == "ercexpo_twitter":
                all_tw.extend(process_ercexpo_twitter(text))

            elif key == "verified_twitter":
                all_tw.extend(process_verified_twitter(text))

            elif key == "tgdataset_list":
                all_tg.extend(process_tgdataset(text))

    # â”€â”€ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ bot.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâ”€â”€ ÙÛŒÙ„ØªØ± Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ")
    new_rss = []
    seen_rss = set(existing_rss)
    for f in all_rss:
        url_clean = f["u"].split("?")[0].rstrip("/").lower()
        if url_clean not in seen_rss:
            new_rss.append(f)
            seen_rss.add(url_clean)

    new_tw = []
    seen_tw = set(existing_tw)
    for t in all_tw:
        h = t["handle"].lower()
        if h not in seen_tw:
            new_tw.append(t)
            seen_tw.add(h)

    new_tg = []
    seen_tg = set(existing_tg)
    for t in all_tg:
        h = t["handle"].lower()
        if h not in seen_tg:
            new_tg.append(t)
            seen_tg.add(h)

    print(f"  Ø¬Ø¯ÛŒØ¯: {len(new_rss)} RSS  {len(new_tw)} Twitter  {len(new_tg)} Telegram")

    save(new_rss, new_tw, new_tg)


if __name__ == "__main__":
    asyncio.run(main())
